{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity Analysis\n",
    "====================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2020-10-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: A surrogate model can be explored to understand the\n",
    "sensitivity of the system. This lecture will review how to perform\n",
    "sensitivity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "%\\newcommand{\\tk}[1]{\\textbf{TK}: #1}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%\n",
    "\n",
    "% Already defined by latex %\n",
    "\n",
    "% Already defined by latex %\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->\n",
    "\n",
    "(Kennedy and O’Hagan, 2000; Saltelli et al., 2010, 2008, 2004; Sobol,\n",
    "2001, 1990)\n",
    "\n",
    "> A possible definition of sensitivity analysis is the following: The\n",
    "> study of how uncertainty in the output of a model (numerical or\n",
    "> otherwise) can be apportioned to different sources of uncertainty in\n",
    "> the model input (Saltelli et al., 2004). A related practice is\n",
    "> ‘uncertainty analysis’, which focuses rather on quantifying\n",
    "> uncertainty in model output. Ideally, uncertainty and sensitivity\n",
    "> analyses should be run in tandem, with uncertainty analysis preceding\n",
    "> in current practice.\n",
    ">\n",
    "> In Chapter 1 of Saltelli et al. (2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emukit\n",
      "  Downloading emukit-0.4.7.tar.gz (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /Users/neil/anaconda3/lib/python3.6/site-packages (from emukit) (46.0.0.post20200309)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/neil/anaconda3/lib/python3.6/site-packages (from emukit) (1.18.2)\n",
      "Collecting GPy[plotting]>=1.9.9\n",
      "  Downloading GPy-1.9.9-cp36-cp36m-macosx_10_7_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting emcee==2.2.1\n",
      "  Downloading emcee-2.2.1.tar.gz (24 kB)\n",
      "Collecting scipy==1.1.0\n",
      "  Downloading scipy-1.1.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (16.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.7 MB 4.3 MB/s eta 0:00:01    |▋                               | 327 kB 3.2 MB/s eta 0:00:06     |██▍                             | 1.3 MB 3.2 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting paramz>=0.9.0\n",
      "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 5.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/neil/anaconda3/lib/python3.6/site-packages (from GPy[plotting]>=1.9.9->emukit) (1.14.0)\n",
      "Collecting plotly>=1.8.6; extra == \"plotting\"\n",
      "  Downloading plotly-4.12.0-py2.py3-none-any.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.0; extra == \"plotting\" in /Users/neil/anaconda3/lib/python3.6/site-packages (from GPy[plotting]>=1.9.9->emukit) (3.1.3)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /Users/neil/anaconda3/lib/python3.6/site-packages (from paramz>=0.9.0->GPy[plotting]>=1.9.9->emukit) (4.4.2)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/neil/anaconda3/lib/python3.6/site-packages (from matplotlib>=3.0; extra == \"plotting\"->GPy[plotting]>=1.9.9->emukit) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/neil/anaconda3/lib/python3.6/site-packages (from matplotlib>=3.0; extra == \"plotting\"->GPy[plotting]>=1.9.9->emukit) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/neil/anaconda3/lib/python3.6/site-packages (from matplotlib>=3.0; extra == \"plotting\"->GPy[plotting]>=1.9.9->emukit) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/neil/anaconda3/lib/python3.6/site-packages (from matplotlib>=3.0; extra == \"plotting\"->GPy[plotting]>=1.9.9->emukit) (2.8.1)\n",
      "Building wheels for collected packages: emukit, emcee, paramz, retrying\n",
      "  Building wheel for emukit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emukit: filename=emukit-0.4.7-py3-none-any.whl size=188043 sha256=ead121aa5b2c3dc39aeee15120d8ea832ddb2d0117f869368b793a11590c2473\n",
      "  Stored in directory: /Users/neil/Library/Caches/pip/wheels/4b/bf/27/4c5c291f56ac6d9eb3fab8445880f1a5a86df61f08ee82017e\n",
      "  Building wheel for emcee (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emcee: filename=emcee-2.2.1-py3-none-any.whl size=29592 sha256=e439aa083879554d4afb4dd9a17b08385c2153cb50b94e88fe28971cc9346b4d\n",
      "  Stored in directory: /Users/neil/Library/Caches/pip/wheels/35/d5/c8/cc1a16238e036f053a0ca074bb104c5520f2a5d2aaa13a97d3\n",
      "  Building wheel for paramz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102549 sha256=8049daa80bc8d236ca1a588fa166aecf161838def9ebea63c8d7c1d6d121d71d\n",
      "  Stored in directory: /Users/neil/Library/Caches/pip/wheels/72/d5/80/7921b405df363b0f10446fc93491092c20bc75491cbbd0a354\n",
      "  Building wheel for retrying (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11430 sha256=b5edfbf68c8bcf7b155bbf57e05786805937bf500f7dbfdd19912a7dbd0f05f4\n",
      "  Stored in directory: /Users/neil/Library/Caches/pip/wheels/ac/cb/8a/b27bf6323e2f4c462dcbf77d70b7c5e7868a7fbe12871770cf\n",
      "Successfully built emukit emcee paramz retrying\n",
      "Installing collected packages: scipy, paramz, retrying, plotly, GPy, emcee, emukit\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: paramz\n",
      "    Found existing installation: paramz 0.8.5\n",
      "    Uninstalling paramz-0.8.5:\n",
      "      Successfully uninstalled paramz-0.8.5\n",
      "  Attempting uninstall: GPy\n",
      "    Found existing installation: GPy 1.8.5\n",
      "    Uninstalling GPy-1.8.5:\n",
      "      Successfully uninstalled GPy-1.8.5\n",
      "Successfully installed GPy-1.9.9 emcee-2.2.1 emukit-0.4.7 paramz-0.9.5 plotly-4.12.0 retrying-1.3.3 scipy-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emukit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mlai.py', <http.client.HTTPMessage at 0x110e53320>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('teaching_plots.py', <http.client.HTTPMessage at 0x110e53828>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gp_tutorial.py', <http.client.HTTPMessage at 0x110e53c18>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity analysis is a statistical technique widely used to test the\n",
    "reliability of real systems. Imagine a simulator of taxis picking up\n",
    "customers in a city like the one showed in the [Emukit\n",
    "playground](https://github.com/amzn/emukit-playground). The profit of\n",
    "the taxi company depends on factors like the number of taxis on the road\n",
    "and the price per trip. In this example, a global sensitivity analysis\n",
    "of the simulator could be useful to decompose the variance of the profit\n",
    "in a way that can be assigned to the input variables of the simulator.\n",
    "\n",
    "There are different ways of doing a sensitivity analysis of the\n",
    "variables of a simulator. In this notebook we will start with an\n",
    "approach based on Monte Carlo sampling that is useful when evaluating\n",
    "the simulator is cheap. If evaluating the simulator is expensive,\n",
    "emulators can then be used to speed up computations. We will show this\n",
    "in the last part of the notebook. Next, we start with a few formal\n",
    "definitions and literature review so we can understand the basics of\n",
    "Sensitivity Analysis and how it can performed with Emukit.\n",
    "\n",
    "Any simulator can be viewed as a function $$\n",
    "y=f(\\mathbf{ x}),\n",
    "$$ where $\\mathbf{ x}$ is a vector of $p$ uncertain model inputs\n",
    "$x_1,\\dots,x_p$, and $y$ is some univariate model output. We assume that\n",
    "$f$ is a square integrable function and that the inputs are\n",
    "statistically independent and uniformly distributed within the hypercube\n",
    "$x_i \\in [0,1]$ for $i=1,2,\\dots,p$, although the bounds can be\n",
    "generalized. The Sobol decomposition of $f(\\cdot)$ allows us to write it\n",
    "as $$\n",
    "y= f_0 + \\sum_{i=1}^pf_i(x_i) + \\sum_{i<j}^{p} f_{ij}(x_i,x_j) + \\cdots + f_{1,2,\\dots,p}(x_1,x_2,\\dots,x_p),\n",
    "$$ where $f_0$ is a constant term, $f_i$ is a function of $x_i$,\n",
    "$f_{ij}$ a function of $x_i$ and $x_j$, etc. A condition of this\n",
    "decomposition is that, $$ \n",
    "\\int_0^1 f_{i_1 i_2 \\dots i_p}(x_{i_1},x_{i_2},\\dots,x_{i_p}) \\text{d}x_{k}=0, \\text{ for } k = i_1,...,i_p. \n",
    "$$ This means that all the terms in the decomposition are orthogonal,\n",
    "which can be written in terms of conditional expected values as\n",
    "$$\\begin{align*}\n",
    "f_0 &= E(y) \\\\\n",
    "f_i(x_i) & = E(y|x_i) - f_0 \\\\\n",
    "f_{ij}(x_i,x_j) & = E(y|x_i,x_j) - f_0 - f_i - f_j \n",
    "\\end{align*}$$ with all the expectations computed over $y$.\n",
    "\n",
    "Each component $f_i$ (main effects) can be seen as the effect on $y$ of\n",
    "varying $x_i$ alone. The same interpretation follows for $f_{ij}$ which\n",
    "accounts for the (extra) variation of changing $x_i$ and $x_j$\n",
    "simultaneously (second-order interaction). Higher-order terms have\n",
    "analogous definitions.\n",
    "\n",
    "The key step to decompose the variation of $y$ is to notice that $$\n",
    "\\text{var}(y) = E(y^2) - E(y)^2 = \\int_0^1 f^2(\\mathbf{ x}) \\text{d}\\mathbf{ x}- f_0^2\n",
    "$$ and that this variance can be decomposed as $$ \n",
    "\\text{var}(y) = \\int_0^1 \\sum_{i=1}^pf_i(x_i) \\text{d}x_i + \\int_0^1 \\sum_{i<j}^{p} f_{ij}(x_i,x_j)\\text{d} x_i \\text{d} x_j + \\cdots +\\int_0^1 f_{1,2,\\dots,d}(x_1,x_2,\\dots,x_p)\\text{d}\\mathbf{ x}.\n",
    "$$ This expression leads to the decomposition of the variance of $y$ as,\n",
    "$$ \n",
    "\\text{var}(y) = \\sum_{i=1}^pV_i + \\sum_{i<j}^{p} V_{ij} + \\cdots + V_{12 \\dots p},\n",
    "$$ where $$ \n",
    "V_{i} = \\text{var}_{x_i} \\left( E_{\\mathbf{ x}_{\\sim i}} (y\\mid x_{i}) \\right),\n",
    "$$ $$ \n",
    "V_{ij} = \\text{var}_{x_{ij}} \\left( E_{\\mathbf{ x}_{\\sim ij}} \\left( y\\mid x_i, x_j\\right)\\right) - \\operatorname{V}_{i} - \\operatorname{V}_{j}\n",
    "$$ and so on. The $\\mathbf{ x}_{\\sim i}$ notation is used to indicate\n",
    "all the set of variables but the $i^{th}$.\n",
    "\n",
    "**Note**: The previous decomposition is important because it shows how\n",
    "the variance in the output $y$ can be associated to each input or\n",
    "interaction separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: the Ishigami function\n",
    "------------------------------\n",
    "\n",
    "We illustrate the exact calculation of the Sobol indexes with the three\n",
    "dimensional Ishigami function of (Ishigami and Homma, 1989). This is a\n",
    "well-known example for uncertainty and sensitivity analysis methods\n",
    "because of its strong nonlinearity and peculiar dependence on $x_3$.\n",
    "More details of this function can be found in (Sobol and Levitan, 1999).\n",
    "\n",
    "Mathematically, the from of the Ishigami function is $$\n",
    "f(\\textbf{x}) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1). \n",
    "$$ In this notebook we will set the parameters to be $a = 5$ and $b=0.1$\n",
    ". The input variables are sampled randomly\n",
    "$x_i \\sim \\text{Uniform}(-\\pi,\\pi)$.\n",
    "\n",
    "Next we create the function object and visualize its shape marginally\n",
    "for each one of its three inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.test_functions.sensitivity import Ishigami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Load the Ishigami function\n",
    "ishigami = Ishigami(a=5, b=0.1)\n",
    "target_simulator = ishigami.fidelity1\n",
    "\n",
    "### --- Define the input space in which the simulator is defined\n",
    "variable_domain = (-np.pi,np.pi)\n",
    "x_grid = np.linspace(*variable_domain,100)\n",
    "X, Y = np.meshgrid(x_grid, x_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to any further analysis, we first plot the non-zero\n",
    "components $f(\\mathbf{ x})$. These components are $$\\begin{align*}\n",
    "f_1(x_1) & = \\sin(x_1) \\\\\n",
    "f_2(x_1) & = a \\sin^2 (x_2) \\\\\n",
    "f_{13}(x_1,x_3) & = b x_3^4 \\sin(x_1) \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = ishigami.f1(x_grid)\n",
    "f2 = ishigami.f2(x_grid)\n",
    "F13 = ishigami.f13(np.array([x_grid,x_grid]).T)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'teaching_plots' has no attribute 'big_wide_figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7701f03ec79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbig_wide_figure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$x_1$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'teaching_plots' has no attribute 'big_wide_figure'"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=plot.big_wide_figure)\n",
    "ax.append(fig.add_subplot(111, projection='3d'))\n",
    "\n",
    "ax[0].plot(x_grid, f1,'-r')\n",
    "ax[0].set_xlabel('$x_1$')\n",
    "ax[0].set_ylabel('$f_1$')\n",
    "\n",
    "ax[1].plot(x_grid,f2,'-r')\n",
    "ax[1].set_xlabel('$x_2$')\n",
    "ax[1].set_ylabel('$f_2$')\n",
    "\n",
    "plt.suptitle('Non-zero Sobol components of the Ishigami function')\n",
    "\n",
    "surf = ax[2].plot_surface(X, Y, F13, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_3$')\n",
    "ax.set_zlabel('$f_{13}$')\n",
    "\n",
    "mlai.write_figure(filename='non-zero-sobol-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/non-zero-sobol-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The non-zero components of the Ishigami function.</i>\n",
    "\n",
    "The total variance $\\text{var}(y)$ in this example is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ishigami.variance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the sum of the variance of $V_1$, $V_2$ and $V_{13}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ishigami.variance_x1, ishigami.variance_x2, ishigami.variance_x13)\n",
    "print(ishigami.variance_x1 + ishigami.variance_x2 + ishigami.variance_x13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Order Sobol Indices using Monte Carlo\n",
    "-------------------------------------------\n",
    "\n",
    "The first order Sobol indexes are a measure of “first order sensitivity”\n",
    "of each input variable. They account for the proportion of variance of\n",
    "$y$ explained by changing each variable alone while marginalizing over\n",
    "the rest. The Sobol index of the $i$th variable is computed as $$\n",
    "S_i = \\frac{V_i}{\\text{var}(y)}.\n",
    "$$ This value is standardized using the total variance so it is possible\n",
    "to account for a fractional contribution of each variable to the total\n",
    "variance of the output.\n",
    "\n",
    "The Sobol indexes for higher order interactions $S_{ij}$ are computed\n",
    "similarly. Note that the sum of all Sobol indexes equals to one.\n",
    "\n",
    "In most cases we are interested in the first order indexes. In the\n",
    "Ishigami function their values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ishigami.main_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most standard way of computing the Sobol indexes is using Monte\n",
    "Carlo. Details are given in (Sobol, 2001).\n",
    "\n",
    "With Emukit, the first-order Sobol indexes can be easily computed. We\n",
    "first need to define the space where of target simulator is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ContinuousParameter, ParameterSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_simulator = ishigami.fidelity1\n",
    "variable_domain = (-np.pi,np.pi)\n",
    "\n",
    "space = ParameterSpace(\n",
    "          [ContinuousParameter('x1', variable_domain[0], variable_domain[1]), \n",
    "           ContinuousParameter('x2', variable_domain[0], variable_domain[1]),\n",
    "           ContinuousParameter('x3', variable_domain[0], variable_domain[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the indexes is as easy as doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.sensitivity.monte_carlo import ModelFreeMonteCarloSensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)  # for reproducibility\n",
    "\n",
    "num_mc = 10000  # Number of MC samples\n",
    "senstivity_ishigami = ModelFreeMonteCarloSensitivity(target_simulator, space)\n",
    "main_effects, total_effects, _ = senstivity_ishigami.compute_effects(num_monte_carlo_points = num_mc)\n",
    "print(main_effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the true effects with the Monte Carlo effects in a bar-plot.\n",
    "The total effects are discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Sobol True': ishigami.main_effects,\n",
    "     'Monte Carlo': main_effects}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar',figsize=plot.big_wide_figsize, ax=ax)\n",
    "ax.set_title('First-order Sobol indices - Ishigami')\n",
    "ax.set_ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='first-order-sobol-indices-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/first-order-sobol-indices-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The non-zero components of the Ishigami function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Effects Using Monte Carlo\n",
    "-------------------------------\n",
    "\n",
    "Computing high order sensitivity indexes can be computationally very\n",
    "demanding in high dimensional scenarios and measuring the total\n",
    "influence of each variable on the variance of the output is infeasible.\n",
    "To solve this issue the *total* indexes are used which account for the\n",
    "contribution to the output variance of $x_i$ including all variance\n",
    "caused by the variable alone and all its interactions of any order. The\n",
    "total effect for $x_i$ is given by: $$ \n",
    "S_{Ti} = \\frac{E_{\\mathbf{ x}_{\\sim i}} \\left(\\text{var}_{x_i} (y\\mid \\mathbf{ x}_{\\sim i}) \\right)}{\\text{var}(y)} = 1 - \\frac{\\text{var}_{\\mathbf{ x}_{\\sim i}} \\left(E_{x_i} (y\\mid \\mathbf{ x}_{\\sim i}) \\right)}{\\text{var}(y)}\n",
    "$$\n",
    "\n",
    "Note that the sum of $S_{Ti}$ is not necessarily one in this case unless\n",
    "the model is additive. In the Ishigami example the value of the total\n",
    "effects is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ishigami.total_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous example, the total effects can be computed with Monte\n",
    "Carlo. In the next plot we show the comparison with the true total\n",
    "effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Sobol True': ishigami.total_effects,\n",
    "     'Monte Carlo': total_effects}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar',figsize=(12, 5), ax=ax)\n",
    "ax.set_title('Total effects - Ishigami')\n",
    "ax.set_ylabel('Effects value')\n",
    "\n",
    "mlai.write_figure(filename='total-effects-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/total-effects-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The total effects from the Ishigami function as computed via\n",
    "Monte Carlo estimate alongside the true total effects for the Ishigami\n",
    "function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the sensitivity coefficients using the output of a model\n",
    "------------------------------------------------------------------\n",
    "\n",
    "In the example used above the Ishigami function is very cheap to\n",
    "evaluate. However, in most real scenarios the functions of interest are\n",
    "expensive and we need to limit ourselves to a few number of evaluations.\n",
    "Using Monte Carlo methods is infeasible in these scenarios as a large\n",
    "number of samples are typically required to provide good estimates of\n",
    "the Sobol coefficients.\n",
    "\n",
    "An alternative in these cases is to use Gaussaian process emulator of\n",
    "the function of interest trained on a few inputs and outputs. If the\n",
    "model is properly trained, its mean prediction which is cheap to\n",
    "evaluate, can be used to compute the Monte Carlo estimates of the Sobol\n",
    "coefficients. Let’s see how we can do this in Emukit.\n",
    "\n",
    "We start by generating 100 samples in the input domain. Note that this a\n",
    "just 1% of the number of samples that we used to compute the Sobol\n",
    "coefficients using Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.initial_designs import RandomDesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desing = RandomDesign(space)\n",
    "x = desing.get_samples(500)\n",
    "y = ishigami.fidelity1(x)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit a standard Gaussian process to the samples and we wrap it as\n",
    "an Emukit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "from emukit.sensitivity.monte_carlo import MonteCarloSensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpy = GPRegression(x,y)\n",
    "model_emukit = GPyModelWrapper(model_gpy)\n",
    "model_emukit.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to compute the coefficients using the class\n",
    "`ModelBasedMonteCarloSensitivity` which directly calls the model and\n",
    "uses its predictive mean to compute the Monte Carlo estimates of the\n",
    "Sobol indices. We plot the true estimates, those computed using 10000\n",
    "direct evaluations of the objecte using Monte Carlo and those computed\n",
    "using a Gaussian process model trained on 100 evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senstivity_ishigami_gpbased = MonteCarloSensitivity(model = model_emukit, input_domain = space)\n",
    "main_effects_gp, total_effects_gp, _ = senstivity_ishigami_gpbased.compute_effects(num_monte_carlo_points = num_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_effects_gp = {ivar: main_effects_gp[ivar][0] for ivar in main_effects_gp}\n",
    "\n",
    "d = {'Sobol True': ishigami.main_effects,\n",
    "     'Monte Carlo': main_effects,\n",
    "     'GP Monte Carlo':main_effects_gp}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar',figsize=(12, 5))\n",
    "plt.title('First-order Sobol indexes - Ishigami')\n",
    "plt.ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='first-order-sobol-indices-gp-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/first-order-sobol-indices-gp-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>First Order sobol indices as estimated by Monte Carlo and\n",
    "GP-emulator based Monte Carlo.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_effects_gp = {ivar: total_effects_gp[ivar][0] for ivar in total_effects_gp}\n",
    "\n",
    "d = {'Sobol True': ishigami.total_effects,\n",
    "     'Monte Carlo': total_effects,\n",
    "     'GP Monte Carlo':total_effects_gp}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar', ax=ax)\n",
    "ax.set_title('Total effects - Ishigami')\n",
    "ax.set_ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='total-effects-sobol-indices-gp-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/total-effects-sobol-indices-gp-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Total effects as estimated by Monte Carlo and GP based Monte\n",
    "Carlo.</i>\n",
    "\n",
    "We observe some discrepacies with respect to the real value of the\n",
    "coefficient when using the Gaussian process but we get a fairly good a\n",
    "approximation a very reduced number of evaluations of the original\n",
    "target function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "-----------\n",
    "\n",
    "The Sobol indexes are a tool for explaining the variance of the output\n",
    "of a function as components of the input variables. Monte Carlo is an\n",
    "approach for computing these indexes if the function is cheap to\n",
    "evaluate. Other approaches will be needed if $f(\\cdot)$ is expensive to\n",
    "compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ishigami, T., Homma, T., 1989. An importance quantification technique in\n",
    "uncertainty analysis for computer models. \\[1990\\] Proceedings. First\n",
    "International Symposium on Uncertainty Modeling and Analysis 398–403.\n",
    "\n",
    "Kennedy, M.C., O’Hagan, A., 2000. Predicting the output from a complex\n",
    "computer code when fast approximations are available. Biometrika 87,\n",
    "1–13.\n",
    "\n",
    "Saltelli, A., Annoni, P., Azzini, I., Campolongo, F., Ratto, M.,\n",
    "Tarantola, S., 2010. Variance based sensitivity analysis of model\n",
    "output. Design and estimator for the total sensitivity index. Computer\n",
    "Physics Communications 181, 259–270.\n",
    "<https://doi.org/10.1016/j.cpc.2009.09.018>\n",
    "\n",
    "Saltelli, A., Ratto, M., Andres, T., Campolongo, F., Cariboni, J.,\n",
    "Gatelli, D., Saisana, M., Tarantola, S., 2008. Global sensitivity\n",
    "analysis: The primer. wiley.\n",
    "\n",
    "Saltelli, A., Tarantola, S., Campolongo, F., Ratto, M., 2004.\n",
    "Sensitivity analysis in practice: A guide to assessing scientific\n",
    "methods. wiley.\n",
    "\n",
    "Sobol, I.M., 2001. Global sensitivity indices for nonlinear mathematical\n",
    "models and their Monte Carlo estimates. Mathematics and Computers in\n",
    "Simulation 55, 271–280. <https://doi.org/10.1016/S0378-4754(00)00270-6>\n",
    "\n",
    "Sobol, I.M., 1990. On sensitivity estimation for nonlinear mathematical\n",
    "models. Matematicheskoe Modelirovanie 2, 112–118.\n",
    "\n",
    "Sobol, I.M., Levitan, Y.L., 1999. On the use of variance reducing\n",
    "multipliers in Monte Carlo computations of a global sensitivity index.\n",
    "Computer Physics Communications 117, 52–61.\n",
    "<https://doi.org/10.1016/S0010-4655(98)00156-8>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
