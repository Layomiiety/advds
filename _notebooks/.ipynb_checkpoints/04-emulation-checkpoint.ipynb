{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emulation\n",
    "=========\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2020-10-29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In this lecture we motivate the use of emulation, and\n",
    "introduce the GPy software as a framework for building Gaussian process\n",
    "emulators.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emulation\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment, Analyze, Design\n",
    "---------------------------\n",
    "\n",
    "One thing about working in an industrial environment, is the way that\n",
    "short term thinking actions become important. For example, in Formula\n",
    "One, the teams are working on a two week cycle to digest information\n",
    "from the previous week’s race and incorporate updates to the car or\n",
    "their strategy.\n",
    "\n",
    "However, businesses have to also think about more medium term horizons.\n",
    "For example, in Formula 1 you need to worry about next year’s car. So\n",
    "while you’re working on updating this year’s car, you also need to think\n",
    "about what will happen for next year and prioritise these conflicting\n",
    "needs appropriately.\n",
    "\n",
    "In the Amazon supply chain, there are the equivalent demands. If we\n",
    "accept that an artificial intelligence is just an automated decision\n",
    "making system. And if we measure in terms of money automatically spent,\n",
    "or goods automatically moved, then Amazon’s buying system is perhaps the\n",
    "world’s largest AI.\n",
    "\n",
    "Those decisions are being made on short time schedules, purchases are\n",
    "made by the system on weekly cycles. But just as in Formula 1, there is\n",
    "also a need to think about what needs to be done next month, next\n",
    "quarter and next year. Planning meetings are held not only on a weekly\n",
    "basis (known as weekly business reviews), but monthly, quarterly and\n",
    "then yearly meetings for planning spends and investments.\n",
    "\n",
    "Amazon is known for being longer term thinking than many companies, and\n",
    "a lot of this is coming from the CEO. One quote from Jeff Bezos that\n",
    "stuck with me was the following.\n",
    "\n",
    "> “I very frequently get the question: ‘What’s going to change in the\n",
    "> next 10 years?’ And that is a very interesting question; it’s a very\n",
    "> common one. I almost never get the question: ‘What’s not going to\n",
    "> change in the next 10 years?’ And I submit to you that that second\n",
    "> question is actually the more important of the two – because you can\n",
    "> build a business strategy around the things that are stable in time. …\n",
    "> \\[I\\]n our retail business, we know that customers want low prices,\n",
    "> and I know that’s going to be true 10 years from now. They want fast\n",
    "> delivery; they want vast selection. It’s impossible to imagine a\n",
    "> future 10 years from now where a customer comes up and says, ‘Jeff I\n",
    "> love Amazon; I just wish the prices were a little higher,’ \\[or\\] ‘I\n",
    "> love Amazon; I just wish you’d deliver a little more slowly.’\n",
    "> Impossible. And so the effort we put into those things, spinning those\n",
    "> things up, we know the energy we put into it today will still be\n",
    "> paying off dividends for our customers 10 years from now. When you\n",
    "> have something that you know is true, even over the long term, you can\n",
    "> afford to put a lot of energy into it.”\n",
    "\n",
    "This quote is incredibly important for long term thinking. Indeed, it’s\n",
    "a failure of many of our simulations that they focus on what is going to\n",
    "happen, not what will not happen. In Amazon, this meant that there was\n",
    "constant focus on these three areas, keeping costs low, making delivery\n",
    "fast and improving selection. For example, shortly before I left Amazon\n",
    "moved its entire US network from 2 day delivery to 1 day delivery. This\n",
    "involves changing the way the entire buying system operates. Or, more\n",
    "recently, the company has had to radically change the portfolio of\n",
    "products it buys in the face of Covid19.\n",
    "\n",
    "<!--These challenges are not just there for Amazon and Formula 1. In Sheffield, we worked closely with a Chesterfield based company called Fusion Group. They make joints that fuse PTFE pipes together. These pipes are used for transporting both water and gas. Their founder, Eric Bridgstock, was an engineer who introduced PTFE piping to the UK when working for DuPont. Eric set up Fusion group to manufacture the fusion fittings. Because PTFE pipes carry water or gas at high pressure, when these fittings fail significant damage can occur. When these fittings were originally installed in the early 1980s, the job was done by a specialist, but nowadays the pipe weld is compelted by the same team that digs the hole. While costs have come down, the number of PTFE weld failures went up. Eric's company focussed on new systems for auto-->\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/ml/experiment-analyze-design.svg\" class=\"\" width=\"50%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The experiment, analyze, design flywheel for scientific\n",
    "innovation.</i>\n",
    "\n",
    "From the perspective of the team we had in the supply chain, we looked\n",
    "at what we most needed to focus on. Amazon moves very quickly, but we\n",
    "could also take a leaf out of Jeff’s book, and instead of worrying about\n",
    "what was going to change, remember what wasn’t going to change.\n",
    "\n",
    "> We don’t know what science we’ll want to do in 5 years time, but we\n",
    "> won’t want slower experiments, we won’t want more expensive\n",
    "> experiments and we won’t want a narrower selection of experiments.\n",
    "\n",
    "As a result, our focus was on how to speed up the process of\n",
    "experiments, increase the diversity of experiments that we can do, and\n",
    "keep the experiments price as low as possible.\n",
    "\n",
    "The faster the innovation fly-wheel can be iterated, then the quicker we\n",
    "can ask about different parts of the supply chain, and the better we can\n",
    "tailor systems to answering those questions.\n",
    "\n",
    "As a result our objective became a two-order magnitude increase in\n",
    "number of experiments run across a five year period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Emulation\n",
    "---------------------\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/simulation/unified_model_systems_13022018_1920.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>The UK Met office runs a shared code base for its simulations\n",
    "of climate and the weather. This plot shows the different spatial and\n",
    "temporal scales used.</i>\n",
    "\n",
    "In many real world systems, decisions are made through simulating the\n",
    "environment. Simulations may operate at different granularities. For\n",
    "example, simulations are used in weather forecasts and climate\n",
    "forecasts. Interestingly, the UK Met office uses the same code for both,\n",
    "it has a [“Unified Model”\n",
    "approach](https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/index),\n",
    "but they operate climate simulations one at greater spatial and temporal\n",
    "resolutions.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/statistical-emulation000.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Real world systems consist of simulators that capture our\n",
    "domain knowledge about how our systems operate. Different simulators run\n",
    "at different speeds and granularities.</i>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/statistical-emulation001.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A statistical emulator is a system that reconstructs the\n",
    "simulation with a statistical model.</i>\n",
    "\n",
    "A statistical emulator is a data-driven model that learns about the\n",
    "underlying simulation. Importantly, learns with uncertainty, so it\n",
    "‘knows what it doesn’t know’. In practice, we can call the emulator in\n",
    "place of the simulator. If the emulator ‘doesn’t know’, it can call the\n",
    "simulator for the answer.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/statistical-emulation004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A statistical emulator is a system that reconstructs the\n",
    "simulation with a statistical model. As well as reconstructing the\n",
    "simulation, a statistical emulator can be used to correlate with the\n",
    "real world.</i>\n",
    "\n",
    "As well as reconstructing an individual simulator, the emulator can\n",
    "calibrate the simulation to the real world, by monitoring differences\n",
    "between the simulator and real data. This allows the emulator to\n",
    "characterise where the simulation can be relied on, i.e. we can validate\n",
    "the simulator.\n",
    "\n",
    "Similarly, the emulator can adjudicate between simulations. This is\n",
    "known as *multi-fidelity emulation*. The emulator characterizes which\n",
    "emulations perform well where.\n",
    "\n",
    "If all this modelling is done with judiscious handling of the\n",
    "uncertainty, the *computational doubt*, then the emulator can assist in\n",
    "desciding what experiment should be run next to aid a decision: should\n",
    "we run a simulator, in which case which one, or should we attempt to\n",
    "acquire data from a real world intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy: A Gaussian Process Framework in Python\n",
    "-------------------------------------------\n",
    "\n",
    "Gaussian processes are a flexible tool for non-parametric analysis with\n",
    "uncertainty. The GPy software was started in Sheffield to provide a easy\n",
    "to use interface to GPs. One which allowed the user to focus on the\n",
    "modelling rather than the mathematics.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/gpy.png\" style=\"width:70%\">\n",
    "\n",
    "Figure: <i>GPy is a BSD licensed software code base for implementing\n",
    "Gaussian process models in Python. It is designed for teaching and\n",
    "modelling. We welcome contributions which can be made through the Github\n",
    "repository\n",
    "<a href=\"https://github.com/SheffieldML/GPy\" class=\"uri\">https://github.com/SheffieldML/GPy</a></i>\n",
    "\n",
    "GPy is a BSD licensed software code base for implementing Gaussian\n",
    "process models in python. This allows GPs to be combined with a wide\n",
    "variety of software libraries.\n",
    "\n",
    "The software itself is available on\n",
    "[GitHub](https://github.com/SheffieldML/GPy) and the team welcomes\n",
    "contributions.\n",
    "\n",
    "The aim for GPy is to be a probabilistic-style programming language,\n",
    "i.e. you specify the model rather than the algorithm. As well as a large\n",
    "range of covariance functions the software allows for non-Gaussian\n",
    "likelihoods, multivariate outputs, dimensionality reduction and\n",
    "approximations for larger data sets.\n",
    "\n",
    "The documentation for GPy can be found\n",
    "[here](https://gpy.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy Tutorial\n",
    "------------\n",
    "\n",
    "This GPy tutorial is based on material we share in the Gaussian process\n",
    "summer school for teaching these models\n",
    "<a href=\"https://gpss.cc\" class=\"uri\">https://gpss.cc</a>. It contains\n",
    "material from various members and former members of the Sheffield\n",
    "machine learning group, but particular mention should be made of\n",
    "[Nicolas\n",
    "Durrande](https://sites.google.com/site/nicolasdurrandehomepage/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a feel for the sofware we’ll start by creating an exponentiated\n",
    "quadratic covariance function, $$\n",
    "k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{ x}- \\mathbf{ x}^\\prime \\right\\Vert_2^2}{2\\ell^2}\\right),\n",
    "$$ where the length scale is $\\ell$ and the variance is $\\alpha$.\n",
    "\n",
    "To set this up in GPy we create a kernel in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=1\n",
    "alpha = 1.0\n",
    "lengthscale = 2.0\n",
    "kern = GPy.kern.RBF(input_dim=input_dim, variance=alpha, lengthscale=lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That builds a kernel object for us. The kernel can be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or because it’s one dimensional, you can also plot the kernel as a\n",
    "function of its inputs (while the other is fixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "kern.plot(ax=ax)\n",
    "mlai.write_figure('gpy-eq-covariance.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-covariance.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function as plotted by\n",
    "the `GPy.kern.plot` command.</i>\n",
    "\n",
    "You can set the lengthscale of the covariance to different values and\n",
    "plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(input_dim=input_dim)     # By default, the parameters are set to 1.\n",
    "lengthscales = np.asarray([0.2,0.5,1.,2.,4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "for lengthscale in lengthscales:\n",
    "    kern.lengthscale=lengthscale\n",
    "    kern.plot(ax=ax)\n",
    "\n",
    "ax.legend(lengthscales)\n",
    "mlai.write_figure('gpy-eq-covariance-lengthscales.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-covariance-lengthscales.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The exponentiated quadratic covariance function plotted for\n",
    "different lengthscales by `GPy.kern.plot` command.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance Functions in GPy\n",
    "---------------------------\n",
    "\n",
    "Many covariance functions are already implemented in GPy. Instead of\n",
    "rbf, try constructing and plotting the following covariance functions:\n",
    "`exponential`, `Matern32`, `Matern52`, `Brownian`, `linear`, `bias`,\n",
    "`rbfcos`, `periodic_Matern32`, etc. Some of these covariance functions,\n",
    "such as `rbfcos`, are not parametrized by a variance and a lengthscale.\n",
    "Furthermore, not all kernels are stationary (i.e., they can’t all be\n",
    "written as\n",
    "$k(\\mathbf{ x}, \\mathbf{ x}^\\prime) = f (\\mathbf{ x}-\\mathbf{ x}^\\prime)$,\n",
    "see for example the Brownian covariance function). For plotting so it\n",
    "may be interesting to change the value of the fixed input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "brownian_kern = GPy.kern.Brownian(input_dim=1)\n",
    "inputs = np.array([2., 4.])\n",
    "for x in inputs:\n",
    "    brownian_kern.plot(x,plot_limits=[0,5], ax=ax)\n",
    "ax.legend(inputs)\n",
    "ax.set_ylim(-0.1,5.1)\n",
    "\n",
    "mlai.write_figure('gpy-brownian-covariance-lengthscales.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Covariance Functions in GPy\n",
    "-------------------------------------\n",
    "\n",
    "In GPy you can easily combine covariance functions you have created\n",
    "using the sum and product operators, `+` and `*`. So, for example, if we\n",
    "wish to combine an exponentiated quadratic covariance with a Matern 5/2\n",
    "then we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(1, variance=1., lengthscale=2.)\n",
    "kern2 = GPy.kern.Matern52(1, variance=2., lengthscale=4.)\n",
    "kern = kern1 + kern2\n",
    "display(kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "kern.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('gpy-eq-plus-matern52-covariance.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-plus-matern52-covariance.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A combination of the exponentiated quadratic covariance plus\n",
    "the Matern $5/2$ covariance.</i>\n",
    "\n",
    "Or if we wanted to multiply them we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(1, variance=1., lengthscale=2.)\n",
    "kern2 = GPy.kern.Matern52(1, variance=2., lengthscale=4.)\n",
    "kern = kern1 * kern2\n",
    "display(kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "kern.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('gpy-eq-times-matern52-covariance.svg', directory='./kern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/kern/gpy-eq-times-matern52-covariance.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A combination of the exponentiated quadratic covariance\n",
    "multiplied by the Matern $5/2$ covariance.</i>\n",
    "\n",
    "You can learn about how to implement [new kernel objects in GPy\n",
    "here](https://gpy.readthedocs.io/en/latest/tuto_creating_new_kernels.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('-sY8zW3Om1Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Designing the covariance function for your Gaussian process\n",
    "is a key place in which you introduce your understanding of the data\n",
    "problem. To learn more about the design of covariance functions, see\n",
    "this talk from GPSS in 2016.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian Process Regression Model\n",
    "-----------------------------------\n",
    "\n",
    "We will now combine the Gaussian process prior with some data to form a\n",
    "GP regression model with GPy. We will generate data from the function $$\n",
    "f( x) = − \\cos(\\pi x) + \\sin(4\\pi x)\n",
    "$$ over the domain $[0, 1]$, adding some noise to gives $$\n",
    "y(x) = f(x) + \\epsilon,\n",
    "$$ with the noise being Gaussian distributed,\n",
    "$\\epsilon\\sim \\mathcal{N}\\left(0,0.01\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0.05,0.95,10)[:,np.newaxis]\n",
    "Y = -np.cos(np.pi*X) + np.sin(4*np.pi*X) + np.random.normal(loc=0.0, scale=0.1, size=(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(X,Y,'kx',mew=1.5, linewidth=2)\n",
    "\n",
    "mlai.write_figure('noisy-sine.svg', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/noisy-sine.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Data from the noisy sine wave for fitting with a GPy\n",
    "model.</i>\n",
    "\n",
    "A GP regression model based on an exponentiated quadratic covariance\n",
    "function can be defined by first defining a covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then combining it with the data to form a Gaussian process model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPy.models.GPRegression(X,Y,kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for the covariance function object, we can find out about the\n",
    "model using the command `display(model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default the model includes some observation noise with\n",
    "variance 1. We can see the posterior mean prediction and visualize the\n",
    "marginal posterior variances using `model.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "model.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('noisy-sine-gp-fit.svg', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/noisy-sine-gp-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian process fit to the noisy sine data. Here the\n",
    "parameters of the process and the covariance function haven’t yet been\n",
    "optimized.</i>\n",
    "\n",
    "You can also look directly at the predictions for the model using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstar = np.linspace(0, 10, 100)[:, np.newaxis]\n",
    "Ystar, Vstar = model.predict(Xstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives you the mean (`Ystar`), the variance (`Vstar`) at the\n",
    "locations given by `Xstar`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance Function Parameter Estimation\n",
    "----------------------------------------\n",
    "\n",
    "As we have seen during the lectures, the parameters values can be\n",
    "estimated by maximizing the likelihood of the observations. Since we\n",
    "don’t want one of the variance to become negative during the\n",
    "optimization, we can constrain all parameters to be positive before\n",
    "running the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.constrain_positive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warnings are because the parameters are already constrained by\n",
    "default, the software is warning us that they are being reconstrained.\n",
    "\n",
    "Now we can optimize the model using the `model.optimize()` method. Here\n",
    "we switch messages on, which allows us to see the progession of the\n",
    "optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the optimization is using a limited memory BFGS optimizer\n",
    "(Byrd et al., 1995).\n",
    "\n",
    "Once again we can display the model, now to see how the parameters have\n",
    "changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengthscale is much smaller, as well as the noise level. The\n",
    "variance of the exponentiated quadratic has also reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "model.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('noisy-sine-gp-optimized-fit.svg', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/noisy-sine-gp-optimized-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian process fit to the noisy sine data with parameters\n",
    "optimized.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy and Emulation\n",
    "-----------------\n",
    "\n",
    "Let $\\mathbf{ x}$ be a random variable defined over the real numbers,\n",
    "$\\Re$, and $f(\\cdot)$ be a function mapping between the real numbers\n",
    "$\\Re \\rightarrow \\Re$.\n",
    "\n",
    "The problem of *uncertainty propagation* is the study of the\n",
    "distribution of the random variable $f(\\mathbf{ x})$.\n",
    "\n",
    "We’re going to address this problem using emulation and GPy. We will see\n",
    "in this section the advantage of using a model when only a few\n",
    "observations of $f$ are available.\n",
    "\n",
    "Firstly we’ll make use of a test function known as the Branin test\n",
    "function. $$\n",
    "f(\\mathbf{ x}) = a(x_2 - bx_1^2 + cx_1 - r)^2 + s(1-t \\cos(x_1)) + s\n",
    "$$ where we are setting $a=1$, $b=5.1/(4\\pi^2)$, $c=5/\\pi$, $r=6$,\n",
    "$s=10$ and $t=1/(8\\pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branin(X):\n",
    "    y = ((X[:,1]-5.1/(4*np.pi**2)*X[:,0]**2+5*X[:,0]/np.pi-6)**2 \n",
    "        + 10*(1-1/(8*np.pi))*np.cos(X[:,0])+10)\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll define a grid of twenty five observations over \\[−5, 10\\] × \\[0,\n",
    "15\\] and a set of 25 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set defined as a 5*5 grid:\n",
    "xg1 = np.linspace(-5,10,5)\n",
    "xg2 = np.linspace(0,15,5)\n",
    "X = np.zeros((xg1.size * xg2.size,2))\n",
    "for i,x1 in enumerate(xg1):\n",
    "    for j,x2 in enumerate(xg2):\n",
    "        X[i+xg1.size*j,:] = [x1,x2]\n",
    "\n",
    "Y = branin(X)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to consider the distribution of $f(U)$, where $U$\n",
    "is a random variable with uniform distribution over the input space of\n",
    "$f$. We focus on the computaiton of two quantities, the expectation of\n",
    "$f(U)$, $\\left<f(U)\\right>$, and the probability that the value is\n",
    "greater than 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of $\\left<f(U)\\right>$\n",
    "----------------------------------\n",
    "\n",
    "The expectation of $f(U )$ is given by\n",
    "$\\int_\\mathbf{ x}f( \\mathbf{ x})\\text{d}\\mathbf{ x}$. A basic approach\n",
    "to approximate this integral is to compute the mean of the 25\n",
    "observations: `np.mean(Y)`. Since the points are distributed on a grid,\n",
    "this can be seen as the approximation of the integral by a rough Riemann\n",
    "sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimate of the expectation is given by: {mean}'.format(mean=Y.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can be compared with the actual mean of the Branin function\n",
    "which is 54.31.\n",
    "\n",
    "Alternatively, we can fit a GP model and compute the integral of the\n",
    "best predictor by Monte Carlo sampling.\n",
    "\n",
    "First we create the covariance function. Here we’re going to use an\n",
    "exponentiated quadratic, but we’ll augment it with the ‘bias’ covariance\n",
    "function. This covariance function represents a single fixed bias that\n",
    "is added to the overall covariance. It allows us to deal with\n",
    "non-zero-mean emulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an exponentiated quadratic plus bias covariance function\n",
    "kern_eq = GPy.kern.RBF(input_dim=2, ARD = True)\n",
    "kern_bias = GPy.kern.Bias(input_dim=2)\n",
    "kern = kern_eq + kern_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct the Gaussian process regression model in GPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a GP model\n",
    "model = GPy.models.GPRegression(X,Y,kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sinusoid example above, we learnt the variance of the process.\n",
    "But in this example, we are fitting an emulator to a function we know is\n",
    "noise-free. However, we don’t fix the noise value to precisely zero, as\n",
    "this can lead to some numerical errors. Instead we fix the variance of\n",
    "the Gaussian noise to a very small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the noise variance\n",
    "model.likelihood.variance.fix(1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the model. Note, that the initial values for the lengthscale\n",
    "are not appropriate. So first set the lengthscale of the model needs to\n",
    "be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern.rbf.lengthscale = np.asarray([3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a common error in Gaussian process fitting to initialise the\n",
    "lengthscale too small or too big. The challenge is that the error\n",
    "surface is normally multimodal, and the final solution can be very\n",
    "sensitive to this initialisation. If the lengthscale is initialized too\n",
    "small, the solution can converge on an place where the signal isn’t\n",
    "extracted by the covariance function. If the lengthscale is initialized\n",
    "too large, then the variations of the function are often missing. Here\n",
    "the lengthscale is set for each dimension of inputs as 3. Now that’s\n",
    "done, we can optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the model and optimize\n",
    "model.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "model.plot(ax=ax)\n",
    "\n",
    "mlai.write_figure('branin-gp-optimized-fit.svg', directory='./gp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/gp/branin-sine-gp-optimized-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A Gaussian process fit to the Branin test function, used to\n",
    "assess the mean of the function by emulation.</i>\n",
    "\n",
    "Finally we can compute the mean of the model predictions using very many\n",
    "Monte Carlo samples.\n",
    "\n",
    "Note, that in this example, because we’re using at est function, we\n",
    "could simply have done the Monte Carlo estimation directly on the Branin\n",
    "function. However, imagine inststead that we were trying to understand\n",
    "the results of a complex Computational Fluid Dynamics simulation, where\n",
    "each run of the simulation (which is equivalent to our function) took\n",
    "many hours. In that case the advantage of the emulator is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of model prediction on 1e5 Monte Carlo samples\n",
    "Xp = np.random.uniform(size=(int(1e5),2))\n",
    "Xp[:,0] = Xp[:,0]*15-5\n",
    "Xp[:,1] = Xp[:,1]*15\n",
    "mu, var = m.predict(Xp)\n",
    "print('The estimate of the mean of the Branin function is {mean}'.format(mean=np.mean(mu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Now think about how to make use of the variance estimation from the\n",
    "Gaussian process to obtain error bars around your estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 2 here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "You’ve seen how the Monte Carlo estimates work with the Gaussian\n",
    "process. Now make your estimate of the probability that the Branin\n",
    "function is greater than 200 with the uniform random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 3 here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty Quantification\n",
    "--------------------------\n",
    "\n",
    "We’re introducing you to the optimization and analysis of real world\n",
    "models through emulation, this domain is part of a broader field known\n",
    "as surrogate modelling.\n",
    "\n",
    "Although we’re approaching this from the machine learning perspective,\n",
    "with a computer-scientist’s approach, you won’t be suprised to find out\n",
    "that this field is not new and there are a range of research groups\n",
    "interested in this domain.\n",
    "\n",
    "This type of challenge, of where to run the simulation to get the answer\n",
    "you require is an old challenge. One classic paper, McKay et al. (1979),\n",
    "reviews three different methods for designing these inputs. They are\n",
    "*random sampling*, *stratified sampling* and *latin hypercube sampling*.\n",
    "\n",
    "> Let the input values $x_1, \\dots, x_n$ be a random sample from $f(x)$.\n",
    "> This method of sampling is perhaps the most obvious, and an entire\n",
    "> body of statistical literature may be used in making infer- ences\n",
    "> regarding the distribution of $Y(t)$.\n",
    "\n",
    "> Using stratified sampling, all areas of the sample space of $X$ are\n",
    "> represented by input values. Let the sample space $S$ of $X$ be\n",
    "> partitioned into $I$ disjoint strata $S_t$. Let $\\pi = P(X \\in S_i)$\n",
    "> represent the size of $S_i$. Obtain a random sample $X_{ij}$,\n",
    "> $j = 1, \\dots, n$ from $S_i$. Then of course the $n_i$ sum to $N$. If\n",
    "> $I = 1$, we have random sampling over the entire sample space.\n",
    "\n",
    "> *Latin Hypercube Sampling.* The same reasoning that led to stratified\n",
    "> sampling, ensuring that all por- tions of $S$ were sampled, could lead\n",
    "> further. If we wish to ensure also that each of the input variables\n",
    "> $X_k$ has all portions of its distribution represented by input\n",
    "> values, we can divide the range of each $X_k$ into $N$ strata of equal\n",
    "> marginal probability $1/N$, and sample once from each stratum. Let\n",
    "> this sample be $X_{kj}$, $j = 1, \\dots, N$. These form the $X_k$\n",
    "> component, $k = 1, \\dots , K$, in $X_i$, $i = 1, \\dots , N$. The\n",
    "> components of the various $X_k$’s are matched at random. This method\n",
    "> of selecting input values is an extension of quota sam- pling\n",
    "> (Steinberg 1963), and can be viewed as a K-dimensional extension of\n",
    "> Latin square sampling (Raj 1968).\n",
    "\n",
    "The paper’s rather dated refernce to “Output from a Computer Code” does\n",
    "carry forward through this literature, which has continued to be a focus\n",
    "of interest for statisticians. Tony O’Hagan, who was a colleague in\n",
    "Sheffield but is also one of the pioneers of Gaussian process models was\n",
    "developing these methods when I first arrived there (Kennedy and\n",
    "O’Hagan, 2001), and continued with a large EPSRC funded project for\n",
    "manacing uncertainty in computaitonal models,\n",
    "<a href=\"http://www.mucm.ac.uk/\" class=\"uri\">http://www.mucm.ac.uk/</a>.\n",
    "You can see a list of [their technical reports\n",
    "here](http://www.mucm.ac.uk/Pages/Dissemination/TechnicalReports.html).\n",
    "\n",
    "Another important group based in France is the “MASCOT-NUM Research\n",
    "Group”,\n",
    "<a href=\"https://www.gdr-mascotnum.fr/\" class=\"uri\">https://www.gdr-mascotnum.fr/</a>.\n",
    "These researchers bring together statisticians, applied mathematicians\n",
    "and engineers in solving these problems.\n",
    "\n",
    "$$\\newcommand{\\sigma}{\\sigma_{\\text{noise}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emukit Playground\n",
    "-----------------\n",
    "\n",
    "Emukit playground is a software toolkit for exploring the use of\n",
    "statistical emulation as a tool. It was built by [Adam\n",
    "Hirst](https://twitter.com/_AdamHirst), during his software engineering\n",
    "internship at Amazon and supervised by Cliff McCollum.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/emukit-playground.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Emukit playground is a tutorial for understanding the\n",
    "simulation/emulation relationship.\n",
    "<a href=\"https://amzn.github.io/emukit-playground/\" class=\"uri\">https://amzn.github.io/emukit-playground/</a></i>\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/emukit-playground-bayes-opt.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Tutorial on Bayesian optimization of the number of taxis\n",
    "deployed from Emukit playground.\n",
    "<a href=\"https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization\" class=\"uri\">https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization</a></i>\n",
    "\n",
    "You can explore Bayesian optimization of a taxi simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byrd, R.H., Lu, P., Nocedal, J., 1995. A limited memory algorithm for\n",
    "bound constrained optimization. SIAM Journal on Scientific and\n",
    "Statistical Computing 16, 1190–1208.\n",
    "\n",
    "Kennedy, M.C., O’Hagan, A., 2001. Bayesian calibration of computer\n",
    "models. Journal of the Royal Statistical Society: Series B (Statistical\n",
    "Methodology) 63, 425–464. <https://doi.org/10.1111/1467-9868.00294>\n",
    "\n",
    "McKay, M.D., Beckman, R.J., Conover, W.J., 1979. A comparison of three\n",
    "methods for selecting values of input variables in the analysis of\n",
    "output from a computer code. Technometrics 21, 239–245."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
