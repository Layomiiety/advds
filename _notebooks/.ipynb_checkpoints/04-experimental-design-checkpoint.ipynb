{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emulation and Experimental Design\n",
    "=================================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2020-10-29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: This lecture will review the ideas behind using surrogate\n",
    "models for experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "%\\newcommand{\\tk}[1]{\\textbf{TK}: #1}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%\n",
    "\n",
    "% Already defined by latex %\n",
    "\n",
    "% Already defined by latex %\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emulation\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Emulation\n",
    "---------------------\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/statistical-emulation000.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Real world systems consiste of simulators, that capture our\n",
    "domain knowledge about how our systems operate. Different simulators run\n",
    "at different speeds and granularities.</i>\n",
    "\n",
    "In many real world systems, decisions are made through simulating the\n",
    "environment. Simulations may operate at different granularities. For\n",
    "example, simulations are used in weather forecasts and climate\n",
    "forecasts. The UK Met office uses the same code for both, but operates\n",
    "climate simulations one at greater spatial and temporal resolutions.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/statistical-emulation001.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A statistical emulator is a system that reconstructs the\n",
    "simulation with a statistical model.</i>\n",
    "\n",
    "A statistical emulator is a data-driven model that learns about the\n",
    "underlying simulation. Importantly, learns with uncertainty, so it\n",
    "‘knows what it doesn’t know’. In practice, we can call the emulator in\n",
    "place of the simulator. If the emulator ‘doesn’t know’, it can call the\n",
    "simulator for the answer.\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/statistical-emulation004.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A statistical emulator is a system that reconstructs the\n",
    "simulation with a statistical model. As well as reconstructing the\n",
    "simulation, a statistical emulator can be used to correlate with the\n",
    "real world.</i>\n",
    "\n",
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/statistical-emulation005.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>In modern machine learning system design, the emulator may\n",
    "also consider the output of ML models (for monitoring bias or accuracy)\n",
    "and Operations Research models..</i>\n",
    "\n",
    "As well as reconstructing an individual simulator, the emulator can\n",
    "calibrate the simulation to the real world, by monitoring differences\n",
    "between the simulator and real data. This allows the emulator to\n",
    "characterise where the simulation can be relied on, i.e. we can validate\n",
    "the simulator.\n",
    "\n",
    "Similarly, the emulator can adjudicate between simulations. This is\n",
    "known as *multi-fidelity emulation*. The emulator characterizes which\n",
    "emulations perform well where.\n",
    "\n",
    "If all this modelling is done with judiscious handling of the\n",
    "uncertainty, the *computational doubt*, then the emulator can assist in\n",
    "desciding what experiment should be run next to aid a decision: should\n",
    "we run a simulator, in which case which one, or should we attempt to\n",
    "acquire data from a real world intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emukit Playground\n",
    "================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emukit Playground\n",
    "-----------------\n",
    "\n",
    "Emukit playground is a software toolkit for exploring the use of\n",
    "statistical emulation as a tool. It was built by [Adam\n",
    "Hirst](https://twitter.com/_AdamHirst), during his software engineering\n",
    "internship at Amazon and supervised by Cliff McCollum.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/emukit-playground.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Emukit playground is a tutorial for understanding the\n",
    "simulation/emulation relationship.\n",
    "<a href=\"https://amzn.github.io/emukit-playground/\" class=\"uri\">https://amzn.github.io/emukit-playground/</a></i>\n",
    "\n",
    "<img class=\"negate\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/emukit-playground-bayes-opt.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>Tutorial on Bayesian optimization of the number of taxis\n",
    "deployed from Emukit playground.\n",
    "<a href=\"https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization\" class=\"uri\">https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization</a></i>\n",
    "\n",
    "You can explore Bayesian optimization of a taxi simulation.\n",
    "\n",
    "Related publications and links will appear here.\n",
    "\n",
    "Emukit\n",
    "https://nbviewer.jupyter.org/github/amzn/emukit/blob/master/notebooks/index.ipynb\n",
    "Emukit Playground: https://amzn.github.io/emukit-playground/\\#!/\n",
    "\n",
    "Examle paper: McKay et al. (1979) Kennedy and O’Hagan (2001)\n",
    "\n",
    "Random Sampling. Let the input values X1, \\* \\* , XN be a random sample\n",
    "from F(x). This method of sam- pling is perhaps the most obvious, and an\n",
    "entire body of statistical literature may be used in making infer- ences\n",
    "regarding the distribution of Y(t). Stratified Sampling. Using\n",
    "stratified sampling, all areas of the sample space of X are represented\n",
    "by input values. Let the sample space S of X be parti- tioned into I\n",
    "disjoint strata St. Let pi = P(X C Si) represent the size of Si. Obtain\n",
    "a random sample XiJ,j = 1, \\* \\* , n from Si. Then of course the ni sum\n",
    "to N. If I = 1, we have random sampling over the entire sample space.\n",
    "Latin Hypercube Sampling. The same reasoning that led to stratified\n",
    "sampling, ensuring that all por- tions of S were sampled, could lead\n",
    "further. If we wish to ensure also that each of the input variables Xk\n",
    "has all portions of its distribution represented by input values, we can\n",
    "divide the range of each Xk into N strata of equal marginal probability\n",
    "1/N, and sample once from each stratum. Let this sample be Xkj,j = 1, …,\n",
    "N. These form the Xk component, k = 1, \\* , K, in Xi, i = 1, \\* , N. The\n",
    "components of the various X,A’s are matched at random. This method of\n",
    "selecting input values is an extension of quota sam- pling \\[13\\], and\n",
    "can be viewed as a K-dimensional extension of Latin square sampling\n",
    "\\[11\\]. One advantage of the Latin hypercube sample ap- pears when the\n",
    "output Y(t) is dominated by only a few of the components of X. This\n",
    "method ensures that each of those components is represented in a fully\n",
    "stratified manner, no matter which components might turn out to be\n",
    "important. We mention here that the N intervals on the range of each\n",
    "component of X combine to form NK cells which cover the sample space of\n",
    "X. These cells, which are labeled by coordinates corresponding to the\n",
    "inter- vals, are used when finding the properties of the sampling plan.\n",
    "\n",
    "This introduction is based on [An Introduction to Experimental Design\n",
    "with\n",
    "Emukit](https://github.com/EmuKit/emukit/blob/master/notebooks/Emukit-tutorial-experimental-design-introduction.ipynb)\n",
    "written by Andrei Paleyes and Maren Mahsereci.\n",
    "\n",
    "Experimental design.\n",
    "\n",
    "Latin hypercube\n",
    "\n",
    "Linear example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.test_functions import forrester_function\n",
    "from emukit.core.loop.user_function import UserFunctionWrapper\n",
    "from emukit.core import ContinuousParameter, ParameterSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_function, space = forrester_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(space.parameters[0].min, space.parameters[0].max, 301)[:, None]\n",
    "y_plot = target_function(x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x_plot, y_plot, \"k\", label=\"target Function\")\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "mlai.write_figure(filename='forrester-function.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/forrester-function.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Forrester function (Forrester et al., 2008).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Design\n",
    "--------------\n",
    "\n",
    "Usually, before we start the actual ExpDesign loop we need to gather a\n",
    "few observations such that we can fit the model. This is called the\n",
    "initial design and common strategies are either a predefined grid or\n",
    "sampling points uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_init = np.array([[0.2],[0.6], [0.9]])\n",
    "Y_init = target_function(X_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(X_init, Y_init, \"ro\", markersize=10, label=\"Observations\")\n",
    "ax.plot(x_plot, y_plot, \"k\", label=\"Target Function\")\n",
    "\n",
    "ax.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "mlai.write_figure(filename='forrester-function-initial-design.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/forrester-function-initial-design.svg.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The initial design for the Forrester function example.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model\n",
    "---------\n",
    "\n",
    "Now we can start with the ExpDesign loop by first fitting a model on the\n",
    "collected data. A popular model for ExpDesign is a Gaussian process (GP)\n",
    "which defines a probability distribution across classes of functions,\n",
    "typically smooth, such that each linear finite-dimensional restriction\n",
    "is multivariate Gaussian (Rasmussen and Williams,\n",
    "2006)(\\#4.-References). GPs are fully parametrized by a mean\n",
    "$\\mu(\\mathbf{ x})$ and a covariance function\n",
    "$k(\\mathbf{ x},\\mathbf{ x}')$. Without loss of generality\n",
    "$\\mu(\\mathbf{ x})$ is assumed to be zero. The covariance function\n",
    "$k(\\mathbf{ x},\\mathbf{ x}')$ characterizes the smoothness and other\n",
    "properties of $f$. It is known that the kernel of the process has to be\n",
    "continuous, symmetric and positive definite. A widely used kernel is the\n",
    "squared exponential or RBF kernel:\n",
    "$$ k(\\mathbf{ x},\\mathbf{ x}') = \\theta_0 \\cdot \\exp{ \\left(-\\frac{\\|\\mathbf{ x}-\\mathbf{ x}'\\|^2}{\\theta_1}\\right)} $$\n",
    "where $\\theta_0$ and and $\\theta_1$ are hyperparameters. To denote that\n",
    "$f$ is a sample from a GP with mean $\\mu$ and covariance $k$ we write $$\n",
    "f\\sim \\mathcal{GP}(\\mu,k).\n",
    "$$\n",
    "\n",
    "For regression tasks, the most important feature of GPs is that process\n",
    "priors are conjugate to the likelihood from finitely many observations\n",
    "$\\mathbf{Y}= (y_1,\\dots,y_n)^T$ and\n",
    "$\\mathbf{X}=\\{\\mathbf{ x}_1,...,\\mathbf{ x}_n\\}$,\n",
    "$\\mathbf{ x}_i\\in \\mathcal{X}$ of the form\n",
    "$y_i = f(\\mathbf{ x}_i) + \\epsilon_i$ where\n",
    "$\\epsilon_i \\sim \\mathcal{N} (0,\\sigma_{noise}^2)$ and we estimate\n",
    "$\\sigma_{noise}$ by an additional hyperparameter $\\theta_2$. We obtain\n",
    "the Gaussian posterior\n",
    "$f(\\mathbf{ x}^*)|\\mathbf{X}, \\mathbf{Y}, \\theta \\sim \\mathcal{N}(\\mu(\\mathbf{ x}^*),\\sigma^2(\\mathbf{ x}^*))$,\n",
    "where $\\mu(\\mathbf{ x}^*)$ and $\\sigma^2(\\mathbf{ x}^*)$ have a close\n",
    "form. See Rasmussen and Williams (2006) for more details.\n",
    "\n",
    "Note that Gaussian processes are also characterized by hyperparameters\n",
    "$\\theta = \\{\\theta_0, ... \\theta_k\\}$ such as for instance the kernel\n",
    "lengthscales. For simplicitly we keep these hyperparameters fixed here.\n",
    "However, we usually either optimize or sample these hyperparameters\n",
    "using the marginal loglikelihood of the GP. Of course we could also use\n",
    "any other model that returns a mean $\\mu(\\mathbf{ x})$ and variance\n",
    "$\\sigma^2(\\mathbf{ x})$ on an arbitrary input points $\\mathbf{ x}$ such\n",
    "as Bayesian neural networks or random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "from emukit.model_wrappers.gpy_model_wrappers import GPyModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(1, lengthscale=0.08, variance=20)\n",
    "gpy_model = GPy.models.GPRegression(X_init, Y_init, kern, noise_var=1e-10)\n",
    "emukit_model = GPyModelWrapper(gpy_model)\n",
    "\n",
    "mu_plot, var_plot = emukit_model.predict(x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(X_init, Y_init, \"ro\", markersize=10, label=\"Observations\")\n",
    "ax.plot(x_plot, y_plot, \"k\", label=\"Objective Function\")\n",
    "ax.plot(x_plot, mu_plot, \"C0\", label=\"Model\")\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.6)\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 2 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 2 * np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.4)\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 3 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 3 * np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.2)\n",
    "ax.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "mlai.write_figure(filename='forrester-function-entropy.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/forrester-function-entropy.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The entropy of the fit to the Forrester function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Acquisition Function\n",
    "------------------------\n",
    "\n",
    "In the second step of our ExpDesign loop we use our model to compute the\n",
    "acquisition function. Two example of ExpDesign acquisition functions\n",
    "are:\n",
    "\n",
    "**Uncertainty Sampling (US)**: Choose the next value $x_{n+1}$ at the\n",
    "location where the model on $f(x)$ has the highest marginal predictive\n",
    "variance\n",
    "\n",
    "$$\n",
    "a_{US}(x) = \\sigma^2(x)\n",
    "$$\n",
    "\n",
    "This makes sure, that we learn the function $f$ everywhere on\n",
    "$\\mathbb{X}$ to a similar level of absolute error.\n",
    "\n",
    "**Integrated variance reduction (IVR)**: Choose the next value $x_{n+1}$\n",
    "such that the total variance of the model is reduced maximally [\\[Sacks\n",
    "et al. 1989\\]](#4.-References).\n",
    "\n",
    "$$\n",
    "a_{IVR} = \\int_{\\mathbb{X}}[\\sigma^2(x') - \\sigma^2(x'; x)]\\mathrm{d}x'\\approx \n",
    "\\frac{1}{\\# samples}\\sum_i^{\\# samples}[\\sigma^2(x_i) - \\sigma^2(x_i; x)].\n",
    "$$\n",
    "\n",
    "Here $\\sigma^2(x'; x)$ is the predictive variance at $x'$ had $x$ been\n",
    "observed. Thus IVR compute the overall reduction in variance (for all\n",
    "points in $\\mathbb{X}$) had $f$ been observed at $x$. The finite sum\n",
    "approximation on the right hand side of the equation is usually used\n",
    "because the integral over $x'$ is not analytic. In that case $x_i$ are\n",
    "sampled randomly. For a GP model the right hand side simplifies to\n",
    "$a_{LCB} \\approx \\frac{1}{\\# samples}\\sum_i^{\\# samples}\\frac{k^2(x_i, x)}{\\sigma^2(x)}$.\n",
    "\n",
    "IVR is arguably te more principled approach, but often US is preferred\n",
    "over IVR simply because it lends itself to gradient based optimization\n",
    "more easily, is cheaper to compute, and is exact. For both of them\n",
    "(stochastic) gradient base optimizers are used to retrieve\n",
    "$x_{n+1} \\in \\operatorname*{arg\\:max}_{x \\in \\mathbb{X}} a(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.experimental_design.acquisitions import IntegratedVarianceReduction, ModelVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_acquisition = ModelVariance(emukit_model)\n",
    "ivr_acquisition = IntegratedVarianceReduction(emukit_model, space)\n",
    "\n",
    "us_plot = us_acquisition.evaluate(x_plot)\n",
    "ivr_plot = ivr_acquisition.evaluate(x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x_plot, us_plot / np.max(us_plot), \"green\", label=\"US\")\n",
    "ax.plot(x_plot, ivr_plot / np.max(ivr_plot) , \"purple\", label=\"IVR\")\n",
    "\n",
    "ax.legend(loc=1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(-0.01, 1)\n",
    "\n",
    "mlai.write_figure('experimental-design-acquisition-functions-forrester.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\includediagrams{../slides/diagrams/uq/experimental-design-acquisition-functions-forrester}{80%}\n",
    "\n",
    "Figure: <i>The *uncertainty sampling* and *integrated variance\n",
    "reduction* acquisition functions for the Forrester example.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the objective function\n",
    "---------------------------------\n",
    "\n",
    "To find the next point to evaluate we optimize the acquisition function\n",
    "using a standard gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.optimization import GradientAcquisitionOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientAcquisitionOptimizer(space)\n",
    "x_new, _ = optimizer.optimize(us_acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x_plot, us_plot / np.max(us_plot), \"green\", label=\"US\")\n",
    "ax.axvline(x_new, color=\"red\", label=\"x_next\", linestyle=\"--\")\n",
    "ax.legend(loc=1, prop={'size': LEGEND_SIZE})\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(-0.01, 1)\n",
    "\n",
    "mlai.write_figure('experimental-design-acquisition-next-point-forrester.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we evaluate the true objective function and append it to our\n",
    "initial observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = target_function(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(X_init, x_new, axis=0)\n",
    "Y = np.append(Y_init, y_new, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the model, you can see that the uncertainty about the\n",
    "true objective function in this region decreases and our model becomes\n",
    "more certain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emukit_model.set_data(X, Y)\n",
    "mu_plot, var_plot = emukit_model.predict(x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(emukit_model.X, emukit_model.Y, \"ro\", markersize=10, label=\"Observations\")\n",
    "ax.plot(x_plot, y_plot, \"k\", label=\"Target Function\")\n",
    "ax.plot(x_plot, mu_plot, \"C0\", label=\"Model\")\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.6)\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 2 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 2 * np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.4)\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 3 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 3 * np.sqrt(var_plot)[:, 0], color=\"C0\", alpha=0.2)\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(-0.01, 1)\n",
    "\n",
    "mlai.write_figure(filename='forrester-function-multi-errorbars.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/forrester-function-multi-errorbars.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The target Forrester function plotted alongside the emulation\n",
    "model and error bars from the emulation at 1, 2 and 3 standard\n",
    "deviations.</i>\n",
    "\n",
    "Entropy of posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation the Objective Function\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.optimization import GradientAcquisitionOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientAcquisitionOptimizer(space)\n",
    "x_new, _ = optimizer.optimize(us_acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x_plot, us_plot / np.max(us_plot), \"green\", label=\"US\")\n",
    "ax.axvline(x_new, color=\"red\", label=\"x_next\", linestyle=\"--\")\n",
    "ax.legend(loc=1)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(-0.01, 1)\n",
    "\n",
    "mlai.write_figure(filename='forrester-function-three-sd-levels.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/forrester-function-three-sd-levels.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The true Forrester function alongside the surrogate model fit\n",
    "and three separate standard deviations of error bars.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emukit’s experimental design interface\n",
    "--------------------------------------\n",
    "\n",
    "Of course in practice we don’t want to implement all of these steps our\n",
    "self. Emukit provides a convenient and flexible interface to apply\n",
    "experimental design. Below we can see how to run experimental design on\n",
    "the exact same function for 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.experimental_design.experimental_design_loop import ExperimentalDesignLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = ExperimentalDesignLoop(space=space, model=emukit_model)\n",
    "\n",
    "ed.run_loop(target_function, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_plot, var_plot = ed.model.predict(x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(ed.loop_state.X, ed.loop_state.Y, \"ro\", markersize=10, label=\"Observations\")\n",
    "ax.plot(x_plot, y_plot, \"k\", label=\"Objective Function\")\n",
    "ax.plot(x_plot, mu_plot, \"C0\", label=\"Model\")\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - np.sqrt(var_plot)[:, 0], \n",
    "                 color=\"C0\", alpha=0.6)\n",
    "\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 2 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 2 * np.sqrt(var_plot)[:, 0], \n",
    "                 color=\"C0\", alpha=0.4)\n",
    "\n",
    "ax.fill_between(x_plot[:, 0],\n",
    "                 mu_plot[:, 0] + 3 * np.sqrt(var_plot)[:, 0],\n",
    "                 mu_plot[:, 0] - 3 * np.sqrt(var_plot)[:, 0], \n",
    "                 color=\"C0\", alpha=0.2)\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$f(x)$\")\n",
    "ax.set_grid(True)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "mlai.write_figure(filename='forrester-function-full-fit.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/forrester-function-full-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The fit of the model to the Forrester function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forrester, A.I.J., Sóbester, A., Keane, A.J., 2008. Engineering design\n",
    "via surrogate modelling: A practical guide. wiley.\n",
    "<https://doi.org/10.1002/9780470770801>\n",
    "\n",
    "Kennedy, M.C., O’Hagan, A., 2001. Bayesian calibration of computer\n",
    "models. Journal of the Royal Statistical Society: Series B (Statistical\n",
    "Methodology) 63, 425–464. <https://doi.org/10.1111/1467-9868.00294>\n",
    "\n",
    "McKay, M.D., Beckman, R.J., Conover, W.J., 1979. A comparison of three\n",
    "methods for selecting values of input variables in the analysis of\n",
    "output from a computer code. Technometrics 21, 239–245.\n",
    "\n",
    "Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine\n",
    "learning. mit, Cambridge, MA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
