{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 1\n",
    "\n",
    "### [Christian Cabrera](https://www.cst.cam.ac.uk/people/chc79), University\n",
    "\n",
    "of Cambridge\n",
    "\n",
    "### [Radzim Sendyka](https://www.cst.cam.ac.uk/people/rs2071), University\n",
    "\n",
    "of Cambridge\n",
    "\n",
    "### [Carl Henrik Ek](http://carlhenrik.com), University of Cambridge\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge\n",
    "\n",
    "### 2024-11-05"
   ],
   "id": "3aa38138-bca4-4934-b045-579951ae5e2d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In this lab session we look at setting up a SQL server,\n",
    "creating and populating a database, and making joins between different\n",
    "tables."
   ],
   "id": "d76f686e-a287-4f01-8a6d-32d4613d1109"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ],
   "id": "8a854c12-38af-4792-b2fe-53074de7c1a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->\n",
    "\n",
    "**The check Session for this Practical is 7th November 2024.**\n",
    "\n",
    "-   This practical should prepare you for the course assessment. Ensure\n",
    "    that you have a solid understanding of the material, with particular\n",
    "    emphasis on the AWS database setup. You should be able to use the\n",
    "    same database that you set up here for the final assessment.\n",
    "-   In that assessment, you will work with datasets that require initial\n",
    "    setup processes. **Start your work on dataset access and database\n",
    "    setup early** to avoid being blocked from work on subsequent stages\n",
    "    later.\n",
    "-   Some tasks will require you to develop skills for searching for\n",
    "    multiple solutions and experimenting with different approaches,\n",
    "    which lecture content may not cover. This environment closely\n",
    "    resembles real-world data science and software engineering\n",
    "    challenges, where there might not be a unique correct solution."
   ],
   "id": "8b327a53-2d57-413e-9bcf-691cc7492428"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief History of the Cloud\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_cloud/includes/history-of-cloud.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_cloud/includes/history-of-cloud.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In the early days of the internet, companies could make use of open\n",
    "source software, such as the Apache web server and the MySQL database\n",
    "for providing on line stores, or other capabilities. But as part of\n",
    "that, they normally had to provide their own server farms. For example,\n",
    "the earliest server for running PageRank from 1996 was hosted on custom\n",
    "made hardware built in a case of Mega Blocks (see Figure ).\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//cloud/The_first_Google_computer_at_Stanford.jpg\" style=\"width:50%\">\n",
    "\n",
    "Figure: <i>The web search engine was hosted on custom built hardware.\n",
    "Photo credit [Christian Heilmann on\n",
    "Flickr](https://www.flickr.com/photos/11414938@N00/2821326488).</i>\n",
    "\n",
    "By September 2000, Google operated 5000 PCs for searching and web\n",
    "crawling, using the LINUX operating system.[1]\n",
    "\n",
    "Cloud computing gives you access to computing at a similar scale, but on\n",
    "an as-needed basis. AWS launched S3 cloud storage in March 2006 and the\n",
    "Elastic Compute Cloud followed in August 2006. These services were\n",
    "inspired by the challenges they had scaling their own web service (known\n",
    "as Obidos) for running the world’s largest e-commerce site. Recent\n",
    "market share estimates indicate that AWS still retains around a third of\n",
    "the cloud infrastructure market.[2]\n",
    "\n",
    "[1] See [infolab at\n",
    "Stanford](http://infolab.stanford.edu/pub/voy/museum/pictures/display/0-4-Google.htm)\n",
    "for more details.\n",
    "\n",
    "[2] There are various reports that try to track market share, here’s one\n",
    "from Gartner from June 2021:\n",
    "<https://www.gartner.com/en/newsroom/press-releases/2021-06-28-gartner-says-worldwide-iaas-public-cloud-services-market-grew-40-7-percent-in-2020>.\n",
    "It looks at revenue that suggests Amazon retains 40% with Microsoft\n",
    "second largest on 20%."
   ],
   "id": "4aa56a04-9284-48c9-ad6a-075bb1d87b35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UK Housing Datasets\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/uk-housing-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/uk-housing-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The UK Price Paid data for housing in dates back to 1995 and contains\n",
    "millions of transactions. This database is available at the [gov.uk\n",
    "site](https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads).\n",
    "The total data is over 4 gigabytes in size and it is available in a\n",
    "single file or in multiple files splitted by years and semester. For\n",
    "example, the first part of the data for 2018 is stored at\n",
    "<http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018-part1.csv>.\n",
    "By applying the divide and conquer principle, we will download the\n",
    "splitted data because these files is less than 100MB each which makes\n",
    "them easier to manage.\n",
    "\n",
    "Let’s download first the two files that contain the price paid data for\n",
    "the transactions that took place in the year 1995:"
   ],
   "id": "5b2e54b7-0eca-4c24-99f5-c203df460e17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ],
   "id": "580cd5fa-7c8e-4d78-add6-df406a5f6d0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL where the dataset is stored \n",
    "    base_url = \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com\""
   ],
   "id": "4c2b5183-a9ef-4ef0-b67b-af1262de7c72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Downloading part 1 from 1995\n",
    "    file_name_part_1 = \"/pp-1995-part1.csv\"\n",
    "    url = base_url + file_name_part_1\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "      with open(\".\" + file_name_part_1, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    # Downloading part 2 from 1995\n",
    "    file_name_part_2 = \"/pp-1995-part2.csv\"\n",
    "    url = base_url + file_name_part_2\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "      with open(\".\" + file_name_part_2, \"wb\") as file:\n",
    "        file.write(response.content)}\n",
    "\n",
    "The data is downloaded as CSV files in the files explorer of this\n",
    "notebook. You can see that the two pieces of code that download the data\n",
    "are quite similar. It makes sense to use a for loop to automate the way\n",
    "we access the dataset for the different years. The following code will\n",
    "download the data from 1996 to 2010."
   ],
   "id": "d848041f-e936-42e4-ba9e-276ed240998a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL where the dataset is stored \n",
    "    base_url = \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com\""
   ],
   "id": "66f0577a-b2ff-46d7-8333-5e35acd60c0b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # File name with placeholders\n",
    "    file_name = \"/pp-<year>-part<part>.csv\"\n",
    "\n",
    "    for year in range(1996,2011):\n",
    "      print (\"Downloading data for year: \" + str(year))\n",
    "      for part in range(1,3):\n",
    "        url = base_url + file_name.replace(\"<year>\", str(year)).replace(\"<part>\", str(part))\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "          with open(\".\" + file_name.replace(\"<year>\", str(year)).replace(\"<part>\", str(part)), \"wb\") as file:\n",
    "            file.write(response.content)}\n",
    "\n",
    "If we think of reusability, it would be good to create a function that\n",
    "can be called from anywhere in your code."
   ],
   "id": "18200eda-498b-4e69-99f3-681ea3ed0760"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ],
   "id": "e9b04bed-3b84-4268-8cbe-d2ea525857e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_price_paid_data(year_from, year_to):\n",
    "    # Base URL where the dataset is stored \n",
    "    base_url = \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com\"\n",
    "    \"\"\"Download UK house price data for given year range\"\"\"\n",
    "    # File name with placeholders\n",
    "    file_name = \"/pp-<year>-part<part>.csv\"\n",
    "    \n",
    "    for year in range(year_from, (year_to+1)):\n",
    "        print (f\"Downloading data for year: {year}\")\n",
    "        for part in range(1,3):\n",
    "            url = base_url + file_name.replace(\"<year>\", str(year)).replace(\"<part>\", str(part))\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(\".\" + file_name.replace(\"<year>\", str(year)).replace(\"<part>\", str(part)), \"wb\") as file:\n",
    "                    file.write(response.content)"
   ],
   "id": "0967d580-ebd2-4b19-beb4-d49d29682de9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the function to download the data between two given\n",
    "years. For example, let’s download the data from 2011 to 2020 by calling\n",
    "the defined function."
   ],
   "id": "268d0d9c-864c-4442-b607-40bfacae1cec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_price_paid_data(2011, 2020)"
   ],
   "id": "b3f0b222-a0ac-4614-9ff9-994456642012"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Add this function to your fynesse library and download the data from\n",
    "2021 to 2024 using your library."
   ],
   "id": "4d8c45c7-91f1-4a30-8862-931acae391fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ],
   "id": "0a3182ea-b0f0-4ee6-b958-e4d6ab865feb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ],
   "id": "506fef8c-7e93-469e-8cfd-5d796310c164"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Hosted Database\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_access/includes/aws-database-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_access/includes/aws-database-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The size of the data makes it unwieldy to manipulate directly in python\n",
    "frameworks such as `pandas`. As a result we will host and access the\n",
    "data in a *relational database*.\n",
    "\n",
    "Using the following ideas: 1. A cloud hosted database (such as MariaDB\n",
    "hosted on the AWS RDS service). 2. SQL code wrapped in appropriately\n",
    "structured python functions. 3. Joining databases tables. You will\n",
    "construct a database containing tables that contain all house prices,\n",
    "latitudes and longitudes from the UK house price data base since 1995."
   ],
   "id": "8fd23af7-38b2-4bfd-8265-ac42be8bf086"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Notes\n",
    "\n",
    "**You will manipulate large datasets along this practical and the final\n",
    "assessment. If your database is blocked or is not responsive after any\n",
    "operation, have a look at the databases dashboard to see if it is using\n",
    "too much CPU. Reboot the database if that is the case.** **If you\n",
    "encounter problems with the online notebook (e.g., interrupted\n",
    "connections with the AWS server), you can use a local IDE to work in\n",
    "your machine.**"
   ],
   "id": "3ddeed93-74f0-4526-8256-966d72743889"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Database Server\n",
    "\n",
    "A typical machine learning installation might have you running a\n",
    "database from a cloud service (such as AWS, Azure or Google Cloud\n",
    "Platform). That cloud service would host the database for you, and you\n",
    "would pay according to the number of queries made. Popular SQL server\n",
    "software includes \\[`MariaDB`\\] https://mariadb.org/) which is open\n",
    "source, or [Microsoft’s SQL\n",
    "Server](https://www.microsoft.com/en-gb/sql-server/sql-server-2019).\n",
    "\n",
    "Many start-up companies were formed on the back of a `MySQL` server\n",
    "hosted on top of AWS. Although since MySQL was sold to Sun, and then\n",
    "passed on to Oracle, the open source community has turned its attention\n",
    "to `MariaDB`, here’s the [AWS instructions on how to set up\n",
    "`MariaDB`](https://aws.amazon.com/getting-started/hands-on/create-mariadb-db/).\n",
    "\n",
    "If you were designing your own ride hailing app, or any other major\n",
    "commercial software you would want to investigate whether you would need\n",
    "to set up a central SQL server in one of these frameworks."
   ],
   "id": "01fdac00-072e-4f8d-8d83-ae76cf532cb1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a MariaDB Server on AWS\n",
    "\n",
    "In this section, we’ll review the setup required to create a MariaDB\n",
    "server on AWS. The earliest AWS services of S3 and EC2 gave storage and\n",
    "compute. Together these could be combined to host a database service.\n",
    "Today cloud providers also provide machines that are already set up to\n",
    "provide a database service. We will make use of AWS’s Relational\n",
    "Database Service to provide our `MariaDB` server.\n",
    "\n",
    "1.  Log in to your AWS account and go to the AWS RDS console\n",
    "    [here](https://console.aws.amazon.com/rds/home).\n",
    "2.  Make sure **the region is set to Europe (London) which is denoted as\n",
    "    eu-west-2.**\n",
    "3.  Scroll down to “Create Database”. Do *not* create an Aurora database\n",
    "    instance.\n",
    "4.  `Standard Create` should be selected. In the box below, which is\n",
    "    titled `Engine Options` you should select `MariaDB`. You can leave\n",
    "    the `Version` as it’s set.\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//cloud/aws-select-mariadb-rds.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>The AWS console box for selecting the `MariaDB` engine.</i>\n",
    "\n",
    "1.  In the box below that, make sure you select `Free tier`.\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//cloud/aws-select-free-tier.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Make sure you select the free tier option for your database\n",
    "server.</i>\n",
    "\n",
    "1.  Name your database server. For your setup we suggest you use\n",
    "    `database-ads-<CRSid>` for the name. `<CRSid>` **corresponds to your\n",
    "    CRSid. So, every student has an independent database.**\n",
    "2.  Set a master password for accessing the database server as admin.\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//cloud/aws-mariadb-settings.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Set the password and username for the database server\n",
    "access.</i>\n",
    "\n",
    "1.  Leave the `DB instance class` as it is.\n",
    "2.  Leave the `DB instance size` at the default setting. Leave the\n",
    "    storage type and allocated storage at the default settings of\n",
    "    `General Purpose` (SSD) and `20`.\n",
    "3.  *Disable* autoscaling in the `Storage Autoscaling` option.\n",
    "4.  In the connectivity leave the VPC selection as `Default VPC` and\n",
    "    *enable* `Publicly accessible` so that you’ll have an IP address for\n",
    "    your database.\n",
    "5.  In `VPC security group` select `Create new` to create a new security\n",
    "    group for the instance.\n",
    "6.  Write `ADSMariaDB` as the group name for the VPC security group.\n",
    "7.  Leave the rest as it is and select `Create database` at the bottom\n",
    "    to launch the database server.\n",
    "\n",
    "Your database server will take a few minutes to launch.\n",
    "\n",
    "While it’s launching you can check the access rules for the database\n",
    "server\n",
    "[here](https://eu-west-2.console.aws.amazon.com/ec2/v2/home?region=eu-west-2#SecurityGroups:).\n",
    "\n",
    "1.  Select the `Default` security group.\n",
    "2.  The source of the active inbound rule must be set to `0.0.0.0/0`. It\n",
    "    means you can connect from any source using IPv4.\n",
    "\n",
    "A wrong inbound rule can cause you fail connecting to the database from\n",
    "this notebook.\n",
    "\n",
    "*Note:* by setting the inbound rule to `0.0.0.0/0` we have opened up\n",
    "access to *any* IP address. If this were production code you wouldn’t do\n",
    "this, you would specify a range of addresses or the specific address of\n",
    "the compute server that needed to access the system. Because we’re using\n",
    "Google colab or another notebook client to access, and we can’t control\n",
    "the IP address of that access, for simplicity we’ve set it up so that\n",
    "any IP address can access the database, but that is *not good practice*\n",
    "for production systems."
   ],
   "id": "26bcce3b-c15f-41fa-a4d0-78830e4725dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to your Database Server\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_access/includes/sql-database-connection.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_access/includes/sql-database-connection.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Before you start, you’re going to need the username and password you\n",
    "set-up above for accessing the database server. You will need to make\n",
    "use of it when your client connects to the server. It’s good practice to\n",
    "never expose passwords in your code directly. So to protect your\n",
    "passowrd, we’re going to create a `credentials.yaml` file locally that\n",
    "will store your username and password so that the client can access the\n",
    "server without ever showing your password in the notebook. This file\n",
    "will also score the URL and port of your database. You can get these\n",
    "details from your database connectivity and security details."
   ],
   "id": "c856b288-b791-443e-a489-68152894e32a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from ipywidgets import interact_manual, Text, Password"
   ],
   "id": "06e03568-250c-4311-8395-da10e8640976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual(username=Text(description=\"Username:\"),\n",
    "                password=Password(description=\"Password:\"),\n",
    "                url=Text(description=\"URL:\"),\n",
    "                port=Text(description=\"Port:\"))\n",
    "def write_credentials(username, password, url, port):\n",
    "    with open(\"credentials.yaml\", \"w\") as file:\n",
    "        credentials_dict = {'username': username,\n",
    "                           'password': password,\n",
    "                           'url': url,\n",
    "                           'port': port}\n",
    "        yaml.dump(credentials_dict, file)"
   ],
   "id": "2f52ec3d-fd4d-443f-9e88-e3e1746a445a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you click `Run Interact` then the credentials you’ve selected will be\n",
    "saved in the `yaml` file.\n",
    "\n",
    "Then, we can read the server credentials using:"
   ],
   "id": "54cb9890-6673-4162-95f5-ea19d8c142da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.yaml\") as file:\n",
    "  credentials = yaml.safe_load(file)\n",
    "username = credentials[\"username\"]\n",
    "password = credentials[\"password\"]\n",
    "url = credentials[\"url\"]\n",
    "port = credentials[\"port\"]"
   ],
   "id": "36f69a61-e28a-4a3a-a972-7516e3fc9e97"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Commands\n",
    "\n",
    "We have all the required data to interact with our database server.\n",
    "There are mainly two ways how we can do that. The first one is using\n",
    "magic SQL commands. For this option, we need to install pymysql and load\n",
    "the sql extension:"
   ],
   "id": "2acea5b4-e285-49c2-9a6f-19ec0612749a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymysql"
   ],
   "id": "1ae336d4-129a-45eb-a630-616100f31f11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ],
   "id": "3927cfd5-4bfc-477d-b226-dbcf26c874b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our first database server connection using magic SQL.\n",
    "The first line establishes the connection and the second one list the\n",
    "databases. For now, you should see the databases that the engine has\n",
    "installed by default."
   ],
   "id": "6e501303-d7c0-4303-9ffb-99f1624bff67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql mariadb+pymysql://$username:$password@$url?local_infile=1\n",
    "%sql SHOW databases"
   ],
   "id": "a6d2b819-c5dd-4d72-a6f8-5b8e9630c2ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This connection also enables the uploading of local files as part of the\n",
    "connection (i.e., `local_infile=1`). We will use this property later."
   ],
   "id": "17b6227b-b5ab-40bf-8bd5-f8d1bae785e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_access/includes/sql-database-schema.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_access/includes/sql-database-schema.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "As a first step after establishing a connection, we should create our\n",
    "own database in the server. We will use this relational database to\n",
    "store, structure, and access the different datasets we will manipulate\n",
    "during this course. We will use SQL code for this and once again magic\n",
    "commands:"
   ],
   "id": "1c244287-6627-4ece-aefc-7d41f77292d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET SQL_MODE = \"NO_AUTO_VALUE_ON_ZERO\";\n",
    "SET time_zone = \"+00:00\";\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS `ads_2024` DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;"
   ],
   "id": "4a6857e6-199a-46c3-9a2d-5bb420fe80bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SHOW databases"
   ],
   "id": "57a37795-6c1b-4b93-a1e3-f9fbb6c13810"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the database is created in our server, we must tell it which\n",
    "database we will use. You will need to run this command after you create\n",
    "a new connection:"
   ],
   "id": "33bb37f1-caaa-4afe-8c77-171123d4849e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE `ads_2024`;"
   ],
   "id": "2156cd8b-76bb-478e-8b1d-cada24d59dd1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A database is composed of tables where data records are stored in rows.\n",
    "The attributes of each record are the columns. We must define an\n",
    "`schema` to create a table in a database. The `schema` tells the\n",
    "database and the server what to expect in the columns of the table\n",
    "(i.e., names and data types of the columns).\n",
    "\n",
    "The `schema` is defined by the data we want to store. If we want to\n",
    "store the UK Price Paid data, we should have a look at [its description\n",
    "first](https://www.gov.uk/guidance/about-the-price-paid-data#explanations-of-column-headers-in-the-ppd).\n",
    "Also, we should a look at the data. Let’s do that for the second\n",
    "semester of 2021."
   ],
   "id": "6ba5685a-ae3c-4dd0-97fc-067c8b12d99e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "id": "932e2918-2909-4c1f-8cfe-7e4ec0d4586b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./pp-2021-part2.csv\"\n",
    "data = pd.read_csv(file_name)\n",
    "data.info(verbose=True)"
   ],
   "id": "2b09b2a3-2a54-4739-a4c2-2467044d8450"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the columns of the dataframe, we should define now the\n",
    "equivalent `schema` of the table in the SQL database. We will use once\n",
    "more SQL magic commands to create a table with the equivalent `schema`:"
   ],
   "id": "f33edfaf-013d-4bf4-859f-fc58abd69a37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: If you run this command after you have uploaded data to the table (in the steps below), you will delete the uploaded data as this command first drops the table if exists (DROP TABLE IF EXISTS `pp_data`;).\n",
    "%%sql\n",
    "--\n",
    "-- Table structure for table `pp_data`\n",
    "--\n",
    "USE `ads_2024`;\n",
    "DROP TABLE IF EXISTS `pp_data`;\n",
    "CREATE TABLE IF NOT EXISTS `pp_data` (\n",
    "  `transaction_unique_identifier` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `price` int(10) unsigned NOT NULL,\n",
    "  `date_of_transfer` date NOT NULL,\n",
    "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `primary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `secondary_addressable_object_name` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `street` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `district` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `county` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `ppd_category_type` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "  `record_status` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "  `db_id` bigint(20) unsigned NOT NULL\n",
    ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;"
   ],
   "id": "a6825807-5968-4283-ad3b-9a74b5039d2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `schema` defines an id field in the table (i.e., `db_id`), which\n",
    "must be unique and will play the role of [primary\n",
    "key](https://www.geeksforgeeks.org/primary-key-in-dbms/), which is a\n",
    "crucial concept in relational databases. The following code sets up the\n",
    "primary key for our table and makes it auto increment when a new row\n",
    "(i.e., record) is insterted into the table."
   ],
   "id": "4f1e3179-8b20-4348-8fea-d43245e1fb56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "--\n",
    "-- Primary key for table `pp_data`\n",
    "--\n",
    "ALTER TABLE `pp_data`\n",
    "ADD PRIMARY KEY (`db_id`);\n",
    "\n",
    "ALTER TABLE `pp_data`\n",
    "MODIFY db_id bigint(20) unsigned NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=1;"
   ],
   "id": "e19ff361-280e-4baa-b266-7d9de7523175"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve created our first table in the database with its respective\n",
    "primary key. Now we need to populate it. There are different methods to\n",
    "do that. [Some of them more efficient than\n",
    "others](https://dev.to/arctype/load-data-infile-vs-insert-in-mysql-why-how-when-247f#:~:text=%E2%80%8BWhen%20working%20with%20MySQL,way%20faster%20than%20INSERT%20does.).\n",
    "In our case, given the size of our data set, we will take advantage of\n",
    "the csv files we downloaded in the first part of this lab. The command\n",
    "`LOAD DATA LOCAL INFILE` allows uploading data to the table from a CSV\n",
    "file. We must specify the name of the local file, the name of the table,\n",
    "and the format of the CSV file we want to use (i.e., separators,\n",
    "enclosers, termination line characters, etc.)\n",
    "\n",
    "The following command uploads the data of the transactions that took\n",
    "place in 1995."
   ],
   "id": "09424b67-60f0-4e27-a6da-d94733671b35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql USE `ads_2024`;\n",
    "%sql LOAD DATA LOCAL INFILE \"./pp-1995-part1.csv\" INTO TABLE `pp_data` FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED by '\"' LINES STARTING BY '' TERMINATED BY '\\n';\n",
    "%sql LOAD DATA LOCAL INFILE \"./pp-1995-part2.csv\" INTO TABLE `pp_data` FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED by '\"' LINES STARTING BY '' TERMINATED BY '\\n';"
   ],
   "id": "32ca54c6-b4ad-4c6b-925f-f9cec87327a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to upload the data for all the years, we will need a command\n",
    "for each CSV in our dataset. Alternatively, we can use Python code\n",
    "together with SQL magic commands as follows:"
   ],
   "id": "28c954a9-33b4-4cfc-9200-f0c3b12c35ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This code will take a long time to finish (i.e., more than 30 minutes) given our dataset's size. The print informs the uploading progress by year.\n",
    "for year in range(1996,2025):\n",
    "  print (\"Uploading data for year: \" + str(year))\n",
    "  for part in range(1,3):\n",
    "    file_name = \"./pp-\" + str(year) + \"-part\" + str(part) + \".csv\"\n",
    "    %sql LOAD DATA LOCAL INFILE \"$file_name\" INTO TABLE `pp_data` FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED by '\"' LINES STARTING BY '' TERMINATED BY '\\n';"
   ],
   "id": "9ad68b53-1d24-4cbd-8691-2bfdc9f2b03b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we uploaded the data, we can retrieve it from the table. We can\n",
    "select the first 5 elements in the `pp_data` using this command:"
   ],
   "id": "e7c5ab94-887d-4d46-a410-19587769c1b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE `ads_2024`;\n",
    "SELECT * FROM `pp_data` LIMIT 5;"
   ],
   "id": "23674772-3b43-4eea-bd2a-5e009320a24c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the number of rows in our table. It can take more than\n",
    "5 minutes to finish. There are almost 30 million of records in the\n",
    "dataset."
   ],
   "id": "f456006a-2e58-4e07-95c6-f2afc4474274"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from `pp_data`;"
   ],
   "id": "e7bf23d4-3215-476a-8208-5819abbb056d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal Codes Dataset\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/postcode-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/postcode-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The UK Price Paid dataset is now available in our database. This dataset\n",
    "provides useful information about the housing transactions in the UK.\n",
    "More complete analysis can be enabled if we join it with additional\n",
    "datasets. That is the goal of the rest of this practical. The [Open\n",
    "Postcode Geo](https://www.getthedata.com/open-postcode-geo) provides\n",
    "additional information about the houses. It is a dataset of British\n",
    "postcodes with easting, northing, latitude, and longitude and with\n",
    "additional fields for geospace applications, including postcode area,\n",
    "postcode district, postcode sector, incode, and outcode.\n",
    "\n",
    "Your task now is to make this dataset available and accessible in our\n",
    "database. The data you need can be found at this url:\n",
    "<https://www.getthedata.com/downloads/open_postcode_geo.csv.zip>. It\n",
    "will need to be unzipped before use.\n",
    "\n",
    "You may find the following schema useful for the postcode data\n",
    "(developed by Christian and Neil)\n",
    "\n",
    "|                                             |\n",
    "|:--------------------------------------------|\n",
    "| – Table structure for table `postcode_data` |\n",
    "\n",
    "DROP TABLE IF EXISTS `postcode_data`; CREATE TABLE IF NOT EXISTS\n",
    "`postcode_data` ( `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "`status` enum(‘live’,‘terminated’) NOT NULL, `usertype` enum(‘small’,\n",
    "‘large’) NOT NULL, `easting` int unsigned, `northing` int unsigned,\n",
    "`positional_quality_indicator` int NOT NULL, `country` enum(‘England’,\n",
    "‘Wales’, ‘Scotland’, ‘Northern Ireland’, ‘Channel Islands’, ‘Isle of\n",
    "Man’) NOT NULL, `latitude` decimal(11,8) NOT NULL, `longitude`\n",
    "decimal(10,8) NOT NULL, `postcode_no_space` tinytext COLLATE utf8_bin\n",
    "NOT NULL, `postcode_fixed_width_seven` varchar(7) COLLATE utf8_bin NOT\n",
    "NULL, `postcode_fixed_width_eight` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "`postcode_area` varchar(2) COLLATE utf8_bin NOT NULL,\n",
    "`postcode_district` varchar(4) COLLATE utf8_bin NOT NULL,\n",
    "`postcode_sector` varchar(6) COLLATE utf8_bin NOT NULL, `outcode`\n",
    "varchar(4) COLLATE utf8_bin NOT NULL, `incode` varchar(3) COLLATE\n",
    "utf8_bin NOT NULL, `db_id` bigint(20) unsigned NOT NULL ) DEFAULT\n",
    "CHARSET=utf8 COLLATE=utf8_bin;\n",
    "\n",
    "And again you’ll want to set up a primary key for the new table.\n",
    "\n",
    "ALTER TABLE `postcode_data` ADD PRIMARY KEY (`db_id`);\n",
    "\n",
    "ALTER TABLE `postcode_data`; MODIFY `db_id` bigint(20) unsigned NOT NULL\n",
    "AUTO_INCREMENT,AUTO_INCREMENT=1;\n",
    "\n",
    "And you can load the CSV file into the table using this command.\n",
    "\n",
    "LOAD DATA LOCAL INFILE ‘open_postcode_geo.csv’ INTO TABLE\n",
    "`postcode_data` FIELDS TERMINATED BY ‘,’ OPTIONALLY ENCLOSED by ‘“’\n",
    "LINES STARTING BY ’’ TERMINATED BY ‘’;"
   ],
   "id": "d11022b8-6d34-4bba-8b88-be4b992fd8ff"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Upload the postcode dataset to your database."
   ],
   "id": "c00223ab-aa61-4860-8147-c31662e01f94"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Answer\n",
    "\n",
    "Write your answer to Exercise 2 here"
   ],
   "id": "3f8df931-b7d1-4cb2-bad1-ce9237ac3b13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ],
   "id": "e7ffaa30-7d6c-4620-9886-9864d59d47ec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Tables\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_access/includes/table-joins.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_access/includes/table-joins.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "When we have two tables with data tha describe the same object, then it\n",
    "makes sense to join the tables together to enrich our knowledge about\n",
    "the object. For example, while the `pp_data` tell us about the\n",
    "transactions a house have been involved, the `postcode_data` tell us\n",
    "details about the location of the house. By joining both, we could\n",
    "answer more interesting questions like what are the coordinates of the\n",
    "most expensive house in 2024?\n",
    "\n",
    "The join of the tables must be done by matching the columns the tables\n",
    "share. In this case, the `pp_data` and the `postcode_data` tables share\n",
    "the column `postcode`. This operation can take long time because the\n",
    "number of records in each database is huge.\n",
    "\n",
    "You will find the issue of operations taking too long when handling\n",
    "large data sets in different scenarios. An appropriate strategy to\n",
    "overcome such issues is to index the tables. Indexing is a way to\n",
    "organise the data so the queries are more efficient time-wise. Now the\n",
    "task is to select the right columns to create the index. This decision\n",
    "depends on the columns we are using in the SQL operations. In the join\n",
    "case, we are selecting and matching records in each table using the\n",
    "`postcode` column. It makes sense then to create an index for these\n",
    "columns. The following code indexes the table `pp_data` by `postcode`."
   ],
   "id": "542b2110-f1ca-4446-9069-23c4d967aa8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Giving the size of the table, this operation takes around 8 minutes.\n",
    "# If your database is not responsive, check the status of your database on the AWS dashboard. You can restart the database from the dashboard.\n",
    "%%sql\n",
    "USE `ads_2024`;\n",
    "CREATE INDEX idx_pp_postcode ON pp_data(postcode);"
   ],
   "id": "1c92ad7b-f84e-4634-a3d6-ed7591be052e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try our join the tables for year 2024 now:"
   ],
   "id": "e9b78e86-98f7-4183-9666-faf3ed1452d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE `ads_2024`;\n",
    "select * from pp_data as pp inner join postcode_data as po on pp.postcode = po.postcode where pp.date_of_transfer BETWEEN '2024-01-01' AND '2024-12-31' limit 5;"
   ],
   "id": "87b38e55-553f-493a-80a6-369958a9cd8f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The index made a difference in the time the join operation took to\n",
    "finish. Write the code to index the table `postcode_data` by `postcode`."
   ],
   "id": "45c22fdc-9052-4faf-8342-4f281154552f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Answer\n",
    "\n",
    "Write your answer to Exercise 3 here"
   ],
   "id": "e0636f7c-08ab-4243-a0a7-8ac939612085"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ],
   "id": "06a7eb71-aaaa-4d6f-8efb-5ff7f729e8c0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let’s try the join again:"
   ],
   "id": "b8e4658e-c2ea-403f-ae96-661b73c2cb27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE `ads_2024`;\n",
    "select * from pp_data as pp inner join postcode_data as po on pp.postcode = po.postcode where pp.date_of_transfer BETWEEN '2024-01-01' AND '2024-12-31' limit 5;"
   ],
   "id": "35a9b5f1-5954-40fe-80aa-f9743184ff6d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Do you see any difference after adding the new index? Why?"
   ],
   "id": "19418ca8-02d7-4d70-8c24-744184b3b904"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Answer\n",
    "\n",
    "Write your answer to Exercise 4 here"
   ],
   "id": "88ad5246-1e52-411c-a2b8-e5de2f5db69b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ],
   "id": "8242a851-3443-47fa-9777-175fb0b1addf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Python Client\n",
    "\n",
    "Now let’s focus on the second way of interacting with the database\n",
    "server. We can use Python code to create a client that communicates with\n",
    "our database server. For this, we need to install the following\n",
    "libraries."
   ],
   "id": "f87019d2-a1ff-4dc4-9b8b-36e26f5ad25f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipython-sql"
   ],
   "id": "89b8e8ee-095f-4efd-bdd7-dc4b057e7ad3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyMySQL"
   ],
   "id": "bdc23659-ab67-4e67-836f-d4cce07ae6ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a method in Python to establish a database connection\n",
    "wherever we like. It should look like the following code:"
   ],
   "id": "b93265d1-8dbd-4980-bdbd-c753e8935fe9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql"
   ],
   "id": "76f353fe-fd2b-43da-828e-d72a9c2fc33a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(user, password, host, database, port=3306):\n",
    "    \"\"\" Create a database connection to the MariaDB database\n",
    "        specified by the host url and database name.\n",
    "    :param user: username\n",
    "    :param password: password\n",
    "    :param host: host url\n",
    "    :param database: database name\n",
    "    :param port: port number\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pymysql.connect(user=user,\n",
    "                               passwd=password,\n",
    "                               host=host,\n",
    "                               port=port,\n",
    "                               local_infile=1,\n",
    "                               db=database\n",
    "                               )\n",
    "        print(f\"Connection established!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the MariaDB Server: {e}\")\n",
    "    return conn"
   ],
   "id": "4a849483-98ec-404c-b5b5-9d2673d032db"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add the code above to your fynesse library. We can now call this\n",
    "function to get a connection:"
   ],
   "id": "e026e3e9-d7d2-4ff2-bafd-b46a4c239f3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code to establish a connection using your fynesee library"
   ],
   "id": "70219c63-d5a7-401a-83d8-3e1c797db550"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s define a Python method that uploads to a table the data\n",
    "product of the join operation between the tables `pp_data` and\n",
    "`postcode_data`. For this, we first need to create the table that will\n",
    "store this data."
   ],
   "id": "7d9e4384-2134-483b-8a9b-e8ca11ecedfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE `ads_2024`;\n",
    "--\n",
    "-- Table structure for table `prices_coordinates_data`\n",
    "--\n",
    "DROP TABLE IF EXISTS `prices_coordinates_data`;\n",
    "CREATE TABLE IF NOT EXISTS `prices_coordinates_data` (\n",
    "  `price` int(10) unsigned NOT NULL,\n",
    "  `date_of_transfer` date NOT NULL,\n",
    "  `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
    "  `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
    "  `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `district` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `county` tinytext COLLATE utf8_bin NOT NULL,\n",
    "  `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
    "  `latitude` decimal(11,8) NOT NULL,\n",
    "  `longitude` decimal(10,8) NOT NULL,\n",
    "  `db_id` bigint(20) unsigned NOT NULL\n",
    ") DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;"
   ],
   "id": "38782f5c-a635-424b-9c2b-5bc38bc231dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should define the primary key for this table too."
   ],
   "id": "7aaec517-e75a-42f6-ace7-9366daf21a60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "ALTER TABLE `prices_coordinates_data`\n",
    "ADD PRIMARY KEY (`db_id`);\n",
    "\n",
    "ALTER TABLE `prices_coordinates_data`\n",
    "MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1;"
   ],
   "id": "13fe8130-d9d5-4bfc-994b-4e6204096376"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing the table `pp_data` by date will be useful for populating the\n",
    "`prices_coordinates_data`. This index can take around 8 minutes to\n",
    "finish."
   ],
   "id": "7928918f-251c-47f5-a227-e9f23b7b728f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE `ads_2024`;\n",
    "CREATE INDEX idx_pp_date_transfer ON pp_data(date_of_transfer);"
   ],
   "id": "01758151-75be-4504-87c2-433c54cf15f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the table exists in our database, let’s create a method for\n",
    "uploading the join data. This method will upload the data for a given\n",
    "year and will use the logic we have used before but in Python code."
   ],
   "id": "7bcc9279-bb75-4adc-b87e-8416b307d2b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ],
   "id": "67f1b322-b757-437a-86cf-357560273bd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ],
   "id": "e357c767-4a82-4eb4-9984-53a6f275baa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def housing_upload_join_data(conn, year):\n",
    "  start_date = str(year) + \"-01-01\"\n",
    "  end_date = str(year) + \"-12-31\"\n",
    "\n",
    "  cur = conn.cursor()\n",
    "  print('Selecting data for year: ' + str(year))\n",
    "  cur.execute(f'SELECT pp.price, pp.date_of_transfer, po.postcode, pp.property_type, pp.new_build_flag, pp.tenure_type, pp.locality, pp.town_city, pp.district, pp.county, po.country, po.latitude, po.longitude FROM (SELECT price, date_of_transfer, postcode, property_type, new_build_flag, tenure_type, locality, town_city, district, county FROM pp_data WHERE date_of_transfer BETWEEN \"' + start_date + '\" AND \"' + end_date + '\") AS pp INNER JOIN postcode_data AS po ON pp.postcode = po.postcode')\n",
    "  rows = cur.fetchall()\n",
    "\n",
    "  csv_file_path = 'output_file.csv'\n",
    "\n",
    "  # Write the rows to the CSV file\n",
    "  with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    # Write the data rows\n",
    "    csv_writer.writerows(rows)\n",
    "  print('Storing data for year: ' + str(year))\n",
    "  cur.execute(f\"LOAD DATA LOCAL INFILE '\" + csv_file_path + \"' INTO TABLE `prices_coordinates_data` FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED by '\\\"' LINES STARTING BY '' TERMINATED BY '\\n';\")\n",
    "  print('Data stored for year: ' + str(year))"
   ],
   "id": "6127d7a3-bb50-4168-9d14-fd6f657baf95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets upload the joined data for 2024. This upload is going to take\n",
    "long time given the size of our datasets:"
   ],
   "id": "f1407a80-99d0-4dd2-94d0-71472ed76487"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_upload_join_data(conn, 2024)"
   ],
   "id": "8df20c3d-809c-492f-9c16-f53ce6a7f635"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Add the `housing_upload_join_data` function to your fynesse library and\n",
    "write the code to upload the joined data for 2023."
   ],
   "id": "583e69bc-4f6f-4140-b03a-f93dbc44d5e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 Answer\n",
    "\n",
    "Write your answer to Exercise 5 here"
   ],
   "id": "a45bfd85-f84c-400d-83a5-e62e5fcdc4ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this box for any code you need\n",
    "\n"
   ],
   "id": "85170d00-cb6e-465d-a16e-f2e490a91fef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical, we have explored how to persist a couple of datasets\n",
    "in a relational database to facilitate future access. We configured a\n",
    "Cloud-hosted database server and had a look at two ways to interact with\n",
    "it. Then, we explored how to join tables using SQL and the benefits of\n",
    "indexing. The tables you created in this practical will be used along\n",
    "the course and we expect you use them for your final assignment. In the\n",
    "following practical, you will `assess` this data using different methods\n",
    "from visualisations to statistical analysis."
   ],
   "id": "14e37bb1-4c65-431e-b3e1-48f39e352fae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   book: [The Atomic\n",
    "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ],
   "id": "026508d0-f0a2-4489-bd0d-2c4839e39a69"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ],
   "id": "a925000e-c30b-498e-8133-2c33de3dfa7b"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
