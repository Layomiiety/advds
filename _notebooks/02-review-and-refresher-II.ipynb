{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review and Refresher II\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of\n",
    "\n",
    "Cambridge\n",
    "\n",
    "### 2023-11-07"
   ],
   "id": "6dfdd5f7-f197-4714-9e1b-4fb9cc984606"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: In this review and refresher notebook we remind ourselves\n",
    "about linear models and use the opportunity to provide some review of\n",
    "linear algebra. Most of the code you need is provided in the notebook,\n",
    "there are a few exercises to help develop your understanding."
   ],
   "id": "d861bc60-692a-406a-9f56-d6f9cf9af5a2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ],
   "id": "b5768712-f0c3-48f5-9785-5c85e3621fc8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ],
   "id": "8a26c95a-9cfb-402a-9672-91ade657e014"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the review and refresh practical. This work is for you to\n",
    "remind yourself about the ways of using the notebook. You will have a\n",
    "chance to ask us questions about the content on 9th of November. Within\n",
    "the material you will find exercises. If you are confident with\n",
    "probability you can likely ignore the reading and exercises which are\n",
    "listed from Rogers and Girolami (2011) and Bishop (2006). There should\n",
    "be *no need* to purchase these books for this course. The suggestions of\n",
    "sections are there just as a reminder.\n",
    "\n",
    "We suggest you run these labs in *Google colab*, there’s a link to doing\n",
    "that on the main lab page at the top. We also suggest that the first\n",
    "thing you do is click `View`-\\>`Expand sections` to make all parts of\n",
    "the lab visibile.\n",
    "\n",
    "We suggest you complete at least Exercises 2-5."
   ],
   "id": "66ee1248-5574-403c-9335-361850d33694"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_notebooks/includes/notebook-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_notebooks/includes/notebook-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "b203e7c0-648f-46c0-bf9f-e2dd00697974"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ],
   "id": "ed76109d-e01f-4bc6-86a1-9cd0c7feded7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--setupplotcode{import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')}-->"
   ],
   "id": "a26530fa-3992-4103-84b2-ca8aca9257a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notutils\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/notutils-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/notutils-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "This small package is a helper package for various notebook utilities\n",
    "used below.\n",
    "\n",
    "The software can be installed using"
   ],
   "id": "0bcb5209-4162-43ee-a47f-3433e3986936"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install notutils"
   ],
   "id": "635d79c8-3c64-47b8-8903-79e20f5c0253"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub:\n",
    "<https://github.com/lawrennd/notutils>\n",
    "\n",
    "Once `notutils` is installed, it can be imported in the usual manner."
   ],
   "id": "17987d0e-7097-4b84-ad26-f70a98bfc5f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils"
   ],
   "id": "e15828cd-7714-4b15-9a7e-32edcc2eccf2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pods\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/pods-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/pods-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science’. Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ],
   "id": "225cd31f-c8ed-4ad3-b337-8ff68f6e8b94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pods"
   ],
   "id": "2d0896fe-4afc-46e8-93de-11e17b76da86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/ods>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ],
   "id": "240ce2d0-50bf-49bd-bfdc-04bf239e8045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ],
   "id": "6ead6387-5332-48df-aa23-373c1c00749e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlai\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/mlai-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_software/includes/mlai-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The `mlai` software is a suite of helper functions for teaching and\n",
    "demonstrating machine learning algorithms. It was first used in the\n",
    "Machine Learning and Adaptive Intelligence course in Sheffield in 2013.\n",
    "\n",
    "The software can be installed using"
   ],
   "id": "cf4f7604-147d-4ecc-b4c8-a46769511d0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlai"
   ],
   "id": "269cc43f-dcc1-4900-9b11-9a665bd9d15b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/mlai>\n",
    "\n",
    "Once `mlai` is installed, it can be imported in the usual manner."
   ],
   "id": "0b4a8bdf-1dad-419b-acc5-dcb1973869e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai"
   ],
   "id": "9017d098-df00-4ba1-966d-1f4ea1516900"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/review-and-refresher-II.gpp.markdown\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/review-and-refresher-II.gpp.markdown', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "-   Last time: Looked at objective functions for movie recommendation.\n",
    "-   Minimized sum of squares objective by steepest descent and\n",
    "    stochastic gradients.\n",
    "-   This time: explore least squares for regression."
   ],
   "id": "d08e5975-9482-4559-93ed-59619e222952"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Examples\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/regression-examples.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/regression-examples.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Regression involves predicting a real value, $y_i$, given an input\n",
    "vector, $\\mathbf{ x}_i$. For example, the Tecator data involves\n",
    "predicting the quality of meat given spectral measurements. Or in\n",
    "radiocarbon dating, the C14 calibration curve maps from radiocarbon age\n",
    "to age measured through a back-trace of tree rings. Regression has also\n",
    "been used to predict the quality of board game moves given expert rated\n",
    "training data."
   ],
   "id": "fc95aca2-b58b-47f6-952f-9f1cc8c069fb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic 100m Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-100m-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-100m-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "\n",
    "-   Gold medal times for Olympic 100 m runners since 1896.\n",
    "-   One of a number of Olypmic data sets collected by Rogers and\n",
    "    Girolami (2011). `{=html}     </td>` `{=html}     <td width=\"50%\">`\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/100m_final_start.jpg\" style=\"width:100%\">\n",
    "\n",
    "Figure: <i>Start of the 2012 London 100m race. *Image from Wikimedia\n",
    "Commons* <http://bit.ly/191adDC></i>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal 100m\n",
    "winners for the Olympics from 1896 to present. First we load in the data\n",
    "and plot."
   ],
   "id": "27141cd9-e6c6-47ce-b546-993aab5bbd99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ],
   "id": "66bbfeea-75ca-479e-b238-8bec2209cdc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_100m_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())"
   ],
   "id": "faebab72-68a6-4f8a-9d9f-f113014b9199"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "8a4971ba-4bf7-45b2-a96e-2e352edcca62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "6aea008f-8e2e-4bb2-8fb0-d4b944112403"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xlim = (1875,2030)\n",
    "ylim = (9, 12)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(filename='olympic-100m.svg', \n",
    "                  directory='./datasets')"
   ],
   "id": "6d7552b6-2191-40b0-8341-ce6e1b5de717"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//datasets/olympic-100m.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic 100m wining times since 1896.</i>"
   ],
   "id": "e9d43243-b407-47ec-9cec-d3dfe2016ed4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Olympic Marathon Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-marathon-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/olympic-marathon-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "-   Marathons before 1924 didn’t have a standardized distance.\n",
    "-   Present results using pace per km.\n",
    "-   In 1904 Marathon was badly organized leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//Stephen_Kiprotich.jpg\" style=\"width:100%\">\n",
    "<small>Image from Wikimedia Commons <http://bit.ly/16kMKHQ></small>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The first thing we will do is load a standard data set for regression\n",
    "modelling. The data consists of the pace of Olympic Gold Medal Marathon\n",
    "winners for the Olympics from 1896 to present. Let’s load in the data\n",
    "and plot."
   ],
   "id": "4dbd876b-f4fc-4f1e-8e75-0319c290704e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pods"
   ],
   "id": "51163cd0-8b9f-4ad0-9d90-1953f23e58af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())\n",
    "yhat = (y - offset)/scale"
   ],
   "id": "fc7de07e-753d-4cb4-9405-cf57776bde48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "76ea6a33-df3e-4687-8f2b-3be4e167a127"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(filename='olympic-marathon.svg', \n",
    "                  directory='./datasets')"
   ],
   "id": "b3310405-6ca7-493c-b4b1-136d0f75f8fb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//datasets/olympic-marathon.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Olympic marathon pace times since 1896.</i>\n",
    "\n",
    "Things to notice about the data include the outlier in 1904, in that\n",
    "year the Olympics was in St Louis, USA. Organizational problems and\n",
    "challenges with dust kicked up by the cars following the race meant that\n",
    "participants got lost, and only very few participants completed. More\n",
    "recent years see more consistently quick marathons."
   ],
   "id": "70f7ac0c-a836-4cb0-9667-2dcf6789f9c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "What is machine learning? At its most basic level machine learning is a\n",
    "combination of\n",
    "\n",
    "$$\\text{data} + \\text{model} \\stackrel{\\text{compute}}{\\rightarrow} \\text{prediction}$$\n",
    "\n",
    "where *data* is our observations. They can be actively or passively\n",
    "acquired (meta-data). The *model* contains our assumptions, based on\n",
    "previous experience. That experience can be other data, it can come from\n",
    "transfer learning, or it can merely be our beliefs about the\n",
    "regularities of the universe. In humans our models include our inductive\n",
    "biases. The *prediction* is an action to be taken or a categorization or\n",
    "a quality score. The reason that machine learning has become a mainstay\n",
    "of artificial intelligence is the importance of predictions in\n",
    "artificial intelligence. The data and the model are combined through\n",
    "computation.\n",
    "\n",
    "In practice we normally perform machine learning using two functions. To\n",
    "combine data with a model we typically make use of:\n",
    "\n",
    "**a prediction function** it is used to make the predictions. It\n",
    "includes our beliefs about the regularities of the universe, our\n",
    "assumptions about how the world works, e.g., smoothness, spatial\n",
    "similarities, temporal similarities.\n",
    "\n",
    "**an objective function** it defines the ‘cost’ of misprediction.\n",
    "Typically, it includes knowledge about the world’s generating processes\n",
    "(probabilistic objectives) or the costs we pay for mispredictions\n",
    "(empirical risk minimization).\n",
    "\n",
    "The combination of data and model through the prediction function and\n",
    "the objective function leads to a *learning algorithm*. The class of\n",
    "prediction functions and objective functions we can make use of is\n",
    "restricted by the algorithms they lead to. If the prediction function or\n",
    "the objective function are too complex, then it can be difficult to find\n",
    "an appropriate learning algorithm. Much of the academic field of machine\n",
    "learning is the quest for new learning algorithms that allow us to bring\n",
    "different types of models and data together.\n",
    "\n",
    "A useful reference for state of the art in machine learning is the UK\n",
    "Royal Society Report, [Machine Learning: Power and Promise of Computers\n",
    "that Learn by\n",
    "Example](https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf).\n",
    "\n",
    "You can also check my post blog post on [What is Machine\n",
    "Learning?](http://inverseprobability.com/2017/07/17/what-is-machine-learning)."
   ],
   "id": "70752346-2e61-49f4-9322-7739637ecbd5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of Squares Error\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/sum-of-squares-error.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/sum-of-squares-error.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Last week we considered a cost function for minimization of the error.\n",
    "We considered items (films) and users and assumed that each movie\n",
    "rating, $y_{i,j}$ could be summarised by an inner product between a\n",
    "vector associated with the item, $\\mathbf{v}_j$ and one associated with\n",
    "the user $\\mathbf{u}_i$. We justified the inner product as a measure of\n",
    "similarity in the space of ‘movie subjects’, where both the users and\n",
    "the items lived, giving the analogy of a library.\n",
    "\n",
    "To make predictions we encouraged the similarity to be high if the movie\n",
    "rating was high using the quadratic error function, $$\n",
    "E_{i,j}(\\mathbf{u}_i, \\mathbf{v}_j) = \\left(\\mathbf{u}_i^\\top \\mathbf{v}_j -\n",
    "y_{i,j}\\right)^2,\n",
    "$$ which we then summed across all the observations to form the total\n",
    "error $$\n",
    "E(\\mathbf{U}, \\mathbf{V}) =\n",
    "\\sum_{i,j}s_{i,j}\\left(\\mathbf{u}_i^\\top \\mathbf{v}_j - y_{i,j}\\right)^2,\n",
    "$$ where $s_{i,j}$ is an indicator variable which is set to 1 if the\n",
    "rating of movie $j$ by user $i$ is provided in our data set. This is\n",
    "known as a sum of squares error.\n",
    "\n",
    "This week we will reinterpret the error as a *probabilistic model*. We\n",
    "will consider the difference between our data and our model to have come\n",
    "from unconsidered factors which exhibit as a probability density. This\n",
    "leads to a more principled definition of least squares error that is\n",
    "originally due to [Carl Friederich\n",
    "Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss), but is\n",
    "mainly inspired by the thinking of [Pierre-Simon\n",
    "Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)."
   ],
   "id": "c0e1a980-ca10-4dd1-876a-2123d717df2e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression: Linear Releationship\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-algebra-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-algebra-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "For many their first encounter with what might be termed a machine\n",
    "learning method is fitting a straight line. A straight line is\n",
    "characterized by two parameters, the scale, $m$, and the offset $c$.\n",
    "\n",
    "$$y_i = m x_i + c$$\n",
    "\n",
    "For the olympic marathon example $y_i$ is the winning pace and it is\n",
    "given as a function of the year which is represented by $x_i$. There are\n",
    "two further parameters of the prediction function. For the olympics\n",
    "example we can interpret these parameters, the scale $m$ is the rate of\n",
    "improvement of the olympic marathon pace on a yearly basis. And $c$ is\n",
    "the winning pace as estimated at year 0."
   ],
   "id": "fbde17c5-70a2-4ae1-934b-5f4e22e2ea0c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overdetermined System\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/overdetermined-system.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/overdetermined-system.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The challenge with a linear model is that it has two unknowns, $m$, and\n",
    "$c$. Observing data allows us to write down a system of simultaneous\n",
    "linear equations. So, for example if we observe two data points, the\n",
    "first with the input value, $x_1 = 1$ and the output value, $y_1 =3$ and\n",
    "a second data point, $x= 3$, $y=1$, then we can write two simultaneous\n",
    "linear equations of the form.\n",
    "\n",
    "point 1: $x= 1$, $y=3$ $$\n",
    "3 = m + c\n",
    "$$ point 2: $x= 3$, $y=1$ $$\n",
    "1 = 3m + c\n",
    "$$\n",
    "\n",
    "The solution to these two simultaneous equations can be represented\n",
    "graphically as\n",
    "\n",
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/over_determined_system003.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The solution of two linear equations represented as the fit\n",
    "of a straight line through two data</i>\n",
    "\n",
    "The challenge comes when a third data point is observed, and it doesn’t\n",
    "fit on the straight line.\n",
    "\n",
    "point 3: $x= 2$, $y=2.5$ $$\n",
    "2.5 = 2m + c\n",
    "$$\n",
    "\n",
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/over_determined_system004.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A third observation of data is inconsistent with the solution\n",
    "dictated by the first two observations</i>\n",
    "\n",
    "Now there are three candidate lines, each consistent with our data.\n",
    "\n",
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/over_determined_system007.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Three solutions to the problem, each consistent with two\n",
    "points of the three observations</i>\n",
    "\n",
    "This is known as an *overdetermined* system because there are more data\n",
    "than we need to determine our parameters. The problem arises because the\n",
    "model is a simplification of the real world, and the data we observe is\n",
    "therefore inconsistent with our model."
   ],
   "id": "57e11957-400c-41e4-92ea-bf650f2cb42b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "97909388-d57c-4db7-b132-dd5b64a00626"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.over_determined_system(diagrams='./ml')"
   ],
   "id": "3fbb5e31-ae4c-4fb2-b6cb-dfd6ca0bbbf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "import notutils as nu"
   ],
   "id": "0c3eb951-0d06-4740-a85d-b08cbb02633c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('over_determined_system{samp:0>3}.svg',\n",
    "                  directory='./ml', \n",
    "                  samp=IntSlider(1,1,7,1))"
   ],
   "id": "765dfaf1-9720-4468-ab0c-be2321128fd0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pierre-Simon Laplace\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/overdetermined-laplace-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/overdetermined-laplace-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The solution was proposed by Pierre-Simon Laplace. His idea was to\n",
    "accept that the model was an incomplete representation of the real\n",
    "world, and the way it was incomplete is *unknown*. His idea was that\n",
    "such unknowns could be dealt with through probability."
   ],
   "id": "eaf92438-3f55-40ea-bc76-f1e0e06ce731"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pierre-Simon Laplace\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_physics/includes/laplace-portrait.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_physics/includes/laplace-portrait.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/Pierre-Simon_Laplace.png\" style=\"width:30%\">\n",
    "\n",
    "Figure: <i>Pierre-Simon Laplace 1749-1827.</i>"
   ],
   "id": "9d7ab00f-b7a1-4574-bd97-20d785667f98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA2')"
   ],
   "id": "df553c21-6624-4603-893c-ead2eeec110d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famously, Laplace considered the idea of a deterministic Universe, one\n",
    "in which the model is *known*, or as the below translation refers to it,\n",
    "“an intelligence which could comprehend all the forces by which nature\n",
    "is animated”. He speculates on an “intelligence” that can submit this\n",
    "vast data to analysis and propsoses that such an entity would be able to\n",
    "predict the future.\n",
    "\n",
    "> Given for one instant an intelligence which could comprehend all the\n",
    "> forces by which nature is animated and the respective situation of the\n",
    "> beings who compose it—an intelligence sufficiently vast to submit\n",
    "> these data to analysis—it would embrace in the same formulate the\n",
    "> movements of the greatest bodies of the universe and those of the\n",
    "> lightest atom; for it, nothing would be uncertain and the future, as\n",
    "> the past, would be present in its eyes.\n",
    "\n",
    "This notion is known as *Laplace’s demon* or *Laplace’s superman*.\n",
    "\n",
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//physics/laplacesDeterminismEnglish.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>Laplace’s determinsim in English translation.</i>"
   ],
   "id": "76c0c55c-e428-4693-ba75-6cfc3442affb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace’s Gremlin\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_physics/includes/laplaces-determinism.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_physics/includes/laplaces-determinism.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Unfortunately, most analyses of his ideas stop at that point, whereas\n",
    "his real point is that such a notion is unreachable. Not so much\n",
    "*superman* as *strawman*. Just three pages later in the “Philosophical\n",
    "Essay on Probabilities” (Laplace, 1814), Laplace goes on to observe:\n",
    "\n",
    "> The curve described by a simple molecule of air or vapor is regulated\n",
    "> in a manner just as certain as the planetary orbits; the only\n",
    "> difference between them is that which comes from our ignorance.\n",
    ">\n",
    "> Probability is relative, in part to this ignorance, in part to our\n",
    "> knowledge."
   ],
   "id": "6fe43ac0-c2ef-4bb5-864c-b8fb2dd1685f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='1YQPAAAAQAAJ', page='PR17-IA4')"
   ],
   "id": "afbcfc8a-7713-4fe2-8508-4e00aa3b8777"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"\" src=\"https://mlatcl.github.io/advds/./slides/diagrams//physics/philosophicaless00lapliala.png\" style=\"width:60%\">\n",
    "\n",
    "Figure: <i>To Laplace, determinism is a strawman. Ignorance of mechanism\n",
    "and data leads to uncertainty which should be dealt with through\n",
    "probability.</i>\n",
    "\n",
    "In other words, we can never make use of the idealistic deterministic\n",
    "Universe due to our ignorance about the world, Laplace’s suggestion, and\n",
    "focus in this essay is that we turn to probability to deal with this\n",
    "uncertainty. This is also our inspiration for using probability in\n",
    "machine learning. This is the true message of Laplace’s essay, not\n",
    "determinism, but the gremlin of uncertainty that emerges from our\n",
    "ignorance.\n",
    "\n",
    "The “forces by which nature is animated” is our *model*, the “situation\n",
    "of beings that compose it” is our *data* and the “intelligence\n",
    "sufficiently vast enough to submit these data to analysis” is our\n",
    "compute. The fly in the ointment is our *ignorance* about these aspects.\n",
    "And *probability* is the tool we use to incorporate this ignorance\n",
    "leading to uncertainty or *doubt* in our predictions."
   ],
   "id": "b4adcdfc-b0a2-43b2-ba15-307943f4cd59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variables\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/laplace-latent-variable-solution.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/laplace-latent-variable-solution.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Laplace’s concept was that the reason that the data doesn’t match up to\n",
    "the model is because of unconsidered factors, and that these might be\n",
    "well represented through probability densities. He tackles the challenge\n",
    "of the unknown factors by adding a variable, $\\epsilon$, that represents\n",
    "the unknown. In modern parlance we would call this a *latent* variable.\n",
    "But in the context Laplace uses it, the variable is so common that it\n",
    "has other names such as a “slack” variable or the *noise* in the system.\n",
    "\n",
    "point 1: $x= 1$, $y=3$ \\[ 3 = m + c + \\_1 \\] point 2: $x= 3$, $y=1$ \\[ 1\n",
    "= 3m + c + \\_2 \\] point 3: $x= 2$, $y=2.5$ \\[ 2.5 = 2m + c + \\_3 \\]\n",
    "\n",
    "Laplace’s trick has converted the *overdetermined* system into an\n",
    "*underdetermined* system. He has now added three variables,\n",
    "$\\{\\epsilon_i\\}_{i=1}^3$, which represent the unknown corruptions of the\n",
    "real world. Laplace’s idea is that we should represent that unknown\n",
    "corruption with a *probability distribution*."
   ],
   "id": "e63a3a32-b3c0-4076-a7d7-ec78fe9f838e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Probabilistic Process\n",
    "\n",
    "However, it was left to an admirer of Laplace to develop a practical\n",
    "probability density for that purpose. It was Carl Friedrich Gauss who\n",
    "suggested that the *Gaussian* density (which at the time was unnamed!)\n",
    "should be used to represent this error.\n",
    "\n",
    "The result is a *noisy* function, a function which has a deterministic\n",
    "part, and a stochastic part. This type of function is sometimes known as\n",
    "a probabilistic or stochastic process, to distinguish it from a\n",
    "deterministic process."
   ],
   "id": "4008a788-35f9-49b2-90e1-82c05eb08e04"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Density\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/univariate-gaussian.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/univariate-gaussian.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The Gaussian density is perhaps the most commonly used probability\n",
    "density. It is defined by a *mean*, $\\mu$, and a *variance*, $\\sigma^2$.\n",
    "The variance is taken to be the square of the *standard deviation*,\n",
    "$\\sigma$.\n",
    "\n",
    "$$\\begin{align}\n",
    "  p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y- \\mu)^2}{2\\sigma^2}\\right)\\\\& \\buildrel\\triangle\\over = \\mathcal{N}\\left(y|\\mu,\\sigma^2\\right)\n",
    "  \\end{align}$$"
   ],
   "id": "b695c9c4-28d3-4bba-8446-5521fc2e6285"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "83f84448-de88-45a0-a675-48bb84f8c294"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.gaussian_of_height(diagrams='./ml')"
   ],
   "id": "429fc6a0-aa9a-41b8-af04-382e28e70f84"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/gaussian_of_height.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The Gaussian PDF with ${\\mu}=1.7$ and variance\n",
    "${\\sigma}^2=0.0225$. Mean shown as red line. It could represent the\n",
    "heights of a population of students.</i>"
   ],
   "id": "67efcfa2-a29e-4b10-82dc-e4aab2758196"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Important Gaussian Properties\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/univariate-gaussian-properties.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/univariate-gaussian-properties.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The Gaussian density has many important properties, but for the moment\n",
    "we’ll review two of them."
   ],
   "id": "0a852ee4-7781-456e-af27-188152e1e83f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Gaussians\n",
    "\n",
    "If we assume that a variable, $y_i$, is sampled from a Gaussian density,\n",
    "\n",
    "$$y_i \\sim \\mathcal{N}\\left(\\mu_i,\\sigma_i^2\\right)$$\n",
    "\n",
    "Then we can show that the sum of a set of variables, each drawn\n",
    "independently from such a density is also distributed as Gaussian. The\n",
    "mean of the resulting density is the sum of the means, and the variance\n",
    "is the sum of the variances,\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n\\mu_i,\\sum_{i=1}^n\\sigma_i^2\\right)\n",
    "$$\n",
    "\n",
    "Since we are very familiar with the Gaussian density and its properties,\n",
    "it is not immediately apparent how unusual this is. Most random\n",
    "variables, when you add them together, change the family of density they\n",
    "are drawn from. For example, the Gaussian is exceptional in this regard.\n",
    "Indeed, other random variables, if they are independently drawn and\n",
    "summed together tend to a Gaussian density. That is the [*central limit\n",
    "theorem*](https://en.wikipedia.org/wiki/Central_limit_theorem) which is\n",
    "a major justification for the use of a Gaussian density."
   ],
   "id": "7c70ca0d-9112-4cd6-87a1-b73d51778a55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling a Gaussian\n",
    "\n",
    "Less unusual is the *scaling* property of a Gaussian density. If a\n",
    "variable, $y$, is sampled from a Gaussian density,\n",
    "\n",
    "$$y\\sim \\mathcal{N}\\left(\\mu,\\sigma^2\\right)$$ and we choose to scale\n",
    "that variable by a *deterministic* value, $w$, then the *scaled\n",
    "variable* is distributed as\n",
    "\n",
    "$$wy\\sim \\mathcal{N}\\left(w\\mu,w^2 \\sigma^2\\right).$$ Unlike the summing\n",
    "properties, where adding two or more random variables independently\n",
    "sampled from a family of densitites typically brings the summed variable\n",
    "*outside* that family, scaling many densities leaves the distribution of\n",
    "that variable in the same *family* of densities. Indeed, many densities\n",
    "include a *scale* parameter (e.g. the [Gamma\n",
    "density](https://en.wikipedia.org/wiki/Gamma_distribution)) which is\n",
    "purely for this purpose. In the Gaussian the standard deviation,\n",
    "$\\sigma$, is the scale parameter. To see why this makes sense, let’s\n",
    "consider, $$z \\sim \\mathcal{N}\\left(0,1\\right),$$ then if we scale by\n",
    "$\\sigma$ so we have, $y=\\sigma z$, we can write,\n",
    "$$y=\\sigma z \\sim \\mathcal{N}\\left(0,\\sigma^2\\right)$$"
   ],
   "id": "9654e170-4195-4ec5-ae7c-04c82c4484f6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace’s Idea\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Laplace had the idea to augment the observations by noise, that is\n",
    "equivalent to considering a probability density whose mean is given by\n",
    "the *prediction function*\n",
    "$$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "This is known as *stochastic process*. It is a function that is\n",
    "corrupted by noise. Laplace didn’t suggest the Gaussian density for that\n",
    "purpose, that was an innovation from Carl Friederich Gauss, which is\n",
    "what gives the Gaussian density its name."
   ],
   "id": "53a94a33-c4ca-4fbb-9d5f-35bd0767ff28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Height as a Function of Weight\n",
    "\n",
    "In the standard Gaussian, parameterized by mean and variance, make the\n",
    "mean a linear function of an *input*.\n",
    "\n",
    "This leads to a regression model. $$\n",
    "\\begin{align*}\n",
    "  y_i=&f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "         \\epsilon_i \\sim & \\mathcal{N}\\left(0,\\sigma^2\\right).\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "Assume $y_i$ is height and $x_i$ is weight."
   ],
   "id": "951cae2b-4915-4c94-8e78-5b8b4aa7f667"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of Squares Error\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/sum-of-squares-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/sum-of-squares-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "7b87545d-fb86-47e3-bc75-9016e864abd7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legendre\n",
    "\n",
    "Minimizing the sum of squares error was first proposed by\n",
    "[Legendre](http://en.wikipedia.org/wiki/Adrien-Marie_Legendre) in 1805\n",
    "(Legendre, 1805). His book, which was on the orbit of comets, is\n",
    "available on google books, we can take a look at the relevant page by\n",
    "calling the code below."
   ],
   "id": "fee09121-6f32-4bb9-ad37-03002bfef889"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "nu.display_google_book(id='spcAAAAAMAAJ', page='PA72')"
   ],
   "id": "f273bb66-bee4-4a84-920d-3b7f6530202c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>Legendre’s book was on the determination of orbits of comets.\n",
    "This page describes the formulation of least squares</i>\n",
    "\n",
    "Of course, the main text is in French, but the key part we are\n",
    "interested in can be roughly translated as\n",
    "\n",
    "> In most matters where we take measures data through observation, the\n",
    "> most accurate results they can offer, it is almost always leads to a\n",
    "> system of equations of the form $$E = a + bx + cy + fz + etc .$$ where\n",
    "> $a$, $b$, $c$, $f$ etc are the known coefficients and $x$, $y$, $z$\n",
    "> etc are unknown and must be determined by the condition that the value\n",
    "> of E is reduced, for each equation, to an amount or zero or very\n",
    "> small.\n",
    "\n",
    "He continues\n",
    "\n",
    "> Of all the principles that we can offer for this item, I think it is\n",
    "> not broader, more accurate, nor easier than the one we have used in\n",
    "> previous research application, and that is to make the minimum sum of\n",
    "> the squares of the errors. By this means, it is between the errors a\n",
    "> kind of balance that prevents extreme to prevail, is very specific to\n",
    "> make known the state of the closest to the truth system. The sum of\n",
    "> the squares of the errors\n",
    "> $E^2 + \\left.E^\\prime\\right.^2 + \\left.E^{\\prime\\prime}\\right.^2 + etc$\n",
    "> being if we wanted a minimum, by varying x alone, we will have the\n",
    "> equation …\n",
    "\n",
    "This is the earliest know printed version of the problem of least\n",
    "squares. The notation, however, is a little awkward for mordern eyes. In\n",
    "particular Legendre doesn’t make use of the sum sign, $$\n",
    "\\sum_{i=1}^3 z_i = z_1 + z_2 + z_3\n",
    "$$ nor does he make use of the inner product.\n",
    "\n",
    "In our notation, if we were to do linear regression, we would need to\n",
    "subsititue: $$\\begin{align*}\n",
    "a &\\leftarrow y_1-c, \\\\ a^\\prime &\\leftarrow y_2-c,\\\\ a^{\\prime\\prime} &\\leftarrow\n",
    "y_3 -c,\\\\ \n",
    "\\text{etc.} \n",
    "\\end{align*}$$ to introduce the data observations $\\{y_i\\}_{i=1}^{n}$\n",
    "alongside $c$, the offset. We would then introduce the input locations\n",
    "$$\\begin{align*}\n",
    "b & \\leftarrow x_1,\\\\\n",
    "b^\\prime & \\leftarrow x_2,\\\\\n",
    "b^{\\prime\\prime} & \\leftarrow x_3\\\\\n",
    "\\text{etc.}\n",
    "\\end{align*}$$ and finally the gradient of the function\n",
    "$$x \\leftarrow -m.$$ The remaining coefficients ($c$ and $f$) would then\n",
    "be zero. That would give us $$\\begin{align*}   &(y_1 -\n",
    "(mx_1+c))^2 \\\\ + &(y_2 -(mx_2 + c))^2\\\\ + &(y_3 -(mx_3 + c))^2 \\\\ + & \\text{etc.}\n",
    "\\end{align*}$$ which we would write in the modern notation for sums as\n",
    "$$\n",
    "\\sum_{i=1}^n(y_i-(mx_i + c))^2\n",
    "$$ which is recognised as the sum of squares error for a linear\n",
    "regression.\n",
    "\n",
    "This shows the advantage of modern [summation\n",
    "operator](http://en.wikipedia.org/wiki/Summation), $\\sum$, in keeping\n",
    "our mathematical notation compact. Whilst it may look more complicated\n",
    "the first time you see it, understanding the mathematical rules that go\n",
    "around it, allows us to go much further with the notation.\n",
    "\n",
    "Inner products (or [dot\n",
    "products](http://en.wikipedia.org/wiki/Dot_product)) are similar. They\n",
    "allow us to write $$\n",
    "\\sum_{i=1}^q u_i v_i\n",
    "$$ in a more compact notation, $\\mathbf{u}\\cdot\\mathbf{v}.$\n",
    "\n",
    "Here we are using bold face to represent vectors, and we assume that the\n",
    "individual elements of a vector $\\mathbf{z}$ are given as a series of\n",
    "scalars $$\n",
    "\\mathbf{z} = \\begin{bmatrix} z_1\\\\ z_2\\\\ \\vdots\\\\ z_n\n",
    "\\end{bmatrix}\n",
    "$$ which are each indexed by their position in the vector."
   ],
   "id": "c3d1608e-8710-40eb-9ffd-0a8254a0b539"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Example: Olympic Marathons\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-linear-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-linear-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Note that `x` and `y` are not `pandas` data frames for this example,\n",
    "they are just arrays of dimensionality $n\\times 1$, where $n$ is the\n",
    "number of data.\n",
    "\n",
    "The aim of this lab is to have you coding linear regression in python.\n",
    "We will do it in two ways, once using iterative updates (coordinate\n",
    "ascent) and then using linear algebra. The linear algebra approach will\n",
    "not only work much better, it is also easy to extend to multiple input\n",
    "linear regression and *non-linear* regression using basis functions."
   ],
   "id": "0c165fed-a099-4781-b15a-3f2de08cfa48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood: Iterative Solution\n",
    "\n",
    "Now we will take the maximum likelihood approach we derived in the\n",
    "lecture to fit a line, $y_i=mx_i + c$, to the data you’ve plotted. We\n",
    "are trying to minimize the error function: $$\n",
    "E(m, c) =  \\sum_{i=1}^n(y_i-mx_i-c)^2\n",
    "$$ with respect to $m$, $c$ and $\\sigma^2$. We can start with an initial\n",
    "guess for $m$,"
   ],
   "id": "21b336d2-4ac6-41c8-97d9-4902bd8fc869"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = -0.4\n",
    "c = 80"
   ],
   "id": "4a8f685c-d38b-4ea6-a0c2-2dff1b22e356"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the maximum likelihood update to find an estimate for the\n",
    "offset, $c$."
   ],
   "id": "943d095f-81c1-4890-b4a3-54b3cadb44b8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-coordinate-ascent.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-coordinate-ascent.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In the movie recommender system example, we minimised the objective\n",
    "function by steepest descent based gradient methods. Our updates\n",
    "required us to compute the gradient at the position we were located,\n",
    "then to update the gradient according to the direction of steepest\n",
    "descent. This time, we will take another approach. It is known as\n",
    "*coordinate descent*. In coordinate descent, we choose to move one\n",
    "parameter at a time. Ideally, we design an algorithm that at each step\n",
    "moves the parameter to its minimum value. At each step we choose to move\n",
    "the individual parameter to its minimum.\n",
    "\n",
    "To find the minimum, we look for the point in the curve where the\n",
    "gradient is zero. This can be found by taking the gradient of $E(m,c)$\n",
    "with respect to the parameter."
   ],
   "id": "5d8bd18b-905e-4b94-a718-ff1cda5de42c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update for Offset\n",
    "\n",
    "Let’s consider the parameter $c$ first. The gradient goes nicely through\n",
    "the summation operator, and we obtain $$\n",
    "\\frac{\\text{d}E(m,c)}{\\text{d}c} = -\\sum_{i=1}^n2(y_i-mx_i-c).\n",
    "$$ Now we want the point that is a minimum. A minimum is an example of a\n",
    "[*stationary point*](http://en.wikipedia.org/wiki/Stationary_point), the\n",
    "stationary points are those points of the function where the gradient is\n",
    "zero. They are found by solving the equation for\n",
    "$\\frac{\\text{d}E(m,c)}{\\text{d}c} = 0$. Substituting in to our gradient,\n",
    "we can obtain the following equation, $$\n",
    "0 = -\\sum_{i=1}^n2(y_i-mx_i-c)\n",
    "$$ which can be reorganised as follows, $$\n",
    "c^* = \\frac{\\sum_{i=1}^n(y_i-m^*x_i)}{n}.\n",
    "$$ The fact that the stationary point is easily extracted in this manner\n",
    "implies that the solution is *unique*. There is only one stationary\n",
    "point for this system. Traditionally when trying to determine the type\n",
    "of stationary point we have encountered we now compute the *second\n",
    "derivative*, $$\n",
    "\\frac{\\text{d}^2E(m,c)}{\\text{d}c^2} = 2n.\n",
    "$$ The second derivative is positive, which in turn implies that we have\n",
    "found a minimum of the function. This means that setting $c$ in this way\n",
    "will take us to the lowest point along that axes."
   ],
   "id": "49b0cb94-f764-47c2-951c-6ec3244bc5f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set c to the minimum\n",
    "c = (y - m*x).mean()\n",
    "print(c)"
   ],
   "id": "65e13498-fa20-4e21-a070-f68e756e46a8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update for Slope\n",
    "\n",
    "Now we have the offset set to the minimum value, in coordinate descent,\n",
    "the next step is to optimise another parameter. Only one further\n",
    "parameter remains. That is the slope of the system.\n",
    "\n",
    "Now we can turn our attention to the slope. We once again peform the\n",
    "same set of computations to find the minima. We end up with an update\n",
    "equation of the following form.\n",
    "\n",
    "$$m^* = \\frac{\\sum_{i=1}^n(y_i - c)x_i}{\\sum_{i=1}^nx_i^2}$$\n",
    "\n",
    "Communication of mathematics in data science is an essential skill, in a\n",
    "moment, you will be asked to rederive the equation above. Before we do\n",
    "that, however, we will briefly review how to write mathematics in the\n",
    "notebook."
   ],
   "id": "d8eb0088-7e1b-404f-b1a7-27e9f645ae9a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\LaTeX$ for Maths\n",
    "\n",
    "These cells use [Markdown\n",
    "format](http://en.wikipedia.org/wiki/Markdown). You can include maths in\n",
    "your markdown using [$\\LaTeX$\n",
    "syntax](http://en.wikipedia.org/wiki/LaTeX), all you have to do is write\n",
    "your answer inside dollar signs, as follows:\n",
    "\n",
    "To write a fraction, we write `$\\frac{a}{b}$`, and it will display like\n",
    "this $\\frac{a}{b}$. To write a subscript we write `$a_b$` which will\n",
    "appear as $a_b$. To write a superscript (for example in a polynomial) we\n",
    "write `$a^b$` which will appear as $a^b$. There are lots of other macros\n",
    "as well, for example we can do greek letters such as\n",
    "`$\\alpha, \\beta, \\gamma$` rendering as $\\alpha, \\beta, \\gamma$. And we\n",
    "can do sum and intergral signs as `$\\sum \\int \\int$`.\n",
    "\n",
    "You can combine many of these operations together for composing\n",
    "expressions."
   ],
   "id": "cc47368a-5eca-4a0d-9d8c-94dd777d6bf9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Convert the following python code expressions into $\\LaTeX$j, writing\n",
    "your answers below. In each case write your answer as a single equality\n",
    "(i.e. your maths should only contain one expression, not several lines\n",
    "of expressions). For the purposes of your $\\LaTeX$ please assume that\n",
    "`x` and `w` are $n$ dimensional vectors.\n",
    "\n",
    "`(a) f = x.sum()`\n",
    "\n",
    "`(b) m = x.mean()`\n",
    "\n",
    "`(c) g = (x*w).sum()`"
   ],
   "id": "1ecc87dd-54bb-4128-9d38-6b71069c8e4f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Answer\n",
    "\n",
    "Write your answer to Exercise 1 here"
   ],
   "id": "9ffebd97-4e0c-4f69-9bb2-2ea4d5c246ec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Point Updates\n",
    "\n",
    "Worked example.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    c^{*}=&\\frac{\\sum\n",
    "_{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n",
    "    m^{*}=&\\frac{\\sum\n",
    "_{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n",
    "\\left.\\sigma^2\\right.^{*}=&\\frac{\\sum\n",
    "_{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "id": "f948535f-abea-4e6c-a731-2dcb6b6197b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient With Respect to the Slope\n",
    "\n",
    "Now that you’ve had a little training in writing maths with $\\LaTeX$, we\n",
    "will be able to use it to answer questions. The next thing we are going\n",
    "to do is a little differentiation practice."
   ],
   "id": "51a3efb6-e8fc-4027-9e46-d8c2b47ce9d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Derive the the gradient of the objective function with respect to the\n",
    "slope, $m$. Rearrange it to show that the update equation written above\n",
    "does find the stationary points of the objective function. By computing\n",
    "its derivative show that it’s a minimum."
   ],
   "id": "2c723121-27b6-4e97-9dee-a72b12811132"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Answer\n",
    "\n",
    "Write your answer to Exercise 2 here"
   ],
   "id": "2e42c7ea-723b-48a0-bfc1-e5a8f4d9870c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ((y - c)*x).sum()/(x**2).sum()\n",
    "print(m)"
   ],
   "id": "aa0317af-5c6d-4955-aff2-16c2c4657bc3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at how good our fit is by computing the prediction\n",
    "across the input space. First create a vector of ‘test points’,"
   ],
   "id": "48222787-2d93-4c0b-98cd-04e1eaa38fd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "id": "091def94-9edc-4b94-9f26-78a6d7c1c92c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(1890, 2020, 130)[:, None]"
   ],
   "id": "9c8ff2bc-6378-410e-bb3e-ffb0bf88bfa8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this vector to compute some test predictions,"
   ],
   "id": "cbed815d-312a-41b3-b97f-e2f3dcb54f7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = m*x_test + c"
   ],
   "id": "932994c9-4463-47a0-b030-7b174d227f28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot those test predictions with a blue line on the same plot as the\n",
    "data,"
   ],
   "id": "9ae75486-4500-4a46-8d70-f24839aff98f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "id": "d621e219-440a-4650-bb45-7c8f5dfd2969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ],
   "id": "80440e7d-1ba2-4421-9d39-7c41f7491302"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit isn’t very good, we need to iterate between these parameter\n",
    "updates in a loop to improve the fit, we have to do this several times,"
   ],
   "id": "9a384c69-ecfe-4893-a6ab-1bb2ce5ce341"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(10):\n",
    "    m = ((y - c)*x).sum()/(x*x).sum()\n",
    "    c = (y-m*x).sum()/y.shape[0]\n",
    "print(m)\n",
    "print(c)"
   ],
   "id": "a96e14d7-580e-412f-8841-86e706a7a74d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let’s try plotting the result again"
   ],
   "id": "63588fe2-5a0c-43a6-97c0-913cb6330c66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = m*x_test + c\n",
    "plt.plot(x_test, f_test, 'b-')\n",
    "plt.plot(x, y, 'rx')"
   ],
   "id": "63904ebb-7a63-4518-88f5-8b30cd3ec1f4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we need more iterations than 10! In the next question you will\n",
    "add more iterations and report on the error as optimisation proceeds."
   ],
   "id": "14b8e67a-9b63-4510-93d1-2625a8c8cf1c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "There is a problem here, we seem to need many interations to get to a\n",
    "good solution. Let’s explore what’s going on. Write code which\n",
    "alternates between updates of `c` and `m`. Include the following\n",
    "features in your code.\n",
    "\n",
    "1.  Initialise with `m=-0.4` and `c=80`.\n",
    "2.  Every 10 iterations compute the value of the objective function for\n",
    "    the training data and print it to the screen (you’ll find hints on\n",
    "    this in [the lab from last week](./week2.ipynb)).\n",
    "3.  Cause the code to stop running when the error change over less than\n",
    "    10 iterations is smaller than $1\\times10^{-4}$. This is known as a\n",
    "    stopping criterion.\n",
    "\n",
    "Why do we need so many iterations to get to the solution?"
   ],
   "id": "cd3f0724-3815-4faf-af3c-600ff4679890"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer to Exercise 3 here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b20ae35e-48aa-471b-9df4-6ee6bf57bd65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Concepts Not Covered\n",
    "\n",
    "-   Other optimization methods:\n",
    "    -   Second order methods, conjugate gradient, quasi-Newton and\n",
    "        Newton.\n",
    "-   Effective heuristics such as momentum.\n",
    "-   Local vs global solutions."
   ],
   "id": "954a5b46-728c-4c44-a416-ef5c9e6e2c06"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions and Regression\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "66ce4ced-7a74-4547-9829-8f724fa71eda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlai"
   ],
   "id": "1ac42534-590e-48e3-8607-e16f089cc23f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(4, 1))"
   ],
   "id": "58fa2e91-4d3b-4bef-a4ae-7510f3423c19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_true = 1.4\n",
    "c_true = -3.1"
   ],
   "id": "09292df8-a370-4e9a-aed7-1387fe4aa089"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m_true*x+c_true"
   ],
   "id": "84b5330e-5a86-4966-9062-3d9277087038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "id": "e7b7df63-959a-4695-94ef-0811908037fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'r.', markersize=10) # plot data as red dots\n",
    "plt.xlim([-3, 3])\n",
    "mlai.write_figure(filename='regression.svg', directory='./ml', transparent=True)"
   ],
   "id": "d522cf08-b57f-4d85-b167-632c3ff2512d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/regression.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A simple linear regression.</i>"
   ],
   "id": "9e64c179-81e3-4f90-8561-d716edd0f3ab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Corrupted Plot"
   ],
   "id": "f93b3ac2-3046-4359-8a02-c934e9749736"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(scale=0.5, size=(4, 1)) # standard deviation of the noise is 0.5\n",
    "y = m_true*x + c_true + noise\n",
    "plt.plot(x, y, 'r.', markersize=10)\n",
    "plt.xlim([-3, 3])\n",
    "mlai.write_figure(filename='regression_noise.svg', directory='./ml', transparent=True)"
   ],
   "id": "ba10b1ce-dc80-4aad-9090-1306a4f72167"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/regression_noise.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>A simple linear regression with noise.</i>"
   ],
   "id": "930d9c41-069a-4c80-8e2d-35d415e859a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Plot of Error Function\n",
    "\n",
    "-   Visualise the error function surface, create vectors of values."
   ],
   "id": "ab30b9b1-3291-4c27-937e-0fe99307b6cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of linearly separated values around m_true\n",
    "m_vals = np.linspace(m_true-3, m_true+3, 100) \n",
    "# create an array of linearly separated values ae\n",
    "c_vals = np.linspace(c_true-3, c_true+3, 100)"
   ],
   "id": "8ecca2e0-28ff-4739-9e3d-2d7fb99a2077"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   create a grid of values to evaluate the error function in 2D."
   ],
   "id": "03be48b3-9bc1-43c5-b493-2d20f266b718"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grid, c_grid = np.meshgrid(m_vals, c_vals)"
   ],
   "id": "abbda9da-935e-48ab-adae-c34dddc94662"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   compute the error function at each combination of $c$ and $m$."
   ],
   "id": "89854282-1152-4491-848d-47b8fe139a54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_grid = np.zeros((100, 100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        E_grid[i, j] = ((y - m_grid[i, j]*x - c_grid[i, j])**2).sum()"
   ],
   "id": "2fc1817f-ba57-4707-bca3-991abf43e9d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Plot of Error"
   ],
   "id": "b7276fb9-a218-4216-9504-b5f7d8b8e13b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "c397b7e9-3e45-42d9-8ea8-b778f31f122d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "plot.regression_contour(f, ax, m_vals, c_vals, E_grid)\n",
    "mlai.write_figure(filename='regression_contour.svg', directory='./ml')"
   ],
   "id": "86c662fe-3c46-4abf-8035-6355fd6fe576"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/regression_contour.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Contours of the objective function for linear regression by\n",
    "minimizing least squares.</i>"
   ],
   "id": "630a704b-fb80-4787-b2b2-d658d37edde8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent"
   ],
   "id": "7ae3dffa-e136-4c30-8702-7d0f083032d9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "-   We start with a guess for $m$ and $c$."
   ],
   "id": "9b2d76bf-2cdf-4f67-9876-179f2ffd02d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_star = 0.0\n",
    "c_star = -5.0"
   ],
   "id": "d9340650-2d2d-4120-bf2d-6233b8440e88"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offset Gradient\n",
    "\n",
    "-   Now we need to compute the gradient of the error function, firstly\n",
    "    with respect to $c$, $$\n",
    "      \\frac{\\text{d}E(m, c)}{\\text{d} c} = -2\\sum_{i=1}^n(y_i - mx_i - c)\n",
    "      $$\n",
    "\n",
    "-   This is computed in python as follows"
   ],
   "id": "be3473e8-630d-4a10-8116-af59ceac9279"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_grad = -2*(y-m_star*x - c_star).sum()\n",
    "print(\"Gradient with respect to c is \", c_grad)"
   ],
   "id": "f0bb0812-7d59-403d-84b1-a0f2bd910e79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the Gradient\n",
    "\n",
    "To see how the gradient was derived, first note that the $c$ appears in\n",
    "every term in the sum. So we are just differentiating\n",
    "$(y_i - mx_i - c)^2$ for each term in the sum. The gradient of this term\n",
    "with respect to $c$ is simply the gradient of the outer quadratic,\n",
    "multiplied by the gradient with respect to $c$ of the part inside the\n",
    "quadratic. The gradient of a quadratic is two times the argument of the\n",
    "quadratic, and the gradient of the inside linear term is just minus one.\n",
    "This is true for all terms in the sum, so we are left with the sum in\n",
    "the gradient."
   ],
   "id": "fbf38d31-9326-4a77-a385-160e33652f16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slope Gradient\n",
    "\n",
    "The gradient with respect tom $m$ is similar, but now the gradient of\n",
    "the quadratic’s argument is $-x_i$ so the gradient with respect to $m$\n",
    "is\n",
    "\n",
    "$$\\frac{\\text{d}E(m, c)}{\\text{d} m} = -2\\sum_{i=1}^nx_i(y_i - mx_i -\n",
    "c)$$\n",
    "\n",
    "which can be implemented in python (numpy) as"
   ],
   "id": "5ba4e5d7-ea49-41c2-a621-8b1534b5088d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_grad = -2*(x*(y-m_star*x - c_star)).sum()\n",
    "print(\"Gradient with respect to m is \", m_grad)"
   ],
   "id": "42968024-f231-4c0f-8956-dcc08a907d6e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Equations\n",
    "\n",
    "-   Now we have gradients with respect to $m$ and $c$.\n",
    "-   Can update our inital guesses for $m$ and $c$ using the gradient.\n",
    "-   We don’t want to just subtract the gradient from $m$ and $c$,\n",
    "-   We need to take a *small* step in the gradient direction.\n",
    "-   Otherwise we might overshoot the minimum.\n",
    "-   We want to follow the gradient to get to the minimum, the gradient\n",
    "    changes all the time."
   ],
   "id": "c870dc93-ca71-4a7c-8d15-39808f921a25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move in Direction of Gradient"
   ],
   "id": "11052676-b97a-4a36-a1cb-761fcb37ca74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai.plot as plot"
   ],
   "id": "73668255-5160-4da4-aefc-519e685aae92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "plot.regression_contour(f, ax, m_vals, c_vals, E_grid)\n",
    "ax.plot(m_star, c_star, 'g*', markersize=20)\n",
    "ax.arrow(m_star, c_star, -m_grad*0.1, -c_grad*0.1, head_width=0.2)\n",
    "mlai.write_figure(filename='regression_contour_step001.svg', directory='./ml/', transparent=True)"
   ],
   "id": "f33f7e8f-b1f7-4382-8549-f769f08867d9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/regression_contour_step001.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Single update descending the contours of the error surface\n",
    "for regression.</i>"
   ],
   "id": "f8742a50-cc4b-4735-b1e6-04930ddbdd07"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Equations\n",
    "\n",
    "-   The step size has already been introduced, it’s again known as the\n",
    "    learning rate and is denoted by $\\eta$. $$\n",
    "    c_\\text{new}\\leftarrow c_{\\text{old}} - \\eta\\frac{\\text{d}E(m, c)}{\\text{d}c}\n",
    "    $$\n",
    "\n",
    "-   gives us an update for our estimate of $c$ (which in the code we’ve\n",
    "    been calling `c_star` to represent a common way of writing a\n",
    "    parameter estimate, $c^*$) and $$\n",
    "    m_\\text{new} \\leftarrow m_{\\text{old}} - \\eta\\frac{\\text{d}E(m, c)}{\\text{d}m}\n",
    "    $$\n",
    "\n",
    "-   Giving us an update for $m$."
   ],
   "id": "665e0b26-2315-4d06-b9bb-72cac76acdc3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Code\n",
    "\n",
    "-   These updates can be coded as"
   ],
   "id": "ebb49161-b380-43c0-8c66-62701fd01e8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original m was\", m_star, \"and original c was\", c_star)\n",
    "learn_rate = 0.01\n",
    "c_star = c_star - learn_rate*c_grad\n",
    "m_star = m_star - learn_rate*m_grad\n",
    "print(\"New m is\", m_star, \"and new c is\", c_star)"
   ],
   "id": "2f1f4eab-b3bc-4b29-a44a-6c7d0b9734f9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating Updates\n",
    "\n",
    "-   Fit model by descending gradient."
   ],
   "id": "58274522-3f97-46b3-b378-22507ba8e55c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm"
   ],
   "id": "0e5e4d81-a678-4bde-9563-428656bcf322"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = plot.regression_contour_fit(x, y, diagrams='./ml')"
   ],
   "id": "c2e3c461-9a12-44f9-af70-b5b08ce80918"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "e3ab210c-6565-4b4b-91ca-6e201bc81c73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "692c16dc-d400-4bec-b041-d5be20dfa6e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('regression_contour_fit{num:0>3}.svg', directory='./ml', num=IntSlider(0, 0, num_plots, 1))"
   ],
   "id": "9157d655-a8b1-4761-bc46-c240d876d0d4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/regression_contour_fit028.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Batch gradient descent for linear regression</i>"
   ],
   "id": "050e6d8e-4dbf-4f69-981d-c75459b5c49c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "-   If $n$ is small, gradient descent is fine.\n",
    "-   But sometimes (e.g. on the internet $n$ could be a billion.\n",
    "-   Stochastic gradient descent is more similar to perceptron.\n",
    "-   Look at gradient of one data point at a time rather than summing\n",
    "    across *all* data points)\n",
    "-   This gives a stochastic estimate of gradient."
   ],
   "id": "39145d86-ed5c-42d8-ac95-291464d78300"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "-   The real gradient with respect to $m$ is given by\n",
    "\n",
    "    $$\\frac{\\text{d}E(m, c)}{\\text{d} m} = -2\\sum_{i=1}^nx_i(y_i -\n",
    "    mx_i - c)$$\n",
    "\n",
    "    but it has $n$ terms in the sum. Substituting in the gradient we can\n",
    "    see that the full update is of the form\n",
    "\n",
    "    $$m_\\text{new} \\leftarrow\n",
    "    m_\\text{old} + 2\\eta\\left[x_1 (y_1 - m_\\text{old}x_1 - c_\\text{old}) + (x_2 (y_2 -   m_\\text{old}x_2 - c_\\text{old}) + \\dots + (x_n (y_n - m_\\text{old}x_n - c_\\text{old})\\right]$$\n",
    "\n",
    "    This could be split up into lots of individual updates\n",
    "    $$m_1 \\leftarrow m_\\text{old} + 2\\eta\\left[x_1 (y_1 - m_\\text{old}x_1 -\n",
    "    c_\\text{old})\\right]$$ $$m_2 \\leftarrow m_1 + 2\\eta\\left[x_2 (y_2 -\n",
    "    m_\\text{old}x_2 - c_\\text{old})\\right]$$\n",
    "    $$m_3 \\leftarrow m_2 + 2\\eta\n",
    "    \\left[\\dots\\right]$$\n",
    "    $$m_n \\leftarrow m_{n-1} + 2\\eta\\left[x_n (y_n -\n",
    "    m_\\text{old}x_n - c_\\text{old})\\right]$$\n",
    "\n",
    "which would lead to the same final update."
   ],
   "id": "fa3e22d4-84db-4a5e-9d8a-0a34174b9b02"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating $c$ and $m$\n",
    "\n",
    "-   In the sum we don’t $m$ and $c$ we use for computing the gradient\n",
    "    term at each update.\n",
    "-   In stochastic gradient descent we *do* change them.\n",
    "-   This means it’s not quite the same as steepest desceint.\n",
    "-   But we can present each data point in a random order, like we did\n",
    "    for the perceptron.\n",
    "-   This makes the algorithm suitable for large scale web use (recently\n",
    "    this domain is know as ‘Big Data’) and algorithms like this are\n",
    "    widely used by Google, Microsoft, Amazon, Twitter and Facebook."
   ],
   "id": "54bc3d86-c159-48f2-b29e-98fcfab10fec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "-   Or more accurate, since the data is normally presented in a random\n",
    "    order we just can write $$\n",
    "    m_\\text{new} = m_\\text{old} + 2\\eta\\left[x_i (y_i - m_\\text{old}x_i - c_\\text{old})\\right]\n",
    "    $$"
   ],
   "id": "db2f1b5b-86d0-4af0-8223-ba75ae1feaab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random point for the update \n",
    "i = np.random.randint(x.shape[0]-1)\n",
    "# update m\n",
    "m_star = m_star + 2*learn_rate*(x[i]*(y[i]-m_star*x[i] - c_star))\n",
    "# update c\n",
    "c_star = c_star + 2*learn_rate*(y[i]-m_star*x[i] - c_star)"
   ],
   "id": "c02ec10d-fae4-4793-90aa-e79ca297e575"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD for Linear Regression\n",
    "\n",
    "Putting it all together in an algorithm, we can do stochastic gradient\n",
    "descent for our regression data."
   ],
   "id": "e05416a2-a123-4b72-a1ea-c8e0968fdfa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = plot.regression_contour_sgd(x, y, diagrams='./ml')"
   ],
   "id": "1a9f9fcd-38a1-4060-a85f-c7b426546081"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu\n",
    "from ipywidgets import IntSlider"
   ],
   "id": "efcaa8da-b450-429b-bc38-dbde5629844e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils as nu"
   ],
   "id": "890aa1c1-f53e-44e8-8f50-4942cf071304"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu.display_plots('regression_sgd_contour_fit{num:0>3}.svg', \n",
    "    directory='./ml', num=IntSlider(0, 0, num_plots, 1))"
   ],
   "id": "5de0e5ad-545e-4c85-9c8c-840c5a00a7b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/regression_sgd_contour_fit058.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Stochastic gradient descent for linear regression.</i>"
   ],
   "id": "b0aa3760-47db-4b81-bc56-22a278b6d789"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection on Linear Regression and Supervised Learning\n",
    "\n",
    "Think about:\n",
    "\n",
    "1.  What effect does the learning rate have in the optimization? What’s\n",
    "    the effect of making it too small, what’s the effect of making it\n",
    "    too big? Do you get the same result for both stochastic and steepest\n",
    "    gradient descent?\n",
    "\n",
    "2.  The stochastic gradient descent doesn’t help very much for such a\n",
    "    small data set. It’s real advantage comes when there are many,\n",
    "    you’ll see this in the lab."
   ],
   "id": "f394d60d-47e1-4d1a-b527-374c2b4934e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood for Multivariate Regression\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-multivariate-log-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-multivariate-log-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
   ],
   "id": "b56775a8-34f4-4b8a-b9c3-6c878b70632f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Loss\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-direct-solution.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-direct-solution.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Now we’ve identified the empirical risk with the loss, we’ll use\n",
    "$E(\\mathbf{ w})$ to represent our objective function. $$\n",
    "E(\\mathbf{ w}) = \\sum_{i=1}^n\\left(y_i - f(\\mathbf{ x}_i, \\mathbf{ w})\\right)^2\n",
    "$$ gives us our objective.\n",
    "\n",
    "In the case of the linear prediction function, we can substitute\n",
    "$f(\\mathbf{ x}_i, \\mathbf{ w}) = \\mathbf{ w}^\\top \\mathbf{ x}_i$. $$\n",
    "E(\\mathbf{ w}) = \\sum_{i=1}^n\\left(y_i - \\mathbf{ w}^\\top \\mathbf{ x}_i\\right)^2\n",
    "$$ To compute the gradient of the objective, we first expand the\n",
    "brackets."
   ],
   "id": "6d0de3d4-f9fa-471d-b029-eacd514ce939"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bracket Expansion\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{ w},\\sigma^2)  = &\n",
    "\\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum\n",
    "_{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum\n",
    "_{i=1}^{n}y_i\\mathbf{ w}^{\\top}\\mathbf{ x}_i\\\\&+\\frac{1}{2\\sigma^2}\\sum\n",
    "_{i=1}^{n}\\mathbf{ w}^{\\top}\\mathbf{ x}_i\\mathbf{ x}_i^{\\top}\\mathbf{ w}\n",
    "+\\text{const}.\\\\\n",
    "    = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum\n",
    "_{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "\\mathbf{ w}^\\top\\sum_{i=1}^{n}\\mathbf{ x}_iy_i\\\\&+\\frac{1}{2\\sigma^2}\n",
    "\\mathbf{ w}^{\\top}\\left[\\sum\n",
    "_{i=1}^{n}\\mathbf{ x}_i\\mathbf{ x}_i^{\\top}\\right]\\mathbf{ w}+\\text{const}.\n",
    "\\end{align*}\n",
    "$$"
   ],
   "id": "e32feded-a021-4c41-992e-ecb9c421e380"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution with Linear Algebra\n",
    "\n",
    "In this section we’re going compute the minimum of the quadratic loss\n",
    "with respect to the parameters. When we do this, we’ll also review\n",
    "*linear algebra*. We will represent all our errors and functions in the\n",
    "form of matrices and vectors.\n",
    "\n",
    "Linear algebra is just a shorthand for performing lots of\n",
    "multiplications and additions simultaneously. What does it have to do\n",
    "with our system then? Well, the first thing to note is that the classic\n",
    "linear function we fit for a one-dimensional regression has the form: $$\n",
    "f(x) = mx + c\n",
    "$$ the classical form for a straight line. From a linear algebraic\n",
    "perspective, we are looking for multiplications and additions. We are\n",
    "also looking to separate our parameters from our data. The data is the\n",
    "*givens*. In French the word is données literally translated means\n",
    "*givens* that’s great, because we don’t need to change the data, what we\n",
    "need to change are the parameters (or variables) of the model. In this\n",
    "function the data comes in through $x$, and the parameters are $m$ and\n",
    "$c$.\n",
    "\n",
    "What we’d like to create is a vector of parameters and a vector of data.\n",
    "Then we could represent the system with vectors that represent the data,\n",
    "and vectors that represent the parameters.\n",
    "\n",
    "We look to turn the multiplications and additions into a linear\n",
    "algebraic form, we have one multiplication ($m\\times c$) and one\n",
    "addition ($mx + c$). But we can turn this into an inner product by\n",
    "writing it in the following way, $$\n",
    "f(x) = m \\times x +\n",
    "c \\times 1,\n",
    "$$ in other words, we’ve extracted the unit value from the offset, $c$.\n",
    "We can think of this unit value like an extra item of data, because it\n",
    "is always given to us, and it is always set to 1 (unlike regular data,\n",
    "which is likely to vary!). We can therefore write each input data\n",
    "location, $\\mathbf{ x}$, as a vector $$\n",
    "\\mathbf{ x}= \\begin{bmatrix} 1\\\\ x\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now we choose to also turn our parameters into a vector. The parameter\n",
    "vector will be defined to contain $$\n",
    "\\mathbf{ w}= \\begin{bmatrix} c \\\\ m\\end{bmatrix}\n",
    "$$ because if we now take the inner product between these two vectors we\n",
    "recover $$\n",
    "\\mathbf{ x}\\cdot\\mathbf{ w}= 1 \\times c + x \\times m = mx + c\n",
    "$$ In `numpy` we can define this vector as follows"
   ],
   "id": "e2b2cb62-2237-4c9c-bcdc-bf547f17f942"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "id": "273eac02-5de0-47ca-bfb0-1b87f3490e64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vector w\n",
    "w = np.zeros(shape=(2, 1))\n",
    "w[0] = m\n",
    "w[1] = c"
   ],
   "id": "1797569b-820c-44ed-b0ce-e0ce99fdff9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the equivalence between original operation and an\n",
    "operation in vector space. Whilst the notation here isn’t a lot shorter,\n",
    "the beauty is that we will be able to add as many features as we like\n",
    "and keep the same representation. In general, we are now moving to a\n",
    "system where each of our predictions is given by an inner product. When\n",
    "we want to represent a linear product in linear algebra, we tend to do\n",
    "it with the transpose operation, so since we have\n",
    "$\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{a}^\\top\\mathbf{b}$ we can write $$\n",
    "f(\\mathbf{ x}_i) = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
    "$$ Where we’ve assumed that each data point, $\\mathbf{ x}_i$, is now\n",
    "written by appending a 1 onto the original vector $$\n",
    "\\mathbf{ x}_i = \\begin{bmatrix} \n",
    "1 \\\\\n",
    "x_i\n",
    "\\end{bmatrix}\n",
    "$$"
   ],
   "id": "5db80eda-30e9-428e-b069-a8f1bf87ffad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Matrix\n",
    "\n",
    "We can do this for the entire data set to form a [*design\n",
    "matrix*](http://en.wikipedia.org/wiki/Design_matrix) $\\mathbf{X}$, $$\n",
    "\\mathbf{X}\n",
    "= \\begin{bmatrix} \n",
    "\\mathbf{ x}_1^\\top \\\\\\ \n",
    "\\mathbf{ x}_2^\\top \\\\\\ \n",
    "\\vdots \\\\\\\n",
    "\\mathbf{ x}_n^\\top\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\\\n",
    "1 & x_2 \\\\\\\n",
    "\\vdots\n",
    "& \\vdots \\\\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix},\n",
    "$$ which in `numpy` can be done with the following commands:"
   ],
   "id": "d3892a39-d50e-4e94-90cf-0c104c1a1381"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "id": "5971e069-8d0c-40b2-9a22-62351cb9cdc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones_like(x), x))\n",
    "print(X)"
   ],
   "id": "b223091a-c7b9-4a2d-a776-74f9cc8245b1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Objective with Linear Algebra\n",
    "\n",
    "When we think of the objective function, we can think of it as the\n",
    "errors where the error is defined in a similar way to what it was in\n",
    "Legendre’s day $y_i - f(\\mathbf{ x}_i)$, in statistics these errors are\n",
    "also sometimes called\n",
    "[*residuals*](http://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics).\n",
    "So, we can think as the objective and the prediction function as two\n",
    "separate parts, first we have, $$\n",
    "E(\\mathbf{ w}) = \\sum_{i=1}^n(y_i - f(\\mathbf{ x}_i; \\mathbf{ w}))^2,\n",
    "$$ where we’ve made the function $f(\\cdot)$’s dependence on the\n",
    "parameters $\\mathbf{ w}$ explicit in this equation. Then we have the\n",
    "definition of the function itself, $$\n",
    "f(\\mathbf{ x}_i; \\mathbf{ w}) = \\mathbf{ x}_i^\\top \\mathbf{ w}.\n",
    "$$ Let’s look again at these two equations and see if we can identify\n",
    "any inner products. The first equation is a sum of squares, which is\n",
    "promising. Any sum of squares can be represented by an inner product, $$\n",
    "a = \\sum_{i=1}^{k} b^2_i = \\mathbf{b}^\\top\\mathbf{b}.\n",
    "$$ If we wish to represent $E(\\mathbf{ w})$ in this way, all we need to\n",
    "do is convert the sum operator to an inner product. We can get a vector\n",
    "from that sum operator by placing both $y_i$ and\n",
    "$f(\\mathbf{ x}_i; \\mathbf{ w})$ into vectors, which we do by defining $$\n",
    "\\mathbf{ y}= \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}\n",
    "$$ and defining $$\n",
    "\\mathbf{ f}(\\mathbf{ x}_1; \\mathbf{ w}) = \\begin{bmatrix}f(\\mathbf{ x}_1; \\mathbf{ w})\\\\ f(\\mathbf{ x}_2; \\mathbf{ w})\\\\ \\vdots \\\\ f(\\mathbf{ x}_n; \\mathbf{ w})\\end{bmatrix}.\n",
    "$$ The second of these is a vector-valued function. This term may appear\n",
    "intimidating, but the idea is straightforward. A vector valued function\n",
    "is simply a vector whose elements are themselves defined as *functions*,\n",
    "i.e., it is a vector of functions, rather than a vector of scalars. The\n",
    "idea is so straightforward, that we are going to ignore it for the\n",
    "moment, and barely use it in the derivation. But it will reappear later\n",
    "when we introduce *basis functions*. So, we will for the moment ignore\n",
    "the dependence of $\\mathbf{ f}$ on $\\mathbf{ w}$ and $\\mathbf{X}$ and\n",
    "simply summarise it by a vector of numbers $$\n",
    "\\mathbf{ f}= \\begin{bmatrix}f_1\\\\f_2\\\\\n",
    "\\vdots \\\\ f_n\\end{bmatrix}.\n",
    "$$ This allows us to write our objective in the folowing, linear\n",
    "algebraic form, $$\n",
    "E(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
    "$$ from the rules of inner products. But what of our matrix $\\mathbf{X}$\n",
    "of input data? At this point, we need to dust off [*matrix-vector\n",
    "multiplication*](http://en.wikipedia.org/wiki/Matrix_multiplication).\n",
    "Matrix multiplication is simply a convenient way of performing many\n",
    "inner products together, and it’s exactly what we need to summarize the\n",
    "operation $$\n",
    "f_i = \\mathbf{ x}_i^\\top\\mathbf{ w}.\n",
    "$$ This operation tells us that each element of the vector $\\mathbf{ f}$\n",
    "(our vector valued function) is given by an inner product between\n",
    "$\\mathbf{ x}_i$ and $\\mathbf{ w}$. In other words, it is a series of\n",
    "inner products. Let’s look at the definition of matrix multiplication,\n",
    "it takes the form $$\n",
    "\\mathbf{c} = \\mathbf{B}\\mathbf{a},\n",
    "$$ where $\\mathbf{c}$ might be a $k$ dimensional vector (which we can\n",
    "interpret as a $k\\times 1$ dimensional matrix), and $\\mathbf{B}$ is a\n",
    "$k\\times k$ dimensional matrix and $\\mathbf{a}$ is a $k$ dimensional\n",
    "vector ($k\\times 1$ dimensional matrix).\n",
    "\n",
    "The result of this multiplication is of the form $$\n",
    "\\begin{bmatrix}c_1\\\\c_2 \\\\ \\vdots \\\\\n",
    "a_k\\end{bmatrix} = \n",
    "\\begin{bmatrix} b_{1,1} & b_{1, 2} & \\dots & b_{1, k} \\\\\n",
    "b_{2, 1} & b_{2, 2} & \\dots & b_{2, k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k, 1} & b_{k, 2} & \\dots & b_{k, k} \\end{bmatrix} \\begin{bmatrix}a_1\\\\a_2 \\\\\n",
    "\\vdots\\\\ c_k\\end{bmatrix} = \\begin{bmatrix} b_{1, 1}a_1 + b_{1, 2}a_2 + \\dots +\n",
    "b_{1, k}a_k\\\\\n",
    "b_{2, 1}a_1 + b_{2, 2}a_2 + \\dots + b_{2, k}a_k \\\\ \n",
    "\\vdots\\\\\n",
    "b_{k, 1}a_1 + b_{k, 2}a_2 + \\dots + b_{k, k}a_k\\end{bmatrix}.\n",
    "$$ We see that each element of the result, $\\mathbf{a}$ is simply the\n",
    "inner product between each *row* of $\\mathbf{B}$ and the vector\n",
    "$\\mathbf{c}$. Because we have defined each element of $\\mathbf{ f}$ to\n",
    "be given by the inner product between each *row* of the design matrix\n",
    "and the vector $\\mathbf{ w}$ we now can write the full operation in one\n",
    "matrix multiplication,\n",
    "\n",
    "$$\n",
    "\\mathbf{ f}= \\mathbf{X}\\mathbf{ w}.\n",
    "$$"
   ],
   "id": "3902a6bd-5d40-4736-8d86-5bc50ac34c3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "id": "266e87ad-d18c-4604-93aa-a45ecf0acb52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = X@w # The @ sign performs matrix multiplication"
   ],
   "id": "3a0a4ade-87d1-4ae4-8096-5eea59ba3da1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this result with our objective function, $$\n",
    "E(\\mathbf{ w}) = (\\mathbf{ y}- \\mathbf{ f})^\\top(\\mathbf{ y}- \\mathbf{ f})\n",
    "$$ we find we have defined the *model* with two equations. One equation\n",
    "tells us the form of our predictive function and how it depends on its\n",
    "parameters, the other tells us the form of our objective function."
   ],
   "id": "b30be651-b4ef-4f82-9b85-9a0822254444"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = (y-f)\n",
    "E = np.dot(resid.T, resid) # matrix multiplication on a single vector is equivalent to a dot product.\n",
    "print(\"Error function is:\", E)"
   ],
   "id": "545343d0-dd9d-453e-9c43-b81534036445"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "The prediction for our movie recommender system had the form $$\n",
    "f_{i,j} = \\mathbf{u}_i^\\top \\mathbf{v}_j\n",
    "$$ and the objective function was then $$\n",
    "E = \\sum_{i,j} s_{i,j}(y_{i,j} - f_{i, j})^2\n",
    "$$ Try writing this down in matrix and vector form. How many of the\n",
    "terms can you do? For each variable and parameter carefully think about\n",
    "whether it should be represented as a matrix or vector. Do as many of\n",
    "the terms as you can. Use $\\LaTeX$ to give your answers and give the\n",
    "*dimensions* of any matrices you create."
   ],
   "id": "de404937-12af-4dd5-a31f-fa7725bf8e92"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Answer\n",
    "\n",
    "Write your answer to Exercise 4 here"
   ],
   "id": "95fe1981-719d-4028-9dd1-bebed401da9e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Optimization\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-objective-optimisation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/linear-regression-objective-optimisation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Our *model* has now been defined with two equations: the prediction\n",
    "function and the objective function. Now we will use multivariate\n",
    "calculus to define an *algorithm* to fit the model. The separation\n",
    "between model and algorithm is important and is often overlooked. Our\n",
    "model contains a function that shows how it will be used for prediction,\n",
    "and a function that describes the objective function we need to optimize\n",
    "to obtain a good set of parameters.\n",
    "\n",
    "The model linear regression model we have described is still the same as\n",
    "the one we fitted above with a coordinate ascent algorithm. We have only\n",
    "played with the notation to obtain the same model in a matrix and vector\n",
    "notation. However, we will now fit this model with a different\n",
    "algorithm, one that is much faster. It is such a widely used algorithm\n",
    "that from the end user’s perspective it doesn’t even look like an\n",
    "algorithm, it just appears to be a single operation (or function).\n",
    "However, underneath the computer calls an algorithm to find the\n",
    "solution. Further, the algorithm we obtain is very widely used, and\n",
    "because of this it turns out to be highly optimized.\n",
    "\n",
    "Once again, we are going to try and find the stationary points of our\n",
    "objective by finding the *stationary points*. However, the stationary\n",
    "points of a multivariate function, are a little bit more complex to\n",
    "find. As before we need to find the point at which the gradient is zero,\n",
    "but now we need to use *multivariate calculus* to find it. This involves\n",
    "learning a few additional rules of differentiation (that allow you to do\n",
    "the derivatives of a function with respect to vector), but in the end it\n",
    "makes things quite a bit easier. We define vectorial derivatives as\n",
    "follows, $$\n",
    "\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}} =\n",
    "\\begin{bmatrix}\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_1}\\\\\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_2}\\end{bmatrix}.\n",
    "$$ where $\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}w_1}$ is the [partial\n",
    "derivative](http://en.wikipedia.org/wiki/Partial_derivative) of the\n",
    "error function with respect to $w_1$.\n",
    "\n",
    "Differentiation through multiplications and additions is relatively\n",
    "straightforward, and since linear algebra is just multiplication and\n",
    "addition, then its rules of differentiation are quite straightforward\n",
    "too, but slightly more complex than regular derivatives."
   ],
   "id": "788507ac-d1f0-4200-a122-610e1c4e7f74"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Derivatives\n",
    "\n",
    "We will need two rules of multivariate or *matrix* differentiation. The\n",
    "first is differentiation of an inner product. By remembering that the\n",
    "inner product is made up of multiplication and addition, we can hope\n",
    "that its derivative is quite straightforward, and so it proves to be. We\n",
    "can start by thinking about the definition of the inner product, $$\n",
    "\\mathbf{a}^\\top\\mathbf{z} = \\sum_{i} a_i\n",
    "z_i,\n",
    "$$ which if we were to take the derivative with respect to $z_k$ would\n",
    "simply return the gradient of the one term in the sum for which the\n",
    "derivative was non-zero, that of $a_k$, so we know that $$\n",
    "\\frac{\\text{d}}{\\text{d}z_k} \\mathbf{a}^\\top \\mathbf{z} = a_k\n",
    "$$ and by our definition for multivariate derivatives, we can simply\n",
    "stack all the partial derivatives of this form in a vector to obtain the\n",
    "result that $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
    "\\mathbf{a}^\\top \\mathbf{z} = \\mathbf{a}.\n",
    "$$ The second rule that’s required is differentiation of a ‘matrix\n",
    "quadratic’. A scalar quadratic in $z$ with coefficient $c$ has the form\n",
    "$cz^2$. If $\\mathbf{z}$ is a $k\\times 1$ vector and $\\mathbf{C}$ is a\n",
    "$k \\times k$ *matrix* of coefficients then the matrix quadratic form is\n",
    "written as $\\mathbf{z}^\\top \\mathbf{C}\\mathbf{z}$, which is itself a\n",
    "*scalar* quantity, but it is a function of a *vector*."
   ],
   "id": "dc846ac4-6228-4332-9cb4-fea576caef11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Dimensions in Matrix Multiplications\n",
    "\n",
    "There’s a trick for telling a multiplication leads to a scalar result.\n",
    "When you are doing mathematics with matrices, it’s always worth pausing\n",
    "to perform a quick sanity check on the dimensions. Matrix multplication\n",
    "only works when the dimensions match. To be precise, the ‘inner’\n",
    "dimension of the matrix must match. What is the inner dimension? If we\n",
    "multiply two matrices $\\mathbf{A}$ and $\\mathbf{B}$, the first of which\n",
    "has $k$ rows and $\\ell$ columns and the second of which has $p$ rows and\n",
    "$q$ columns, then we can check whether the multiplication works by\n",
    "writing the dimensionalities next to each other, $$\n",
    "\\mathbf{A} \\mathbf{B} \\rightarrow (k \\times\n",
    "\\underbrace{\\ell)(p}_\\text{inner dimensions} \\times q) \\rightarrow (k\\times q).\n",
    "$$ The inner dimensions are the two inside dimensions, $\\ell$ and $p$.\n",
    "The multiplication will only work if $\\ell=p$. The result of the\n",
    "multiplication will then be a $k\\times q$ matrix: this dimensionality\n",
    "comes from the ‘outer dimensions’. Note that matrix multiplication is\n",
    "not [*commutative*](http://en.wikipedia.org/wiki/Commutative_property).\n",
    "And if you change the order of the multiplication, $$\n",
    "\\mathbf{B} \\mathbf{A} \\rightarrow (\\ell \\times \\underbrace{k)(q}_\\text{inner dimensions} \\times p) \\rightarrow (\\ell \\times p).\n",
    "$$ Firstly, it may no longer even work, because now the condition is\n",
    "that $k=q$, and secondly the result could be of a different\n",
    "dimensionality. An exception is if the matrices are square matrices\n",
    "(e.g., same number of rows as columns) and they are both *symmetric*. A\n",
    "symmetric matrix is one for which $\\mathbf{A}=\\mathbf{A}^\\top$, or\n",
    "equivalently, $a_{i,j} = a_{j,i}$ for all $i$ and $j$.\n",
    "\n",
    "For applying and developing machine learning algorithms you should get\n",
    "familiar with working with matrices and vectors. You should have come\n",
    "across them before, but you may not have used them as extensively as we\n",
    "are doing now. It’s worth getting used to using this trick to check your\n",
    "work and ensure you know what the dimension of an output matrix should\n",
    "be. For our matrix quadratic form, it turns out that we can see it as a\n",
    "special type of inner product. $$\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z} \\rightarrow (1\\times\n",
    "\\underbrace{k) (k}_\\text{inner dimensions}\\times k) (k\\times 1) \\rightarrow\n",
    "\\mathbf{b}^\\top\\mathbf{z}\n",
    "$$ where $\\mathbf{b} = \\mathbf{C}\\mathbf{z}$ so therefore the result is\n",
    "a scalar, $$\n",
    "\\mathbf{b}^\\top\\mathbf{z} \\rightarrow\n",
    "(1\\times \\underbrace{k) (k}_\\text{inner dimensions}\\times 1) \\rightarrow\n",
    "(1\\times 1)\n",
    "$$ where a $(1\\times 1)$ matrix is recognised as a scalar.\n",
    "\n",
    "This implies that we should be able to differentiate this form, and\n",
    "indeed the rule for its differentiation is slightly more complex than\n",
    "the inner product, but still quite simple, $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}}\n",
    "\\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}= \\mathbf{C}\\mathbf{z} + \\mathbf{C}^\\top\n",
    "\\mathbf{z}.\n",
    "$$ Note that in the special case where $\\mathbf{C}$ is symmetric then we\n",
    "have $\\mathbf{C} = \\mathbf{C}^\\top$ and the derivative simplifies to $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{z}} \\mathbf{z}^\\top\\mathbf{C}\\mathbf{z}=\n",
    "2\\mathbf{C}\\mathbf{z}.\n",
    "$$"
   ],
   "id": "75c09fdd-9595-4b9b-bb87-efba0210b2bf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiate the Objective\n",
    "\n",
    "First, we need to compute the full objective by substituting our\n",
    "prediction function into the objective function to obtain the objective\n",
    "in terms of $\\mathbf{ w}$. Doing this we obtain $$\n",
    "E(\\mathbf{ w})= (\\mathbf{ y}- \\mathbf{X}\\mathbf{ w})^\\top (\\mathbf{ y}- \\mathbf{X}\\mathbf{ w}).\n",
    "$$ We now need to differentiate this *quadratic form* to find the\n",
    "minimum. We differentiate with respect to the *vector* $\\mathbf{ w}$.\n",
    "But before we do that, we’ll expand the brackets in the quadratic form\n",
    "to obtain a series of scalar terms. The rules for bracket expansion\n",
    "across the vectors are similar to those for the scalar system giving, $$\n",
    "(\\mathbf{a} - \\mathbf{b})^\\top\n",
    "(\\mathbf{c} - \\mathbf{d}) = \\mathbf{a}^\\top \\mathbf{c} - \\mathbf{a}^\\top\n",
    "\\mathbf{d} - \\mathbf{b}^\\top \\mathbf{c} + \\mathbf{b}^\\top \\mathbf{d}\n",
    "$$ which substituting for $\\mathbf{a} = \\mathbf{c} = \\mathbf{ y}$ and\n",
    "$\\mathbf{b}=\\mathbf{d} = \\mathbf{X}\\mathbf{ w}$ gives $$\n",
    "E(\\mathbf{ w})=\n",
    "\\mathbf{ y}^\\top\\mathbf{ y}- 2\\mathbf{ y}^\\top\\mathbf{X}\\mathbf{ w}+\n",
    "\\mathbf{ w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w}\n",
    "$$ where we used the fact that\n",
    "$\\mathbf{ y}^\\top\\mathbf{X}\\mathbf{ w}=\\mathbf{ w}^\\top\\mathbf{X}^\\top\\mathbf{ y}$.\n",
    "\n",
    "Now we can use our rules of differentiation to compute the derivative of\n",
    "this form, which is, $$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{ w}}E(\\mathbf{ w})=- 2\\mathbf{X}^\\top \\mathbf{ y}+\n",
    "2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w},\n",
    "$$ where we have exploited the fact that $\\mathbf{X}^\\top\\mathbf{X}$ is\n",
    "symmetric to obtain this result."
   ],
   "id": "7543383e-9b59-4db1-80a0-aa75dd30b4ba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Use the equivalence between our vector and our matrix formulations of\n",
    "linear regression, alongside our definition of vector derivates, to\n",
    "match the gradients we’ve computed directly for\n",
    "$\\frac{\\text{d}E(c, m)}{\\text{d}c}$ and\n",
    "$\\frac{\\text{d}E(c, m)}{\\text{d}m}$ to those for\n",
    "$\\frac{\\text{d}E(\\mathbf{ w})}{\\text{d}\\mathbf{ w}}$."
   ],
   "id": "d676eccd-69f5-42d2-8d2f-f74c44eac8a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 Answer\n",
    "\n",
    "Write your answer to Exercise 5 here"
   ],
   "id": "2b5273d4-2553-47c5-8481-53ccdec65aff"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Equation for Global Optimum\n",
    "\n",
    "We need to find the minimum of our objective function. Using our\n",
    "objective function, we can minimize for our parameter vector\n",
    "$\\mathbf{ w}$. Firstly, we seek stationary points by find parameter\n",
    "vectors that solve for when the gradients are zero, $$\n",
    "\\mathbf{0}=- 2\\mathbf{X}^\\top\n",
    "\\mathbf{ y}+ 2\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w},\n",
    "$$ where $\\mathbf{0}$ is a *vector* of zeros. Rearranging this equation,\n",
    "we find the solution to be $$\n",
    "\\mathbf{X}^\\top \\mathbf{X}\\mathbf{ w}= \\mathbf{X}^\\top\n",
    "\\mathbf{ y}\n",
    "$$ which is a matrix equation of the familiar form\n",
    "$\\mathbf{A}\\mathbf{x} = \\mathbf{b}$."
   ],
   "id": "8d522c80-552f-4216-8043-fc8b186ca664"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Multivariate System\n",
    "\n",
    "The solution for $\\mathbf{ w}$ can be written mathematically in terms of\n",
    "a matrix inverse of $\\mathbf{X}^\\top\\mathbf{X}$, but computation of a\n",
    "matrix inverse requires an algorithm to resolve it. You’ll know this if\n",
    "you had to invert, by hand, a $3\\times 3$ matrix in high school. From a\n",
    "numerical stability perspective, it is also best not to compute the\n",
    "matrix inverse directly, but rather to ask the computer to *solve* the\n",
    "system of linear equations given by $$\n",
    "\\mathbf{X}^\\top\\mathbf{X}\\mathbf{ w}= \\mathbf{X}^\\top\\mathbf{ y}\n",
    "$$ for $\\mathbf{ w}$."
   ],
   "id": "1b74ac56-db02-46f8-818a-2c99bbb2a813"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "A major advantage of the new system is that we can build a linear\n",
    "regression on a multivariate system. The matrix calculus didn’t specify\n",
    "what the length of the vector $\\mathbf{ x}$ should be, or equivalently\n",
    "the size of the design matrix."
   ],
   "id": "d7224fec-bd61-4bbd-bc15-ac83880429dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Body Count Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/movie-body-count-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_datasets/includes/movie-body-count-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "This is a data set created by Simon Garnier and Rany Olson for exploring\n",
    "the differences between R and Python for data science. The data contains\n",
    "information about different movies augmented by estimates about how many\n",
    "on-screen deaths are contained in the movie. The data is craped from\n",
    "<http://www.moviebodycounts.com>. The data contains the following\n",
    "featuers for each movie: `Year`, `Body_Count`, `MPAA_Rating`, `Genre`,\n",
    "`Director`, `Actors`, `Length_Minutes`, `IMDB_Rating`."
   ],
   "id": "5d046d18-2bb3-40bc-85c1-f1a1ff2f9855"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ],
   "id": "4f8cf09a-1a6f-4480-bf11-2f3a34cd585a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.movie_body_count()\n",
    "movies = data['Y']"
   ],
   "id": "81e8e104-c494-45f1-b6c7-b06077b1bc13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided to us in the form of a pandas data frame, we can\n",
    "see the features we’re provided with by inspecting the columns of the\n",
    "data frame."
   ],
   "id": "1b24fa5c-b91b-4f3b-99a1-8a36d95225b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join(movies.columns))"
   ],
   "id": "d3a21ac8-87bd-4d93-b9e0-5c53d41b9742"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression on Movie Body Count Data\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/movie-body-count-linear-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/movie-body-count-linear-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Now we will build a design matrix based on the numeric features: year,\n",
    "Body_Count, Length_Minutes in an effort to predict the rating. We build\n",
    "the design matrix as follows:\n",
    "\n",
    "Bias as an additional feature."
   ],
   "id": "1cf72587-1a78-4008-83c6-6db911c4029c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features = ['Year', 'Body_Count', 'Length_Minutes']\n",
    "X = movies[select_features]\n",
    "X['Eins'] = 1 # add a column for the offset\n",
    "y = movies[['IMDB_Rating']]"
   ],
   "id": "ab6cedb8-ae67-41f5-99e8-cd5d8c508d6d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s perform a linear regression. But this time, we will create a\n",
    "pandas data frame for the result so we can store it in a form that we\n",
    "can visualise easily."
   ],
   "id": "02c3f21b-8c6f-486d-aa1e-144e38d4b858"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "id": "0d3b94e6-24bd-4bdd-9dd7-856f405aff62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.DataFrame(data=np.linalg.solve(X.T@X, X.T@y),  # solve linear regression here\n",
    "                 index = X.columns,  # columns of X become rows of w\n",
    "                 columns=['regression_coefficient']) # the column of X is the value of regression coefficient"
   ],
   "id": "6c8b0a49-229f-4931-9bde-0eafca445102"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the residuals to see how good our estimates are. First we\n",
    "create a pandas data frame containing the predictions and use it to\n",
    "compute the residuals."
   ],
   "id": "4838e99b-f286-47db-ab44-db9a6aa529f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = pd.DataFrame(data=(X@w).values, columns=['IMDB_Rating'])\n",
    "resid = y-ypred"
   ],
   "id": "4101cfb5-2a5c-410f-97ab-f94dd3a474fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlai.plot as plot\n",
    "import mlai"
   ],
   "id": "d4ce3b45-d3ca-4fca-af02-d6d6f9781c7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "resid.hist(ax=ax)\n",
    "mlai.write_figure(filename='movie-body-count-rating-residuals.svg', \n",
    "                  directory='./ml')"
   ],
   "id": "a1014845-3faa-4c80-ad1f-1a96bf787e8a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mlatcl.github.io/advds/./slides/diagrams//ml/movie-body-count-rating-residuals.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Residual values for the ratings from the prediction of the\n",
    "movie rating given the data from the film.</i>\n",
    "\n",
    "Which shows our model *hasn’t* yet done a great job of representation,\n",
    "because the spread of values is large. We can check what the rating is\n",
    "dominated by in terms of regression coefficients."
   ],
   "id": "7af28eef-6cf7-4f29-81d4-aa2af7ce4e12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ],
   "id": "e153fe2e-d776-4d2a-9754-28e4b8f4821d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have to be a little careful about interpretation because our\n",
    "input values live on different scales, however it looks like we are\n",
    "dominated by the bias, with a small negative effect for later films (but\n",
    "bear in mind the years are large, so this effect is probably larger than\n",
    "it looks) and a positive effect for length. So it looks like long\n",
    "earlier films generally do better, but the residuals are so high that we\n",
    "probably haven’t modelled the system very well."
   ],
   "id": "9f33a386-98b8-4f7c-96d7-ef2424bcf7cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('ui-uNlFHoms')"
   ],
   "id": "130ac4a0-f9e1-4242-a7cc-e619ce9a78ad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>MLAI Lecture 15 from 2014 on Multivariate Regression.</i>"
   ],
   "id": "03b52082-6710-42dc-bdef-5529c23cef7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('78YNphT90-k')"
   ],
   "id": "d020fb88-a8b0-4388-8ca6-5f821306ed4c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: <i>MLAI Lecture 3 from 2012 on Maximum Likelihood</i>"
   ],
   "id": "e44bc3d5-1137-419c-86f5-a9140f6a6248"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution with QR Decomposition\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/qr-decomposition-regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/qr-decomposition-regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "Performing a solve instead of a matrix inverse is the more numerically\n",
    "stable approach, but we can do even better. A\n",
    "[QR-decomposition](http://en.wikipedia.org/wiki/QR_decomposition) of a\n",
    "matrix factorizes it into a matrix which is an orthogonal matrix\n",
    "$\\mathbf{Q}$, so that $\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}$. And a\n",
    "matrix which is upper triangular, $\\mathbf{R}$. $$\n",
    "\\mathbf{X}^\\top \\mathbf{X}\\boldsymbol{\\beta} =\n",
    "\\mathbf{X}^\\top \\mathbf{ y}\n",
    "$$ and we substitute $\\mathbf{X}= \\mathbf{Q}{\\mathbf{R}$ so we have $$\n",
    "(\\mathbf{Q}\\mathbf{R})^\\top\n",
    "(\\mathbf{Q}\\mathbf{R})\\boldsymbol{\\beta} = (\\mathbf{Q}\\mathbf{R})^\\top\n",
    "\\mathbf{ y}\n",
    "$$ $$\n",
    "\\mathbf{R}^\\top (\\mathbf{Q}^\\top \\mathbf{Q}) \\mathbf{R}\n",
    "\\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top \\mathbf{ y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{R}^\\top \\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{R}^\\top \\mathbf{Q}^\\top\n",
    "\\mathbf{ y}\n",
    "$$ $$\n",
    "\\mathbf{R} \\boldsymbol{\\beta} = \\mathbf{Q}^\\top \\mathbf{ y}\n",
    "$$ which leaves us with a lower triangular system to solve.\n",
    "\n",
    "This is a more numerically stable solution because it removes the need\n",
    "to compute $\\mathbf{X}^\\top\\mathbf{X}$ as an intermediate. Computing\n",
    "$\\mathbf{X}^\\top\\mathbf{X}$ is a bad idea because it involves squaring\n",
    "all the elements of $\\mathbf{X}$ and thereby potentially reducing the\n",
    "numerical precision with which we can represent the solution. Operating\n",
    "on $\\mathbf{X}$ directly preserves the numerical precision of the model.\n",
    "\n",
    "This can be more particularly seen when we begin to work with *basis\n",
    "functions* in the next session. Some systems that can be resolved with\n",
    "the QR decomposition cannot be resolved by using solve directly."
   ],
   "id": "007ab061-1a16-4a7d-b388-7e43d9e3fbf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ],
   "id": "66a88e09-6565-4cb8-944c-608c74a7e864"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = np.linalg.qr(X)\n",
    "w = sp.linalg.solve_triangular(R, Q.T@y) \n",
    "w = pd.DataFrame(w, index=X.columns)\n",
    "w"
   ],
   "id": "11724385-f37c-4749-9c62-0fdc281e5c71"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ],
   "id": "86c26d70-f25c-425b-aa51-b51665bb2c70"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "-   For fitting linear models: Section 1.1-1.2 of Rogers and Girolami\n",
    "    (2011)\n",
    "\n",
    "-   Section 1.2.5 up to equation 1.65 of Bishop (2006)\n",
    "\n",
    "-   Section 1.3 for Matrix & Vector Review of Rogers and Girolami (2011)"
   ],
   "id": "549225da-2fda-4664-a7d5-b69ce141d790"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ],
   "id": "9933b7d3-001c-4a7a-9c26-5dee03b5faa7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bishop, C.M., 2006. Pattern recognition and machine learning. springer.\n",
    "\n",
    "Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.\n",
    "Courcier, Paris.\n",
    "\n",
    "Legendre, A.-M., 1805. Nouvelles méthodes pour la détermination des\n",
    "orbites des comètes. F. Didot.\n",
    "\n",
    "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
    "Press."
   ],
   "id": "31db2ab7-ba51-4d3b-8d1f-123a1b3859e9"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
