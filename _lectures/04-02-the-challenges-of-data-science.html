---
title: "The Challenges of Data Science"
venue: "LT2, William Gates Building"
abstract: "<p>In the first lecture, we laid out the underpinning
phenomena that give us the landscape of data science. In this lecture we
unpick the challenges that landscape presents us with. The material
gives you context for why data science is very different from standard
software engineering, and how data science problems need to be
approached including the many different aspects that need to be
considered. We will look at the challenges of deploying data science
solutions in practice. We categorize them into three groups.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_advds/the-challenges-of-data-science.md
date: 2023-10-30
published: 2023-10-30
time: "10:00"
week: 4
session: 2
featured_image: assets/images/the-challenges-of-data-science.png
reveal: 04-02-the-challenges-of-data-science.slides.html
transition: None
youtube: "BQKfIJHPiCQ"
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<!-- defines the technical variant-->
<h1 id="introduction">Introduction</h1>
<p>Data science is an emerging discipline. That makes it harder to make
clean decisions about what any given individual will need to know to
become a data scientist. Those of you who are studying now will be those
that define the discipline. As we deploy more data driven decision
making in the world, the role will be refined. Until we achieve that
refinement, your knowledge needs to be broad based.</p>
<p>In this lecture we will first continue our theme of how our
limitations as humans mean that our analysis of data can be affected,
and I will introduce an analogy that should help you understand
<em>how</em> data science differs significantly from traditional
software engineering. We’ll then contextualize some of the challenges
the domain into three different groups.</p>
<h2 id="the-gartner-hype-cycle">The Gartner Hype Cycle</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/gartner-hype-cycle.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/gartner-hype-cycle.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="gartner-hype-cycle-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//Gartner_Hype_Cycle.svg" width="80%" style=" ">
</object>
</div>
<div id="gartner-hype-cycle-magnify" class="magnify"
onclick="magnifyFigure(&#39;gartner-hype-cycle&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gartner-hype-cycle-caption" class="caption-frame">
<p>Figure: The Gartner Hype Cycle places technologies on a graph that
relates to the expectations we have of a technology against its actual
influence. Early hope for a new techology is often displaced by
disillusionment due to the time it takes for a technology to be usefully
deployed.</p>
</div>
</div>
<p>The <a href="https://en.wikipedia.org/wiki/Hype_cycle">Gartner Hype
Cycle</a> tries to assess where an idea is in terms of maturity and
adoption. It splits the evolution of technology into a technological
trigger, a peak of expectations followed by a trough of disillusionment
and a final ascension into a useful technology. It looks rather like a
classical control response to a final set point.</p>
<h2 id="cycle-for-ml-terms">Cycle for ML Terms</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/gartner-hype-cycle-ai-bd-dm-dl-ml.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/gartner-hype-cycle-ai-bd-dm-dl-ml.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="google-trends">Google Trends</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/gartner-hype-cycle-base.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/gartner-hype-cycle-base.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pytrends</span></code></pre></div>
<div class="figure">
<div id="ai-bd-dm-dl-ml-gartner-hype-cycle-google-trends-figure"
class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//data-science/ai-bd-dm-dl-ml-google-trends.svg" width="80%" style=" ">
</object>
</div>
<div id="ai-bd-dm-dl-ml-gartner-hype-cycle-google-trends-magnify"
class="magnify"
onclick="magnifyFigure(&#39;ai-bd-dm-dl-ml-gartner-hype-cycle-google-trends&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ai-bd-dm-dl-ml-gartner-hype-cycle-google-trends-caption"
class="caption-frame">
<p>Figure: A Google trends search for ‘artificial intelligence’, ‘big
data’, ‘data mining’, ‘deep learning’, ‘machine learning’ as different
technological terms gives us insight into their popularity over
time.</p>
</div>
</div>
<p>Google trends gives us insight into the interest for different terms
over time.</p>
<p>Examining Google trends search for ‘artificial intelligence’, ‘big
data’, ‘data mining’, ‘deep learning’ and ‘machine learning’ we can see
that ‘artificial intelligence’ <em>may</em> be entering a plateau of
productivity, ‘big data’ is entering the trough of disillusionment, and
‘data mining’ seems to be deeply within the trough. On the other hand,
‘deep learning’ and ‘machine learning’ appear to be ascending to the
peak of inflated expectations having experienced a technology
trigger.</p>
<p>For deep learning that technology trigger was the ImageNet result of
2012 <span class="citation"
data-cites="Krizhevsky:imagenet12">(Krizhevsky et al., n.d.)</span>.
This step change in performance on object detection in images was
achieved through convolutional neural networks, popularly known as ‘deep
learning’.</p>
<h2 id="data-science-as-debugging">Data Science as Debugging</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/data-science-as-debugging.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_data-science/includes/data-science-as-debugging.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>One challenge for existing information technology professionals is
realizing the extent to which a software ecosystem based on data differs
from a classical ecosystem. In particular, by ingesting data we bring
unknowns/uncontrollables into our decision-making system. This presents
opportunity for adversarial exploitation and unforeseen operation.</p>
<p>blog post on <a
href="http://inverseprobability.com/2017/03/14/data-science-as-debugging">Data
Science as Debugging</a>.</p>
<p>Starting with the analysis of a data set, the nature of data science
is somewhat difference from classical software engineering.</p>
<p>One analogy I find helpful for understanding the depth of change we
need is the following. Imagine as a software engineer, you find a USB
stick on the ground. And for some reason you <em>know</em> that on that
USB stick is a particular API call that will enable you to make a
significant positive difference on a business problem. You don’t know
which of the many library functions on the USB stick are the ones that
will help. And it could be that some of those library functions will
hinder, perhaps because they are just inappropriate or perhaps because
they have been placed there maliciously. The most secure thing to do
would be to <em>not</em> introduce this code into your production system
at all. But what if your manager told you to do so, how would you go
about incorporating this code base?</p>
<p>The answer is <em>very</em> carefully. You would have to engage in a
process more akin to debugging than regular software engineering. As you
understood the code base, for your work to be reproducible, you should
be documenting it, not just what you discovered, but how you discovered
it. In the end, you typically find a single API call that is the one
that most benefits your system. But more thought has been placed into
this line of code than any line of code you have written before.</p>
<p>An enormous amount of debugging would be required. As the nature of
the code base is understood, software tests to verify it also need to be
constructed. At the end of all your work, the lines of software you
write to interact with the software on the USB stick are likely to be
minimal. But more thought would be put into those lines than perhaps any
other lines of code in the system.</p>
<p>Even then, when your API code is introduced into your production
system, it needs to be deployed in an environment that monitors it. We
cannot rely on an individual’s decision making to ensure the quality of
all our systems. We need to create an environment that includes quality
controls, checks, and bounds, tests, all designed to ensure that
assumptions made about this foreign code base are remaining valid.</p>
<p>This situation is akin to what we are doing when we incorporate data
in our production systems. When we are consuming data from others, we
cannot assume that it has been produced in alignment with our goals for
our own systems. Worst case, it may have been adversarially produced. A
further challenge is that data is dynamic. So, in effect, the code on
the USB stick is evolving over time.</p>
<p>It might see that this process is easy to formalize now, we simply
need to check what the formal software engineering process is for
debugging, because that is the current software engineering activity
that data science is closest to. But when we look for a formalization of
debugging, we find that there is none. Indeed, modern software
engineering mainly focusses on ensuring that code is written without
bugs in the first place.</p>
<h3 id="lessons">Lessons</h3>
<ol type="1">
<li>When you begin an analysis, behave as a debugger.</li>
</ol>
<ul>
<li>Write test code as you go. Document those tests and ensure they are
accessible by others.</li>
<li>Understand the landscape of your data. Be prepared to try several
different approaches to the data set.</li>
<li>Be constantly skeptical.</li>
<li>Use the best tools available, develop a deep understand how they
work.</li>
<li>Share your experience of what challenges you’re facing. Have others
(software engineers, fellow data analysts, your manager) review your
work.</li>
<li>Never go straight for the goal: you’d never try and write the API
call straight away on the discarded hard drive, so why are you launching
your classification algorithm before visualizing the data?</li>
<li>Ensure your analysis is documented and accessible. If your code does
go wrong in production, you’ll need to be able to retrace to where the
error crept in.</li>
</ul>
<ol start="2" type="1">
<li>When managing the data science process, don’t treat it as standard
code development.</li>
</ol>
<ul>
<li>Don’t deploy a traditional agile development pipeline and expect it
to work the same way it does for standard code development. Think about
how you handle bugs, think about how you would handle very many
bugs.</li>
<li>Don’t leave the data scientist alone to wade through the mess.</li>
<li>Integrate the data analysis with your other team activities. Have
the software engineers and domain experts work closely with the data
scientists. This is vital for providing the data scientists with the
technical support they need, but also managing the expectations of the
engineers in terms of when and how the data will be able to
deliver.</li>
</ul>
<p><strong>Recommendation</strong>: Anecdotally, resolving a machine
learning challenge requires 80% of the resource to be focused on the
data and perhaps 20% to be focused on the model. But many companies are
too keen to employ machine learning engineers who focus on the models,
not the data. We should change our hiring priorities and training.
Universities cannot provide the understanding of how to data-wrangle.
Companies must fill this gap.</p>
<h2 id="statistics-to-deep-learning">Statistics to Deep Learning</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/statistics-to-deep-learning.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/statistics-to-deep-learning.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="what-is-machine-learning">What is Machine Learning?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml-2.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml-2.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Machine learning allows us to extract knowledge from data to form a
prediction.</p>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>A machine learning prediction is made by combining a model with data
to form the prediction. The manner in which this is done gives us the
machine learning <em>algorithm</em>.</p>
<p>Machine learning models are <em>mathematical models</em> which make
weak assumptions about data, e.g. smoothness assumptions. By combining
these assumptions with the data, we observe we can interpolate between
data points or, occasionally, extrapolate into the future.</p>
<p>Machine learning is a technology which strongly overlaps with the
methodology of statistics. From a historical/philosophical view point,
machine learning differs from statistics in that the focus in the
machine learning community has been primarily on accuracy of prediction,
whereas the focus in statistics is typically on the interpretability of
a model and/or validating a hypothesis through data collection.</p>
<p>The rapid increase in the availability of compute and data has led to
the increased prominence of machine learning. This prominence is
surfacing in two different but overlapping domains: data science and
artificial intelligence.</p>
<h2 id="from-model-to-decision">From Model to Decision</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml-end-to-end.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml-end-to-end.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The real challenge, however, is end-to-end decision making. Taking
information from the environment and using it to drive decision making
to achieve goals.</p>
<h2 id="classical-statistical-analysis">Classical Statistical
Analysis</h2>
<p>Despite the shift of emphasis, traditional statistical techniques are
more important than ever. One of the few ways we have to validate the
analyses we create is to make use of visualizations, randomized testing
and other forms of statistical analysis. You will have explored some of
these ideas in earlier courses in machine learning. In this unit we
provide some review material in a practical sheet to bring some of those
ideas together in the context of data science.</p>
<h1 id="what-is-machine-learning-1">What is Machine Learning?</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-is-ml.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>What is machine learning? At its most basic level machine learning is
a combination of</p>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>where <em>data</em> is our observations. They can be actively or
passively acquired (meta-data). The <em>model</em> contains our
assumptions, based on previous experience. That experience can be other
data, it can come from transfer learning, or it can merely be our
beliefs about the regularities of the universe. In humans our models
include our inductive biases. The <em>prediction</em> is an action to be
taken or a categorization or a quality score. The reason that machine
learning has become a mainstay of artificial intelligence is the
importance of predictions in artificial intelligence. The data and the
model are combined through computation.</p>
<p>In practice we normally perform machine learning using two functions.
To combine data with a model we typically make use of:</p>
<p><strong>a prediction function</strong> it is used to make the
predictions. It includes our beliefs about the regularities of the
universe, our assumptions about how the world works, e.g., smoothness,
spatial similarities, temporal similarities.</p>
<p><strong>an objective function</strong> it defines the ‘cost’ of
misprediction. Typically, it includes knowledge about the world’s
generating processes (probabilistic objectives) or the costs we pay for
mispredictions (empirical risk minimization).</p>
<p>The combination of data and model through the prediction function and
the objective function leads to a <em>learning algorithm</em>. The class
of prediction functions and objective functions we can make use of is
restricted by the algorithms they lead to. If the prediction function or
the objective function are too complex, then it can be difficult to find
an appropriate learning algorithm. Much of the academic field of machine
learning is the quest for new learning algorithms that allow us to bring
different types of models and data together.</p>
<p>A useful reference for state of the art in machine learning is the UK
Royal Society Report, <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a>.</p>
<p>You can also check my post blog post on <a
href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What
is Machine Learning?</a>.</p>
<h2 id="artificial-intelligence-and-data-science">Artificial
Intelligence and Data Science</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/data-science-vs-ai.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/data-science-vs-ai.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Machine learning technologies have been the driver of two related,
but distinct disciplines. The first is <em>data science</em>. Data
science is an emerging field that arises from the fact that we now
collect so much data by happenstance, rather than by <em>experimental
design</em>. Classical statistics is the science of drawing conclusions
from data, and to do so statistical experiments are carefully designed.
In the modern era we collect so much data that there’s a desire to draw
inferences directly from the data.</p>
<p>As well as machine learning, the field of data science draws from
statistics, cloud computing, data storage (e.g. streaming data),
visualization and data mining.</p>
<p>In contrast, artificial intelligence technologies typically focus on
emulating some form of human behaviour, such as understanding an image,
or some speech, or translating text from one form to another. The recent
advances in artificial intelligence have come from machine learning
providing the automation. But in contrast to data science, in artificial
intelligence the data is normally collected with the specific task in
mind. In this sense it has strong relations to classical statistics.</p>
<p>Classically artificial intelligence worried more about <em>logic</em>
and <em>planning</em> and focused less on data driven decision making.
Modern machine learning owes more to the field of <em>Cybernetics</em>
<span class="citation" data-cites="Wiener:cybernetics48">(Wiener,
1948)</span> than artificial intelligence. Related fields include
<em>robotics</em>, <em>speech recognition</em>, <em>language
understanding</em> and <em>computer vision</em>.</p>
<p>There are strong overlaps between the fields, the wide availability
of data by happenstance makes it easier to collect data for designing AI
systems. These relations are coming through wide availability of sensing
technologies that are interconnected by cellular networks, WiFi and the
internet. This phenomenon is sometimes known as the <em>Internet of
Things</em>, but this feels like a dangerous misnomer. We must never
forget that we are interconnecting people, not things.</p>
<center>
Convention for the Protection of <em>Individuals</em> with regard to
Automatic Processing of <em>Personal Data</em> (1981/1/28)
</center>
<h2 id="what-does-machine-learning-do">What does Machine Learning
do?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-does-machine-learning-do.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-does-machine-learning-do.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Any process of automation allows us to scale what we do by codifying
a process in some way that makes it efficient and repeatable. Machine
learning automates by emulating human (or other actions) found in data.
Machine learning codifies in the form of a mathematical function that is
learnt by a computer. If we can create these mathematical functions in
ways in which they can interconnect, then we can also build systems.</p>
<p>Machine learning works through codifying a prediction of interest
into a mathematical function. For example, we can try and predict the
probability that a customer wants to by a jersey given knowledge of
their age, and the latitude where they live. The technique known as
logistic regression estimates the odds that someone will by a jumper as
a linear weighted sum of the features of interest.</p>
<p><span class="math display">\[ \text{odds} =
\frac{p(\text{bought})}{p(\text{not bought})} \]</span></p>
<p><span class="math display">\[ \log \text{odds}  = w_0 + w_1
\text{age} + w_2 \text{latitude}.\]</span> Here <span
class="math inline">\(w_0\)</span>, <span
class="math inline">\(w_1\)</span> and <span
class="math inline">\(w_2\)</span> are the parameters of the model. If
<span class="math inline">\(w_1\)</span> and <span
class="math inline">\(w_2\)</span> are both positive, then the log-odds
that someone will buy a jumper increase with increasing latitude and
age, so the further north you are and the older you are the more likely
you are to buy a jumper. The parameter <span
class="math inline">\(w_0\)</span> is an offset parameter and gives the
log-odds of buying a jumper at zero age and on the equator. It is likely
to be negative<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> indicating that the purchase is
odds-against. This is also a classical statistical model, and models
like logistic regression are widely used to estimate probabilities from
ad-click prediction to disease risk.</p>
<p>This is called a generalized linear model, we can also think of it as
estimating the <em>probability</em> of a purchase as a nonlinear
function of the features (age, latitude) and the parameters (the <span
class="math inline">\(w\)</span> values). The function is known as the
<em>sigmoid</em> or <a
href="https://en.wikipedia.org/wiki/Logistic_regression">logistic
function</a>, thus the name <em>logistic</em> regression.</p>
<h3 id="sigmoid-function">Sigmoid Function</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/sigmoid-function.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/sigmoid-function.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-logistic-function-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//ml/logistic.svg" width="80%" style=" ">
</object>
</div>
<div id="the-logistic-function-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-logistic-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-logistic-function-caption" class="caption-frame">
<p>Figure: The logistic function.</p>
</div>
</div>
<p>The function has this characeristic ‘s’-shape (from where the term
sigmoid, as in sigma, comes from). It also takes the input from the
entire real line and ‘squashes’ it into an output that is between zero
and one. For this reason it is sometimes also called a ‘squashing
function’.</p>
<p>The sigmoid comes from the inverting the odds ratio, <span
class="math display">\[
\frac{\pi}{(1-\pi)}
\]</span> where <span class="math inline">\(\pi\)</span> is the
probability of a positive outcome and <span
class="math inline">\(1-\pi\)</span> is the probability of a negative
outcome</p>
<p><span class="math display">\[ p(\text{bought}) =  \sigma\left(w_0 +
w_1 \text{age} + w_2 \text{latitude}\right).\]</span></p>
<p>In the case where we have <em>features</em> to help us predict, we
sometimes denote such features as a vector, <span
class="math inline">\(\mathbf{ x}\)</span>, and we then use an inner
product between the features and the parameters, <span
class="math inline">\(\mathbf{ w}^\top \mathbf{ x}= w_1 x_1 + w_2 x_2 +
w_3 x_3 ...\)</span>, to represent the argument of the sigmoid.</p>
<p><span class="math display">\[ p(\text{bought})
=  \sigma\left(\mathbf{ w}^\top \mathbf{ x}\right).\]</span> More
generally, we aim to predict some aspect of our data, <span
class="math inline">\(y\)</span>, by relating it through a mathematical
function, <span class="math inline">\(f(\cdot)\)</span>, to the
parameters, <span class="math inline">\(\mathbf{ w}\)</span> and the
data, <span class="math inline">\(\mathbf{ x}\)</span>.</p>
<p><span class="math display">\[ y=  f\left(\mathbf{ x}, \mathbf{
w}\right).\]</span> We call <span
class="math inline">\(f(\cdot)\)</span> the <em>prediction
function</em>.</p>
<p>To obtain the fit to data, we use a separate function called the
<em>objective function</em> that gives us a mathematical representation
of the difference between our predictions and the real data.</p>
<p><span class="math display">\[E(\mathbf{ w}, \mathbf{Y},
\mathbf{X})\]</span> A commonly used examples (for example in a
regression problem) is least squares, <span
class="math display">\[E(\mathbf{ w}, \mathbf{Y}, \mathbf{X}) =
\sum_{i=1}^n\left(y_i - f(\mathbf{ x}_i, \mathbf{
w})\right)^2.\]</span></p>
<p>If a linear prediction function is combined with the least squares
objective function, then that gives us a classical <em>linear
regression</em>, another classical statistical model. Statistics often
focusses on linear models because it makes interpretation of the model
easier. Interpretation is key in statistics because the aim is normally
to validate questions by analysis of data. Machine learning has
typically focused more on the prediction function itself and worried
less about the interpretation of parameters. In statistics, where
interpretation is typically more important than prediction, parameters
are normally denoted by <span
class="math inline">\(\boldsymbol{\beta}\)</span> instead of <span
class="math inline">\(\mathbf{ w}\)</span>.</p>
<p>A key difference between statistics and machine learning, is that
(traditionally) machine learning has focussed on predictive capability
and statistics has focussed on interpretability. That means that in a
statistics class far more emphasis will be placed on interpretation of
the parameters. In machine learning, the parameters, $, are just a means
to an end. But in statistics, when we denote the parameters by <span
class="math inline">\(\boldsymbol{\beta}\)</span>, we often use the
parameters to tell us something about the disease.</p>
<p>So we move between <span class="math display">\[ p(\text{bought})
=  \sigma\left(w_0 + w_1 \text{age} + w_2
\text{latitude}\right).\]</span></p>
<p>to denote the emphasis is on predictive power to</p>
<p><span class="math display">\[ p(\text{bought}) =  \sigma\left(\beta_0
+ \beta_1 \text{age} + \beta_2 \text{latitude}\right).\]</span></p>
<p>to denote the emphasis is on interpretation of the parameters.</p>
<p>Another effect of the focus on prediction in machine learning is that
<em>non-linear</em> approaches, which can be harder to interpret, are
more widely deployedin machine learning – they tend to improve quality
of predictions at the expense of interpretability.</p>
<h2 id="logistic-regression">Logistic Regression</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/logistic-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/logistic-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>A logistic regression is an approach to classification which extends
the linear basis function models we’ve already explored. Rather than
modeling the output of the function directly the assumption is that we
model the <em>log-odds</em> with the basis functions.</p>
<p>The <a href="http://en.wikipedia.org/wiki/Odds">odds</a> are defined
as the ratio of the probability of a positive outcome, to the
probability of a negative outcome. If the probability of a positive
outcome is denoted by <span class="math inline">\(\pi\)</span>, then the
odds are computed as <span
class="math inline">\(\frac{\pi}{1-\pi}\)</span>. Odds are widely used
by <a href="http://en.wikipedia.org/wiki/Bookmaker">bookmakers</a> in
gambling, although a bookmakers odds won’t normalise: i.e. if you look
at the equivalent probabilities, and sum over the probability of all
outcomes the bookmakers are considering, then you won’t get one. This is
how the bookmaker makes a profit. Because a probability is always
between zero and one, the odds are always between <span
class="math inline">\(0\)</span> and <span
class="math inline">\(\infty\)</span>. If the positive outcome is
unlikely the odds are close to zero, if it is very likely then the odds
become close to infinite. Taking the logarithm of the odds maps the odds
from the positive half space to being across the entire real line. Odds
that were between 0 and 1 (where the negative outcome was more likely)
are mapped to the range between <span
class="math inline">\(-\infty\)</span> and <span
class="math inline">\(0\)</span>. Odds that are greater than 1 are
mapped to the range between <span class="math inline">\(0\)</span> and
<span class="math inline">\(\infty\)</span>. Considering the log odds
therefore takes a number between 0 and 1 (the probability of positive
outcome) and maps it to the entire real line. The function that does
this is known as the <a href="http://en.wikipedia.org/wiki/Logit">logit
function</a>, <span class="math inline">\(g^{-1}(p_i) =
\log\frac{p_i}{1-p_i}\)</span>. This function is known as a <em>link
function</em>.</p>
<p>For a standard regression we take, <span class="math display">\[
f(\mathbf{ x}) = \mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}),
\]</span> if we want to perform classification we perform a logistic
regression. <span class="math display">\[
\log \frac{\pi}{(1-\pi)} = \mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})
\]</span> where the odds ratio between the positive class and the
negative class is given by <span class="math display">\[
\frac{\pi}{(1-\pi)}
\]</span> The odds can never be negative, but can take any value from 0
to <span class="math inline">\(\infty\)</span>. We have defined the link
function as taking the form <span
class="math inline">\(g^{-1}(\cdot)\)</span> implying that the inverse
link function is given by <span class="math inline">\(g(\cdot)\)</span>.
Since we have defined, <span class="math display">\[
g^{-1}(\pi) =
\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x})
\]</span> we can write <span class="math inline">\(\pi\)</span> in terms
of the <em>inverse link</em> function, <span
class="math inline">\(g(\cdot)\)</span> as <span class="math display">\[
\pi = g(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})).
\]</span></p>
<h2 id="basis-function">Basis Function</h2>
<p>We’ll define our prediction, objective and gradient functions below.
But before we start, we need to define a basis function for our model.
Let’s start with the linear basis.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> linear</span></code></pre></div>
<h2 id="prediction-function">Prediction Function</h2>
<p>Now we have the basis function let’s define the prediction
function.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(w, x, basis<span class="op">=</span>linear, <span class="op">**</span>kwargs):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Generates the prediction function and the basis matrix.&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> basis(x, <span class="op">**</span>kwargs)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> np.dot(Phi, w)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>f)), Phi</span></code></pre></div>
<p>This inverse of the link function is known as the <a
href="http://en.wikipedia.org/wiki/Logistic_function">logistic</a> (thus
the name logistic regression) or sometimes it is called the sigmoid
function. For a particular value of the input to the link function,
<span class="math inline">\(f_i = \mathbf{ w}^\top \boldsymbol{
\phi}(\mathbf{ x}_i)\)</span> we can plot the value of the inverse link
function as below.</p>
<p>By replacing the inverse link with the sigmoid we can write <span
class="math inline">\(\pi\)</span> as a function of the input and the
parameter vector as, <span class="math display">\[
\pi(\mathbf{ x},\mathbf{ w}) = \frac{1}{1+\exp\left(-\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})\right)}.
\]</span> The process for logistic regression is as follows. Compute the
output of a standard linear basis function composition (<span
class="math inline">\(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\)</span>, as we did for linear regression) and then apply the
inverse link function, <span class="math inline">\(g(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}))\)</span>. In logistic regression this
involves <em>squashing</em> it with the logistic (or sigmoid) function.
Use this value, which now has an interpretation as a
<em>probability</em> in a Bernoulli distribution to form the likelihood.
Then we can assume conditional independence of each data point given the
parameters and develop a likelihod for the entire data set.</p>
<p>As we discussed last time, the Bernoulli likelihood is of the form,
<span class="math display">\[
P(y_i|\mathbf{ w}, \mathbf{ x}) =
\pi_i^{y_i} (1-\pi_i)^{1-y_i}
\]</span> which we can think of as clever trick for mathematically
switching between two probabilities if we were to write it as code it
would be better described as</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli(x, y, pi):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> y <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pi(x)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>pi(x)</span></code></pre></div>
<p>but writing it mathematically makes it easier to write our objective
function within a single mathematical equation.</p>
<h2 id="maximum-likelihood">Maximum Likelihood</h2>
<p>To obtain the parameters of the model, we need to maximize the
likelihood, or minimize the objective function, normally taken to be the
negative log likelihood. With a data conditional independence assumption
the likelihood has the form, <span class="math display">\[
P(\mathbf{ y}|\mathbf{ w},
\mathbf{X}) = \prod_{i=1}^nP(y_i|\mathbf{ w}, \mathbf{ x}_i).
\]</span> which can be written as a log likelihood in the form <span
class="math display">\[
\log P(\mathbf{ y}|\mathbf{ w},
\mathbf{X}) = \sum_{i=1}^n\log P(y_i|\mathbf{ w}, \mathbf{ x}_i) =
\sum_{i=1}^n
y_i \log \pi_i + \sum_{i=1}^n(1-y_i)\log (1-\pi_i)
\]</span> and if we take the probability of positive outcome for the
<span class="math inline">\(i\)</span>th data point to be given by <span
class="math display">\[
\pi_i = g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x}_i)\right),
\]</span> where <span class="math inline">\(g(\cdot)\)</span> is the
<em>inverse</em> link function, then this leads to an objective function
of the form, <span class="math display">\[
E(\mathbf{ w}) = -  \sum_{i=1}^ny_i \log
g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i)\right) -
\sum_{i=1}^n(1-y_i)\log \left(1-g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}_i)\right)\right).
\]</span></p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(g, y):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Computes the objective function.&quot;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    labs <span class="op">=</span> np.asarray(y, dtype<span class="op">=</span><span class="bu">float</span>).flatten()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    posind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">1</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    negind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">0</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log(g[posind, :]).<span class="bu">sum</span>() <span class="op">-</span> np.log(<span class="dv">1</span><span class="op">-</span>g[negind, :]).<span class="bu">sum</span>()</span></code></pre></div>
<p>As normal, we would like to minimize this objective. This can be done
by differentiating with respect to the parameters of our prediction
function, <span class="math inline">\(\pi(\mathbf{ x};\mathbf{
w})\)</span>, for optimisation. The gradient of the likelihood with
respect to <span class="math inline">\(\pi(\mathbf{ x};\mathbf{
w})\)</span> is of the form, <span class="math display">\[
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = -\sum_{i=1}^n
\frac{y_i}{g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{
x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i) +  \sum_{i=1}^n
\frac{1-y_i}{1-g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{
x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i)
\]</span> where we used the chain rule to develop the derivative in
terms of <span
class="math inline">\(\frac{\text{d}g(f_i)}{\text{d}f_i}\)</span>, which
is the gradient of the inverse link function (in our case the gradient
of the sigmoid function).</p>
<p>So the objective function now depends on the gradient of the inverse
link function, as well as the likelihood depends on the gradient of the
inverse link function, as well as the gradient of the log likelihood,
and naturally the gradient of the argument of the inverse link function
with respect to the parameters, which is simply <span
class="math inline">\(\boldsymbol{ \phi}(\mathbf{ x}_i)\)</span>.</p>
<p>The only missing term is the gradient of the inverse link function.
For the sigmoid squashing function we have, <span
class="math display">\[\begin{align*}
g(f_i) &amp;= \frac{1}{1+\exp(-f_i)}\\
&amp;=(1+\exp(-f_i))^{-1}
\end{align*}\]</span> and the gradient can be computed as <span
class="math display">\[\begin{align*}
\frac{\text{d}g(f_i)}{\text{d} f_i} &amp; =
\exp(-f_i)(1+\exp(-f_i))^{-2}\\
&amp; = \frac{1}{1+\exp(-f_i)}
\frac{\exp(-f_i)}{1+\exp(-f_i)} \\
&amp; = g(f_i) (1-g(f_i))
\end{align*}\]</span> so the full gradient can be written down as <span
class="math display">\[
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = -\sum_{i=1}^n
y_i\left(1-g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i) +  \sum_{i=1}^n
(1-y_i)\left(g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i).
\]</span></p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient(g, Phi, y):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Generates the gradient of the parameter vector.&quot;</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    labs <span class="op">=</span> np.asarray(y, dtype<span class="op">=</span><span class="bu">float</span>).flatten()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    posind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> <span class="op">-</span>(Phi[posind]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>g[posind])).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    negind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">0</span> )</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">+=</span> (Phi[negind]<span class="op">*</span>g[negind]).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dw[:, <span class="va">None</span>]</span></code></pre></div>
<h2 id="optimization-of-the-function">Optimization of the Function</h2>
<p>Reorganizing the gradient to find a stationary point of the function
with respect to the parameters <span class="math inline">\(\mathbf{
w}\)</span> turns out to be impossible. Optimization has to proceed by
<em>numerical methods</em>. Options include the multidimensional variant
of <a href="http://en.wikipedia.org/wiki/Newton%27s_method">Newton’s
method</a> or <a
href="http://en.wikipedia.org/wiki/Gradient_method">gradient based
optimization methods</a> like we used for optimizing matrix
factorization for the movie recommender system. We recall from matrix
factorization that, for large data, <em>stochastic gradient descent</em>
or the Robbins Munro <span class="citation"
data-cites="Robbins:stoch51">(Robbins and Monro, 1951)</span>
optimization procedure worked best for function minimization.</p>
<ul>
<li><p>These are interpretable models: vital for disease etc.</p></li>
<li><p>Modern machine learning methods are less interpretable</p></li>
<li><p>Example: face recognition</p></li>
</ul>
<h1 id="deep-learning">Deep Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classical statistical models and simple machine learning models have
a great deal in common. The main difference between the fields is
philosophical. Machine learning practitioners are typically more
concerned with the quality of prediciton (e.g. measured by ROC curve)
while statisticians tend to focus more on the interpretability of the
model and the validity of any decisions drawn from that interpretation.
For example, a statistical model may be used to validate whether a large
scale intervention (such as the mass provision of mosquito nets) has had
a long term effect on disease (such as malaria). In this case one of the
covariates is likely to be the provision level of nets in a particular
region. The response variable would be the rate of malaria disease in
the region. The parmaeter, <span class="math inline">\(\beta_1\)</span>
associated with that covariate will demonstrate a positive or negative
effect which would be validated in answering the question. The focus in
statistics would be less on the accuracy of the response variable and
more on the validity of the interpretation of the effect variable, <span
class="math inline">\(\beta_1\)</span>.</p>
<p>A machine learning practitioner on the other hand would typically
denote the parameter <span class="math inline">\(w_1\)</span>, instead
of <span class="math inline">\(\beta_1\)</span> and would only be
interested in the output of the prediction function, <span
class="math inline">\(f(\cdot)\)</span> rather than the parameter
itself. The general formalism of the prediction function allows for
<em>non-linear</em> models. In machine learning, the emphasis on
prediction over interpretability means that non-linear models are often
used. The parameters, <span class="math inline">\(\mathbf{w}\)</span>,
are a means to an end (good prediction) rather than an end in themselves
(interpretable).</p>
<!-- No slide titles in this context -->
<h2 id="deepface">DeepFace</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-face.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-face.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deep-face-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-face&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-face-caption" class="caption-frame">
<p>Figure: The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.</p>
</div>
</div>
<p>The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span> consists
of layers that deal with <em>translation</em> invariances, known as
convolutional layers. These layers are followed by three
locally-connected layers and two fully-connected layers. Color
illustrates feature maps produced at each layer. The neural network
includes more than 120 million parameters, where more than 95% come from
the local and fully connected layers.</p>
<h3 id="deep-learning-as-pinball">Deep Learning as Pinball</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-as-pinball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/deep-learning-as-pinball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-pinball-caption" class="caption-frame">
<p>Figure: Deep learning models are composition of simple functions. We
can think of a pinball machine as an analogy. Each layer of pins
corresponds to one of the layers of functions in the model. Input data
is represented by the location of the ball from left to right when it is
dropped in from the top. Output class comes from the position of the
ball as it leaves the pins at the bottom.</p>
</div>
</div>
<p>Sometimes deep learning models are described as being like the brain,
or too complex to understand, but one analogy I find useful to help the
gist of these models is to think of them as being similar to early pin
ball machines.</p>
<p>In a deep neural network, we input a number (or numbers), whereas in
pinball, we input a ball.</p>
<p>Think of the location of the ball on the left-right axis as a single
number. Our simple pinball machine can only take one number at a time.
As the ball falls through the machine, each layer of pins can be thought
of as a different layer of ‘neurons’. Each layer acts to move the ball
from left to right.</p>
<p>In a pinball machine, when the ball gets to the bottom it might fall
into a hole defining a score, in a neural network, that is equivalent to
the decision: a classification of the input object.</p>
<p>An image has more than one number associated with it, so it is like
playing pinball in a <em>hyper-space</em>.</p>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-initialization-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-initialization&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-initialization-caption" class="caption-frame">
<p>Figure: At initialization, the pins, which represent the parameters
of the function, aren’t in the right place to bring the balls to the
correct decisions.</p>
</div>
</div>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-trained-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-trained&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-trained-caption" class="caption-frame">
<p>Figure: After learning the pins are now in the right place to bring
the balls to the correct decisions.</p>
</div>
</div>
<p>Learning involves moving all the pins to be in the correct position,
so that the ball ends up in the right place when it’s fallen through the
machine. But moving all these pins in hyperspace can be difficult.</p>
<p>In a hyper-space you have to put a lot of data through the machine
for to explore the positions of all the pins. Even when you feed many
millions of data points through the machine, there are likely to be
regions in the hyper-space where no ball has passed. When future test
data passes through the machine in a new route unusual things can
happen.</p>
<p><em>Adversarial examples</em> exploit this high dimensional space. If
you have access to the pinball machine, you can use gradient methods to
find a position for the ball in the hyper space where the image looks
like one thing, but will be classified as another.</p>
<p>Probabilistic methods explore more of the space by considering a
range of possible paths for the ball through the machine. This helps to
make them more data efficient and gives some robustness to adversarial
examples.</p>
<h2 id="example-prediction-of-malaria-incidence-in-uganda">Example:
Prediction of Malaria Incidence in Uganda</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_health/includes/malaria-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_health/includes/malaria-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/advds/./slides/diagrams//people/martin-mubangizi.png" clip-path="url(#clip0)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacecho
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/advds/./slides/diagrams//people/ricardo-andrade-pacheco.png" clip-path="url(#clip1)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/advds/./slides/diagrams//people/john-quinn.jpg" clip-path="url(#clip2)"/>
</svg>
</div>
<p>As an example of using Gaussian process models within the full
pipeline from data to decsion, we’ll consider the prediction of Malaria
incidence in Uganda. For the purposes of this study malaria reports come
in two forms, HMIS reports from health centres and Sentinel data, which
is curated by the WHO. There are limited sentinel sites and many HMIS
sites.</p>
<p>The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in
collaboration with John Quinn and Martin Mubangizi <span
class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>. John and Martin were
initally from the AI-DEV group from the University of Makerere in
Kampala and more latterly they were based at UN Global Pulse in Kampala.
You can see the work summarized on the UN Global Pulse <a
href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">disease
outbreaks project site here</a>.</p>
<ul>
<li>See <a href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">UN
Global Pulse Disease Outbreaks Site</a></li>
</ul>
<p>Malaria data is spatial data. Uganda is split into districts, and
health reports can be found for each district. This suggests that models
such as conditional random fields could be used for spatial modelling,
but there are two complexities with this. First of all, occasionally
districts split into two. Secondly, sentinel sites are a specific
location within a district, such as Nagongera which is a sentinel site
based in the Tororo district.</p>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uganda-districts-2006-magnify" class="magnify"
onclick="magnifyFigure(&#39;uganda-districts-2006&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uganda-districts-2006-caption" class="caption-frame">
<p>Figure: Ugandan districts. Data SRTM/NASA from <a
href="https://dds.cr.usgs.gov/srtm/version2_1"
class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.</p>
</div>
</div>
<div style="text-align:right">
<span class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>
</div>
<p>The common standard for collecting health data on the African
continent is from the Health management information systems (HMIS).
However, this data suffers from missing values <span class="citation"
data-cites="Gething:hmis06">(Gething et al., 2006)</span> and diagnosis
of diseases like typhoid and malaria may be confounded.</p>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/advds/./slides/diagrams//health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="tororo-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;tororo-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="tororo-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Tororo district, where the sentinel site, Nagongera, is
located.</p>
</div>
</div>
<p><a
href="https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/">World
Health Organization Sentinel Surveillance systems</a> are set up “when
high-quality data are needed about a particular disease that cannot be
obtained through a passive system”. Several sentinel sites give accurate
assessment of malaria disease levels in Uganda, including a site in
Nagongera.</p>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/advds/./slides/diagrams//health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sentinel-nagongera-magnify" class="magnify"
onclick="magnifyFigure(&#39;sentinel-nagongera&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sentinel-nagongera-caption" class="caption-frame">
<p>Figure: Sentinel and HMIS data along with rainfall and temperature
for the Nagongera sentinel station in the Tororo district.</p>
</div>
</div>
<p>In collaboration with the AI Research Group at Makerere we chose to
investigate whether Gaussian process models could be used to assimilate
information from these two different sources of disease informaton.
Further, we were interested in whether local information on rainfall and
temperature could be used to improve malaria estimates.</p>
<p>The aim of the project was to use WHO Sentinel sites, alongside
rainfall and temperature, to improve predictions from HMIS data of
levels of malaria.</p>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/advds/./slides/diagrams//health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="mubende-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;mubende-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mubende-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Mubende District.</p>
</div>
</div>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="malaria-prediction-mubende-magnify" class="magnify"
onclick="magnifyFigure(&#39;malaria-prediction-mubende&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="malaria-prediction-mubende-caption" class="caption-frame">
<p>Figure: Prediction of malaria incidence in Mubende.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The project arose out of the Gaussian process summer school
held at Makerere in Kampala in 2013. The school led, in turn, to the
Data Science Africa initiative.</p>
</div>
</div>
<h2 id="early-warning-systems">Early Warning Systems</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/advds/./slides/diagrams//health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kabarole-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kabarole district in Uganda.</p>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="kabarole-disease-over-time-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-disease-over-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-disease-over-time-caption" class="caption-frame">
<p>Figure: Estimate of the current disease situation in the Kabarole
district over time. Estimate is constructed with a Gaussian process with
an additive covariance funciton.</p>
</div>
</div>
<p>Health monitoring system for the Kabarole district. Here we have
fitted the reports with a Gaussian process with an additive covariance
function. It has two components, one is a long time scale component (in
red above) the other is a short time scale component (in blue).</p>
<p>Monitoring proceeds by considering two aspects of the curve. Is the
blue line (the short term report signal) above the red (which represents
the long term trend? If so we have higher than expected reports. If this
is the case <em>and</em> the gradient is still positive (i.e. reports
are going up) we encode this with a <em>red</em> color. If it is the
case and the gradient of the blue line is negative (i.e. reports are
going down) we encode this with an <em>amber</em> color. Conversely, if
the blue line is below the red <em>and</em> decreasing, we color
<em>green</em>. On the other hand if it is below red but increasing, we
color <em>yellow</em>.</p>
<p>This gives us an early warning system for disease. Red is a bad
situation getting worse, amber is bad, but improving. Green is good and
getting better and yellow good but degrading.</p>
<p>Finally, there is a gray region which represents when the scale of
the effect is small.</p>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-warning-system-map-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-warning-system-map&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-warning-system-map-caption" class="caption-frame">
<p>Figure: The map of Ugandan districts with an overview of the Malaria
situation in each district.</p>
</div>
</div>
<p>These colors can now be observed directly on a spatial map of the
districts to give an immediate impression of the current status of the
disease across the country.</p>
<h2 id="what-are-large-language-models">What are Large Language
Models?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-are-large-language-models.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/what-are-large-language-models.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="probability-conversations">Probability Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/conversation-probability.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/conversation-probability.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-probability-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//ai/anne-probability-conversation.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-probability-conversation-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-probability-conversation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-probability-conversation-caption" class="caption-frame">
<p>Figure: The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.</p>
</div>
</div>
<p>In practice we know that probabilities can be very unintuitive, for
example in court there is a fallacy known as the “prosecutor’s fallacy”
that confuses conditional probabilities. This can cause problems in jury
trials <span class="citation" data-cites="Thompson-juries89">(Thompson,
1989)</span>.</p>
<h2 id="llm-conversations">LLM Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/conversation-llm.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/conversation-llm.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-llm-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//ai/anne-llm-conversation.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-llm-conversation-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-llm-conversation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-llm-conversation-caption" class="caption-frame">
<p>Figure: The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.</p>
</div>
</div>
<div class="figure">
<div id="ai-for-data-analytics-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/0sJjdxn5kcI?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="ai-for-data-analytics-magnify" class="magnify"
onclick="magnifyFigure(&#39;ai-for-data-analytics&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ai-for-data-analytics-caption" class="caption-frame">
<p>Figure: The Inner Monologue paper suggests using LLMs for robotic
planning <span class="citation" data-cites="Huang-inner22">(Huang et
al., 2023)</span>.</p>
</div>
</div>
<p>By interacting directly with machines that have an understanding of
human cultural context, it should be possible to share the nature of
uncertainty in the same way humans do. See for example the paper <a
href="https://innermonologue.github.io/">Inner Monologue: Embodied
Reasoning through Planning</a> <span class="citation"
data-cites="Huang-inner22">Huang et al. (2023)</span>.</p>
<h2 id="the-moniac">The MONIAC</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_simulation/includes/the-moniac.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_simulation/includes/the-moniac.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><a href="https://en.wikipedia.org/wiki/MONIAC">The MONIAC</a> was an
analogue computer designed to simulate the UK economy. Analogue
comptuers work through analogy, the analogy in the MONIAC is that both
money and water flow. The MONIAC exploits this through a system of
tanks, pipes, valves and floats that represent the flow of money through
the UK economy. Water flowed from the treasury tank at the top of the
model to other tanks representing government spending, such as health
and education. The machine was initially designed for teaching support
but was also found to be a useful economic simulator. Several were built
and today you can see the original at Leeds Business School, there is
also one in the London Science Museum and one <a
href="https://www.econ.cam.ac.uk/economics-alumni/drip-down-economics-phillips-machine">in
the Unisversity of Cambridge’s economics faculty</a>.</p>
<div class="figure">
<div id="the-moniac-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-moniac-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-moniac&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-moniac-caption" class="caption-frame">
<p>Figure: Bill Phillips and his MONIAC (completed in 1949). The machine
is an analogue computer designed to simulate the workings of the UK
economy.</p>
</div>
</div>
<h2 id="donald-mackay">Donald MacKay</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/donald-mackay-brain.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/donald-mackay-brain.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="donald-maccrimmon-mackay-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//people/DonaldMacKay1952.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="donald-maccrimmon-mackay-magnify" class="magnify"
onclick="magnifyFigure(&#39;donald-maccrimmon-mackay&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="donald-maccrimmon-mackay-caption" class="caption-frame">
<p>Figure: Donald M. MacKay (1922-1987), a physicist who was an early
member of the cybernetics community and member of the Ratio Club.</p>
</div>
</div>
<p>Donald MacKay was a physicist who worked on naval gun targetting
during the second world war. The challenge with gun targetting for ships
is that both the target and the gun platform are moving. The challenge
was tackled using analogue computers, for example in the US the <a
href="https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer">Mark I
fire control computer</a> which was a mechanical computer. MacKay worked
on radar systems for gun laying, here the velocity and distance of the
target could be assessed through radar and an mechanical electrical
analogue computer.</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li>Section 5.2.2 up to pg 182 of <span class="citation"
data-cites="Rogers:book11">Rogers and Girolami (2011)</span></li>
</ul>
<h2 id="fire-control-systems">Fire Control Systems</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/../_ai/includes/fire-control-systems.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/../_ai/includes/fire-control-systems.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Naval gunnery systems deal with targeting guns while taking into
account movement of ships. The Royal Navy’s Gunnery Pocket Book <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span> gives details of one system for gun laying.</p>
<p>Like many challenges we face today, in the second world war, fire
control was handled by a hybrid system of humans and computers. This
means deploying human beings for the tasks that they can manage, and
machines for the tasks that are better performed by a machine. This
leads to a division of labour between the machine and the human that can
still be found in our modern digital ecosystems.</p>
<div class="figure">
<div id="low-angle-fire-control-team-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//ai/low-angle-fire-control-team.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="low-angle-fire-control-team-magnify" class="magnify"
onclick="magnifyFigure(&#39;low-angle-fire-control-team&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="low-angle-fire-control-team-caption" class="caption-frame">
<p>Figure: The fire control computer set at the centre of a system of
observation and tracking <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.</p>
</div>
</div>
<p>As analogue computers, fire control computers from the second world
war would contain components that directly represented the different
variables that were important in the problem to be solved, such as the
inclination between two ships.</p>
<div class="figure">
<div id="the-measurement-of-inclination-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//ai/the-measurement-of-inclination.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-measurement-of-inclination-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-measurement-of-inclination&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-measurement-of-inclination-caption" class="caption-frame">
<p>Figure: Measuring inclination between two ships <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span>. Sophisticated fire control computers allowed the ship to
continue to fire while under maneuvers.</p>
</div>
</div>
<p>The fire control systems were electro-mechanical analogue computers
that represented the “state variables” of interest, such as inclination
and ship speed with gears and cams within the machine.</p>
<div class="figure">
<div id="typical-modern-fire-control-table-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//ai/typical-modern-fire-control-table.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="typical-modern-fire-control-table-magnify" class="magnify"
onclick="magnifyFigure(&#39;typical-modern-fire-control-table&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="typical-modern-fire-control-table-caption"
class="caption-frame">
<p>Figure: A second world war gun computer’s control table <span
class="citation" data-cites="Admiralty-gunnery45">(The Admiralty,
1945)</span>.</p>
</div>
</div>
<p>For more details on fire control computers, you can watch a 1953 film
on the the US the <a
href="https://en.wikipedia.org/wiki/Mark_I_Fire_Control_Computer">Mark
IA fire control computer</a> from Periscope Film.</p>
<div class="figure">
<div id="us-navy-training-film-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/gwf5mAlI7Ug?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="us-navy-training-film-magnify" class="magnify"
onclick="magnifyFigure(&#39;us-navy-training-film&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="us-navy-training-film-caption" class="caption-frame">
<p>Figure: U.S. Navy training film MN-6783a. Basic Mechanisms of Fire
Control Computers. Mechanical Computer Instructional Film 27794 (1953)
for the Mk 1A Fire Control Computer.</p>
</div>
</div>
<h2 id="behind-the-eye">Behind the Eye</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/../_books/includes/behind-the-eye.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/../_books/includes/behind-the-eye.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="behind-the-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//books/behind-the-eye.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="behind-the-eye-magnify" class="magnify"
onclick="magnifyFigure(&#39;behind-the-eye&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="behind-the-eye-caption" class="caption-frame">
<p>Figure: <a
href="https://www.amazon.co.uk/Behind-Eye-Gifford-Lectures-MACKAY/dp/0631173323">Behind
the Eye</a> <span class="citation" data-cites="Mackay-behind91">(MacKay,
1991)</span> summarises MacKay’s Gifford Lectures, where MacKay uses the
operation of the eye as a window on the operation of the brain.</p>
</div>
</div>
<p>Donald MacKay was at King’s College for his PhD. He was just down the
road from Bill Phillips at LSE who was building the MONIAC. He was part
of the Ratio Club. A group of early career scientists who were
interested in communication and control in animals and humans, or more
specifically they were interested in computers and brains. The were part
of an international movement known as cybernetics.</p>
<p>Donald MacKay wrote of the influence that his own work on radar had
on his interest in the brain.</p>
<blockquote>
<p>… during the war I had worked on the theory of automated and
electronic computing and on the theory of information, all of which are
highly relevant to such things as automatic pilots and automatic gun
direction. I found myself grappling with problems in the design of
artificial sense organs for naval gun-directors and with the principles
on which electronic circuits could be used to simulate situations in the
external world so as to provide goal-directed guidance for ships,
aircraft, missiles and the like.</p>
</blockquote>
<blockquote>
<p>Later in the 1940’s, when I was doing my Ph.D. work, there was much
talk of the brain as a computer and of the early digital computers that
were just making the headlines as “electronic brains.” As an analogue
computer man I felt strongly convinced that the brain, whatever it was,
was not a digital computer. I didn’t think it was an analogue computer
either in the conventional sense.</p>
</blockquote>
<blockquote>
<p>But this naturally rubbed under my skin the question: well, if it is
not either of these, what kind of system is it? Is there any way of
following through the kind of analysis that is appropriate to their
artificial automata so as to understand better the kind of system the
human brain is? That was the beginning of my slippery slope into brain
research.</p>
<p><em>Behind the Eye</em> pg 40. Edited version of the 1986 Gifford
Lectures given by Donald M. MacKay and edited by Valerie MacKay</p>
</blockquote>
<p>Importantly, MacKay distinguishes between the <em>analogue</em>
computer and the <em>digital</em> computer. As he mentions, his
experience was with analogue machines. An analogue machine is
<em>literally</em> an analogue. The radar systems that Wiener and MacKay
both worked on were made up of electronic components such as resistors,
capacitors, inductors and/or mechanical components such as cams and
gears. Together these components could represent a physical system, such
as an anti-aircraft gun and a plane. The design of the analogue computer
required the engineer to simulate the real world in analogue
electronics, using dualities that exist between e.g. mechanical circuits
(mass, spring, damper) and electronic circuits (inductor, resistor,
capacitor). The analogy between mass and a damper, between spring and a
resistor and between capacitor and a damper works because the underlying
mathematics is approximated with the same linear system: a second order
differential equation. This mathematical analogy allowed the designer to
map from the real world, through mathematics, to a virtual world where
the components reflected the real world through analogy.</p>
<h2 id="human-analogue-machine">Human Analogue Machine</h2>
<p>The machine learning systems we have built today that can reconstruct
human text, or human classification of images, necessarily must have
some aspects to them that are analagous to our understanding. As MacKay
suggests the brain is neither a digital or an analogue computer, and the
same can be said of the modern neural network systems that are being
tagged as “artificial intelligence”.</p>
<p>{I believe a better term for them is “human-analogue machines”,
because what we have built is not a system that can make intelligent
decisions from first principles (a rational approach) but one that
observes how humans have made decisions through our data and
reconstructs that process. Machine learning is more empiricist than
rational, but now we n empirical approach that distils our evolved
intelligence.</p>
<div class="figure">
<div id="human-analogue-machine-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//ai/human-analogue-machine.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="human-analogue-machine-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-analogue-machine&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-analogue-machine-caption" class="caption-frame">
<p>Figure: The human analogue machine creates a feature space which is
analagous to that we use to reason, one way of doing this is to have a
machine attempt to compress all human generated text in an
auto-regressive manner.</p>
</div>
</div>
<ul>
<li><p>A human-analogue machine is a machine that has created a feature
space that is analagous to the “feature space” our brain uses to
reason.</p></li>
<li><p>The latest generation of LLMs are exhibiting this charateristic,
giving them ability to converse.</p></li>
</ul>
<p>The perils of developing this capability include counterfeit people,
a notion that the philosopher <a
href="https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/">Daniel
Dennett has described in <em>The Atlantic</em></a>. This is where
computers can represent themselves as human and fool people into doing
things on that basis.</p>
<p>{* Perils of this include <em>counterfeit people</em>. * Daniel
Dennett has described the challenges these bring in <a
href="https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/">an
article in The Atlantic</a>.</p>
<h2 id="intellectual-debt">Intellectual Debt</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/intellectual-debt-blog-post.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/intellectual-debt-blog-post.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="intellectual-debt-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//ai/2020-02-12-intellectual-debt.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="intellectual-debt-magnify" class="magnify"
onclick="magnifyFigure(&#39;intellectual-debt&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intellectual-debt-caption" class="caption-frame">
<p>Figure: Jonathan Zittrain’s term to describe the challenges of
explanation that come with AI is Intellectual Debt.</p>
</div>
</div>
<p>In the context of machine learning and complex systems, Jonathan
Zittrain has coined the term <a
href="https://medium.com/berkman-klein-center/from-technical-debt-to-intellectual-debt-in-ai-e05ac56a502c">“Intellectual
Debt”</a> to describe the challenge of understanding what you’ve
created. In <a
href="https://mlatcl.github.io/projects/data-oriented-architectures-for-ai-based-systems.html">the
ML@CL group we’ve been foucssing on developing the notion of a
<em>data-oriented architecture</em></a> to deal with intellectual debt
<span class="citation" data-cites="Cabrera-realworld23">(Cabrera et al.,
2023)</span>.</p>
<p>Zittrain points out the challenge around the lack of interpretability
of individual ML models as the origin of intellectual debt. In machine
learning I refer to work in this area as fairness, interpretability and
transparency or FIT models. To an extent I agree with Zittrain, but if
we understand the context and purpose of the decision making, I believe
this is readily put right by the correct monitoring and retraining
regime around the model. A concept I refer to as “progression testing”.
Indeed, the best teams do this at the moment, and their failure to do it
feels more of a matter of technical debt rather than intellectual,
because arguably it is a maintenance task rather than an explanation
task. After all, we have good statistical tools for interpreting
individual models and decisions when we have the context. We can
linearise around the operating point, we can perform counterfactual
tests on the model. We can build empirical validation sets that explore
fairness or accuracy of the model.</p>
<p>But if we can avoid the pitfalls of counterfeit people, this also
offers us an opportunity to <em>psychologically represent</em> <span
class="citation" data-cites="Heider:interpersonal58">(Heider,
1958)</span> the machine in a manner where humans can communicate
without special training. This in turn offers the opportunity to
overcome the challenge of <em>intellectual debt</em>.</p>
<div class="figure">
<div id="new-flow-of-information-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//data-science/new-flow-of-information004.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-4-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-4-caption" class="caption-frame">
<p>Figure: The trinity of human, data, and computer, and highlights the
modern phenomenon. The communication channel between computer and data
now has an extremely high bandwidth. The channel between human and
computer and the channel between data and human is narrow. New direction
of information flow, information is reaching us mediated by the
computer. The focus on classical statistics reflected the importance of
the direct communication between human and data. The modern challenges
of data science emerge when that relationship is being mediated by the
machine.</p>
</div>
</div>
<div class="figure">
<div id="new-flow-of-information-ham-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//data-science/new-flow-of-information-ham.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-ham-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-ham&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-ham-caption" class="caption-frame">
<p>Figure: The HAM now sits between us and the traditional digital
computer.</p>
</div>
</div>
<h2 id="networked-interactions">Networked Interactions</h2>
<p>Our modern society intertwines the machine with human interactions.
The key question is who has control over these interfaces between humans
and machines.</p>
<div class="figure">
<div id="human-computers-interacting-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//ai/human-computers-interacting.svg" width="80%" style=" ">
</object>
</div>
<div id="human-computers-interacting-magnify" class="magnify"
onclick="magnifyFigure(&#39;human-computers-interacting&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="human-computers-interacting-caption" class="caption-frame">
<p>Figure: Humans and computers interacting should be a major focus of
our research and engineering efforts.</p>
</div>
</div>
<p>So the real challenge that we face for society is understanding which
systemic interventions will encourage the right interactions between the
humans and the machine at all of these interfaces.</p>
<h2 id="conclusions">Conclusions</h2>
<p>In today’s lecture we’ve drilled down further on a difficult aspect
of data science. By focusing too much on the data and the technical
challenges we face, we can forget the context. But to do data science
well, we must not forget the context of the data. We need to pay
attention to domain experts and introduce their understanding to our
analysis. Above all we must not forget that data is almost always (in
the end) about people.</p>
<h1 id="references">References</h1>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Andrade:consistent14" class="csl-entry" role="listitem">
Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.
Consistent mapping of government malaria records across a changing
territory delimitation. Malaria Journal 13. <a
href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>
</div>
<div id="ref-Cabrera-realworld23" class="csl-entry" role="listitem">
Cabrera, C., Paleyes, A., Thodoroff, P., Lawrence, N.D., 2023. <a
href="https://arxiv.org/abs/2302.04810">Real-world machine learning
systems: A survey from a data-oriented architecture perspective</a>.
</div>
<div id="ref-Gething:hmis06" class="csl-entry" role="listitem">
Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,
Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data
from health management information systems in <span>A</span>frica using
space–time geostatistics. PLoS Medicine 3. <a
href="https://doi.org/10.1371/journal.pmed.0030271">https://doi.org/10.1371/journal.pmed.0030271</a>
</div>
<div id="ref-Heider:interpersonal58" class="csl-entry" role="listitem">
Heider, F., 1958. The psychology of interpersonal relations. John Wiley.
</div>
<div id="ref-Huang-inner22" class="csl-entry" role="listitem">
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng,
A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T.,
Brown, N., Luu, L., Levine, S., Hausman, K., ichter, brian, 2023. <a
href="https://proceedings.mlr.press/v205/huang23c.html">Inner monologue:
Embodied reasoning through planning with language models</a>, in: Liu,
K., Kulic, D., Ichnowski, J. (Eds.), Proceedings of the 6th Conference
on Robot Learning, Proceedings of Machine Learning Research. PMLR, pp.
1769–1782.
</div>
<div id="ref-Krizhevsky:imagenet12" class="csl-entry" role="listitem">
Krizhevsky, A., Sutskever, I., Hinton, G.E., n.d. ImageNet
classification with deep convolutional neural networks. pp. 1097–1105.
</div>
<div id="ref-Mackay-behind91" class="csl-entry" role="listitem">
MacKay, D.M., 1991. Behind the eye. Basil Blackwell.
</div>
<div id="ref-Mubangizi:malaria14" class="csl-entry" role="listitem">
Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,
N.D., 2014. Malaria surveillance with multiple data sources using
<span>Gaussian</span> process models, in: 1st International Conference
on the Use of Mobile <span>ICT</span> in Africa.
</div>
<div id="ref-Robbins:stoch51" class="csl-entry" role="listitem">
Robbins, H., Monro, S., 1951. A stochastic approximation method. Annals
of Mathematical Statistics 22, 400–407.
</div>
<div id="ref-Rogers:book11" class="csl-entry" role="listitem">
Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC
Press.
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Admiralty-gunnery45" class="csl-entry" role="listitem">
The Admiralty, 1945. <a href="https://www.maritime.org/doc/br224/">The
gunnery pocket book, b.r. 224/45</a>.
</div>
<div id="ref-Thompson-juries89" class="csl-entry" role="listitem">
Thompson, W.C., 1989. <a href="http://www.jstor.org/stable/1191906">Are
juries competent to evaluate statistical evidence?</a> Law and
Contemporary Problems 52, 9–41.
</div>
<div id="ref-Wiener:cybernetics48" class="csl-entry" role="listitem">
Wiener, N., 1948. Cybernetics: Control and communication in the animal
and the machine. MIT Press, Cambridge, MA.
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>The logarithm of a number less than one is negative, for
a number greater than one the logarithm is positive. So if odds are
greater than evens (odds-on) the log-odds are positive, if the odds are
less than evens (odds-against) the log-odds will be negative.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

