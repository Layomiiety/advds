---
title: "AI and Data Science"
venue: "LT2, William Gates Building"
abstract: "<p>In the first lecture, we laid out the underpinning
phenomena that give us the landscape of data science. In this lecture we
unpick the challenges that landscape presents us with. The material
gives you context for why data science is very different from standard
software engineering, and how data science problems need to be
approached including the many different aspects that need to be
considered. We will look at the challenges of deploying data science
solutions in practice. We categorize them into three groups.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Cambridge
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orcid: 
edit_url: https://github.com/mlatcl/advds/edit/gh-pages/_lamd/ai-and-data-science.md
date: 2024-11-04
published: 2024-11-04
time: "10:00"
week: 4
session: 2
featured_image: assets/images/the-challenges-of-data-science.png
reveal: 04-02-ai-and-data-science.slides.html
transition: None
ipynb: 04-02-ai-and-data-science.ipynb
youtube: "BQKfIJHPiCQ"
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="introduction">Introduction</h1>
<p>Data science is an emerging discipline. That makes it harder to make
clean decisions about what any given individual will need to know to
become a data scientist. Those of you who are studying now will be those
that define the discipline. As we deploy more data driven decision
making in the world, the role will be refined. Until we achieve that
refinement, your knowledge needs to be broad based.</p>
<p>In this lecture we will first continue our theme of how our
limitations as humans mean that our analysis of data can be affected,
and I will introduce an analogy that should help you understand
<em>how</em> data science differs significantly from traditional
software engineering.</p>
<h2 id="notutils">notutils</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>This small package is a helper package for various notebook utilities
used below.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install notutils</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/notutils"
class="uri">https://github.com/lawrennd/notutils</a></p>
<p>Once <code>notutils</code> is installed, it can be imported in the
usual manner.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> notutils</span></code></pre></div>
<h2 id="mlai">mlai</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The <code>mlai</code> software is a suite of helper functions for
teaching and demonstrating machine learning algorithms. It was first
used in the Machine Learning and Adaptive Intelligence course in
Sheffield in 2013.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install mlai</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/mlai"
class="uri">https://github.com/lawrennd/mlai</a></p>
<p>Once <code>mlai</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> plot</span></code></pre></div>
<h2 id="pods">pods</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>In Sheffield we created a suite of software tools for ‘Open Data
Science’. Open data science is an approach to sharing code, models and
data that should make it easier for companies, health professionals and
scientists to gain access to data science techniques.</p>
<p>You can also check this blog post on <a
href="http://inverseprobability.com/2014/07/01/open-data-science">Open
Data Science</a>.</p>
<p>The software can be installed using</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install pods</span></code></pre></div>
<p>from the command prompt where you can access your python
installation.</p>
<p>The code is also available on GitHub: <a
href="https://github.com/lawrennd/ods"
class="uri">https://github.com/lawrennd/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual
manner.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<!-- defines the technical variant-->
<h2 id="statistics-to-deep-learning">Statistics to Deep Learning</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/statistics-to-deep-learning.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/statistics-to-deep-learning.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="what-is-machine-learning">What is Machine Learning?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-2.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-2.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Machine learning allows us to extract knowledge from data to form a
prediction.</p>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>A machine learning prediction is made by combining a model with data
to form the prediction. The manner in which this is done gives us the
machine learning <em>algorithm</em>.</p>
<p>Machine learning models are <em>mathematical models</em> which make
weak assumptions about data, e.g. smoothness assumptions. By combining
these assumptions with the data, we observe we can interpolate between
data points or, occasionally, extrapolate into the future.</p>
<p>Machine learning is a technology which strongly overlaps with the
methodology of statistics. From a historical/philosophical view point,
machine learning differs from statistics in that the focus in the
machine learning community has been primarily on accuracy of prediction,
whereas the focus in statistics is typically on the interpretability of
a model and/or validating a hypothesis through data collection.</p>
<p>The rapid increase in the availability of compute and data has led to
the increased prominence of machine learning. This prominence is
surfacing in two different but overlapping domains: data science and
artificial intelligence.</p>
<h2 id="from-model-to-decision">From Model to Decision</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-end-to-end.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml-end-to-end.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The real challenge, however, is end-to-end decision making. Taking
information from the environment and using it to drive decision making
to achieve goals.</p>
<h2 id="classical-statistical-analysis">Classical Statistical
Analysis</h2>
<p>Despite the shift of emphasis, traditional statistical techniques are
more important than ever. One of the few ways we have to validate the
analyses we create is to make use of visualizations, randomized testing
and other forms of statistical analysis. You will have explored some of
these ideas in earlier courses in machine learning. In this unit we
provide some review material in a practical sheet to bring some of those
ideas together in the context of data science.</p>
<h1 id="what-is-machine-learning-1">What is Machine Learning?</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-is-ml.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>What is machine learning? At its most basic level machine learning is
a combination of</p>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<p>where <em>data</em> is our observations. They can be actively or
passively acquired (meta-data). The <em>model</em> contains our
assumptions, based on previous experience. That experience can be other
data, it can come from transfer learning, or it can merely be our
beliefs about the regularities of the universe. In humans our models
include our inductive biases. The <em>prediction</em> is an action to be
taken or a categorization or a quality score. The reason that machine
learning has become a mainstay of artificial intelligence is the
importance of predictions in artificial intelligence. The data and the
model are combined through computation.</p>
<p>In practice we normally perform machine learning using two functions.
To combine data with a model we typically make use of:</p>
<p><strong>a prediction function</strong> it is used to make the
predictions. It includes our beliefs about the regularities of the
universe, our assumptions about how the world works, e.g., smoothness,
spatial similarities, temporal similarities.</p>
<p><strong>an objective function</strong> it defines the ‘cost’ of
misprediction. Typically, it includes knowledge about the world’s
generating processes (probabilistic objectives) or the costs we pay for
mispredictions (empirical risk minimization).</p>
<p>The combination of data and model through the prediction function and
the objective function leads to a <em>learning algorithm</em>. The class
of prediction functions and objective functions we can make use of is
restricted by the algorithms they lead to. If the prediction function or
the objective function are too complex, then it can be difficult to find
an appropriate learning algorithm. Much of the academic field of machine
learning is the quest for new learning algorithms that allow us to bring
different types of models and data together.</p>
<p>A useful reference for state of the art in machine learning is the UK
Royal Society Report, <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a>.</p>
<p>You can also check my post blog post on <a
href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What
is Machine Learning?</a>.</p>
<h2 id="artificial-intelligence-and-data-science">Artificial
Intelligence and Data Science</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/data-science-vs-ai.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/data-science-vs-ai.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Machine learning technologies have been the driver of two related,
but distinct disciplines. The first is <em>data science</em>. Data
science is an emerging field that arises from the fact that we now
collect so much data by happenstance, rather than by <em>experimental
design</em>. Classical statistics is the science of drawing conclusions
from data, and to do so statistical experiments are carefully designed.
In the modern era we collect so much data that there’s a desire to draw
inferences directly from the data.</p>
<p>As well as machine learning, the field of data science draws from
statistics, cloud computing, data storage (e.g. streaming data),
visualization and data mining.</p>
<p>In contrast, artificial intelligence technologies typically focus on
emulating some form of human behaviour, such as understanding an image,
or some speech, or translating text from one form to another. The recent
advances in artificial intelligence have come from machine learning
providing the automation. But in contrast to data science, in artificial
intelligence the data is normally collected with the specific task in
mind. In this sense it has strong relations to classical statistics.</p>
<p>Classically artificial intelligence worried more about <em>logic</em>
and <em>planning</em> and focused less on data driven decision making.
Modern machine learning owes more to the field of <em>Cybernetics</em>
<span class="citation" data-cites="Wiener-cybernetics48">(Wiener,
1948)</span> than artificial intelligence. Related fields include
<em>robotics</em>, <em>speech recognition</em>, <em>language
understanding</em> and <em>computer vision</em>.</p>
<p>There are strong overlaps between the fields, the wide availability
of data by happenstance makes it easier to collect data for designing AI
systems. These relations are coming through wide availability of sensing
technologies that are interconnected by cellular networks, WiFi and the
internet. This phenomenon is sometimes known as the <em>Internet of
Things</em>, but this feels like a dangerous misnomer. We must never
forget that we are interconnecting people, not things.</p>
<center>
Convention for the Protection of <em>Individuals</em> with regard to
Automatic Processing of <em>Personal Data</em> (1981/1/28)
</center>
<h2 id="what-does-machine-learning-do">What does Machine Learning
do?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-does-machine-learning-do.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-does-machine-learning-do.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Any process of automation allows us to scale what we do by codifying
a process in some way that makes it efficient and repeatable. Machine
learning automates by emulating human (or other actions) found in data.
Machine learning codifies in the form of a mathematical function that is
learnt by a computer. If we can create these mathematical functions in
ways in which they can interconnect, then we can also build systems.</p>
<p>Machine learning works through codifying a prediction of interest
into a mathematical function. For example, we can try and predict the
probability that a customer wants to by a jersey given knowledge of
their age, and the latitude where they live. The technique known as
logistic regression estimates the odds that someone will by a jumper as
a linear weighted sum of the features of interest.</p>
<p><span class="math display">\[ \text{odds} =
\frac{p(\text{bought})}{p(\text{not bought})} \]</span></p>
<p><span class="math display">\[ \log \text{odds}  = w_0 + w_1
\text{age} + w_2 \text{latitude}.\]</span> Here <span
class="math inline">\(w_0\)</span>, <span
class="math inline">\(w_1\)</span> and <span
class="math inline">\(w_2\)</span> are the parameters of the model. If
<span class="math inline">\(w_1\)</span> and <span
class="math inline">\(w_2\)</span> are both positive, then the log-odds
that someone will buy a jumper increase with increasing latitude and
age, so the further north you are and the older you are the more likely
you are to buy a jumper. The parameter <span
class="math inline">\(w_0\)</span> is an offset parameter and gives the
log-odds of buying a jumper at zero age and on the equator. It is likely
to be negative<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> indicating that the purchase is
odds-against. This is also a classical statistical model, and models
like logistic regression are widely used to estimate probabilities from
ad-click prediction to disease risk.</p>
<p>This is called a generalized linear model, we can also think of it as
estimating the <em>probability</em> of a purchase as a nonlinear
function of the features (age, latitude) and the parameters (the <span
class="math inline">\(w\)</span> values). The function is known as the
<em>sigmoid</em> or <a
href="https://en.wikipedia.org/wiki/Logistic_regression">logistic
function</a>, thus the name <em>logistic</em> regression.</p>
<h3 id="sigmoid-function">Sigmoid Function</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/sigmoid-function.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/sigmoid-function.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="the-logistic-function-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//ml/logistic.svg" width="80%" style=" ">
</object>
</div>
<div id="the-logistic-function-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-logistic-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-logistic-function-caption" class="caption-frame">
<p>Figure: The logistic function.</p>
</div>
</div>
<p>The function has this characeristic ‘s’-shape (from where the term
sigmoid, as in sigma, comes from). It also takes the input from the
entire real line and ‘squashes’ it into an output that is between zero
and one. For this reason it is sometimes also called a ‘squashing
function’.</p>
<p>The sigmoid comes from the inverting the odds ratio, <span
class="math display">\[
\frac{\pi}{(1-\pi)}
\]</span> where <span class="math inline">\(\pi\)</span> is the
probability of a positive outcome and <span
class="math inline">\(1-\pi\)</span> is the probability of a negative
outcome</p>
<p><span class="math display">\[ p(\text{bought}) =  \sigma\left(w_0 +
w_1 \text{age} + w_2 \text{latitude}\right).\]</span></p>
<p>In the case where we have <em>features</em> to help us predict, we
sometimes denote such features as a vector, <span
class="math inline">\(\mathbf{ x}\)</span>, and we then use an inner
product between the features and the parameters, <span
class="math inline">\(\mathbf{ w}^\top \mathbf{ x}= w_1 x_1 + w_2 x_2 +
w_3 x_3 ...\)</span>, to represent the argument of the sigmoid.</p>
<p><span class="math display">\[ p(\text{bought})
=  \sigma\left(\mathbf{ w}^\top \mathbf{ x}\right).\]</span> More
generally, we aim to predict some aspect of our data, <span
class="math inline">\(y\)</span>, by relating it through a mathematical
function, <span class="math inline">\(f(\cdot)\)</span>, to the
parameters, <span class="math inline">\(\mathbf{ w}\)</span> and the
data, <span class="math inline">\(\mathbf{ x}\)</span>.</p>
<p><span class="math display">\[ y=  f\left(\mathbf{ x}, \mathbf{
w}\right).\]</span> We call <span
class="math inline">\(f(\cdot)\)</span> the <em>prediction
function</em>.</p>
<p>To obtain the fit to data, we use a separate function called the
<em>objective function</em> that gives us a mathematical representation
of the difference between our predictions and the real data.</p>
<p><span class="math display">\[E(\mathbf{ w}, \mathbf{Y},
\mathbf{X})\]</span> A commonly used examples (for example in a
regression problem) is least squares, <span
class="math display">\[E(\mathbf{ w}, \mathbf{Y}, \mathbf{X}) =
\sum_{i=1}^n\left(y_i - f(\mathbf{ x}_i, \mathbf{
w})\right)^2.\]</span></p>
<p>If a linear prediction function is combined with the least squares
objective function, then that gives us a classical <em>linear
regression</em>, another classical statistical model. Statistics often
focusses on linear models because it makes interpretation of the model
easier. Interpretation is key in statistics because the aim is normally
to validate questions by analysis of data. Machine learning has
typically focused more on the prediction function itself and worried
less about the interpretation of parameters. In statistics, where
interpretation is typically more important than prediction, parameters
are normally denoted by <span
class="math inline">\(\boldsymbol{\beta}\)</span> instead of <span
class="math inline">\(\mathbf{ w}\)</span>.</p>
<p>A key difference between statistics and machine learning, is that
(traditionally) machine learning has focussed on predictive capability
and statistics has focussed on interpretability. That means that in a
statistics class far more emphasis will be placed on interpretation of
the parameters. In machine learning, the parameters, $, are just a means
to an end. But in statistics, when we denote the parameters by <span
class="math inline">\(\boldsymbol{\beta}\)</span>, we often use the
parameters to tell us something about the disease.</p>
<p>So we move between <span class="math display">\[ p(\text{bought})
=  \sigma\left(w_0 + w_1 \text{age} + w_2
\text{latitude}\right).\]</span></p>
<p>to denote the emphasis is on predictive power to</p>
<p><span class="math display">\[ p(\text{bought}) =  \sigma\left(\beta_0
+ \beta_1 \text{age} + \beta_2 \text{latitude}\right).\]</span></p>
<p>to denote the emphasis is on interpretation of the parameters.</p>
<p>Another effect of the focus on prediction in machine learning is that
<em>non-linear</em> approaches, which can be harder to interpret, are
more widely deployedin machine learning – they tend to improve quality
of predictions at the expense of interpretability.</p>
<h2 id="logistic-regression">Logistic Regression</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/logistic-regression.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>A logistic regression is an approach to classification which extends
the linear basis function models we’ve already explored. Rather than
modeling the output of the function directly the assumption is that we
model the <em>log-odds</em> with the basis functions.</p>
<p>The <a href="http://en.wikipedia.org/wiki/Odds">odds</a> are defined
as the ratio of the probability of a positive outcome, to the
probability of a negative outcome. If the probability of a positive
outcome is denoted by <span class="math inline">\(\pi\)</span>, then the
odds are computed as <span
class="math inline">\(\frac{\pi}{1-\pi}\)</span>. Odds are widely used
by <a href="http://en.wikipedia.org/wiki/Bookmaker">bookmakers</a> in
gambling, although a bookmakers odds won’t normalise: i.e. if you look
at the equivalent probabilities, and sum over the probability of all
outcomes the bookmakers are considering, then you won’t get one. This is
how the bookmaker makes a profit. Because a probability is always
between zero and one, the odds are always between <span
class="math inline">\(0\)</span> and <span
class="math inline">\(\infty\)</span>. If the positive outcome is
unlikely the odds are close to zero, if it is very likely then the odds
become close to infinite. Taking the logarithm of the odds maps the odds
from the positive half space to being across the entire real line. Odds
that were between 0 and 1 (where the negative outcome was more likely)
are mapped to the range between <span
class="math inline">\(-\infty\)</span> and <span
class="math inline">\(0\)</span>. Odds that are greater than 1 are
mapped to the range between <span class="math inline">\(0\)</span> and
<span class="math inline">\(\infty\)</span>. Considering the log odds
therefore takes a number between 0 and 1 (the probability of positive
outcome) and maps it to the entire real line. The function that does
this is known as the <a href="http://en.wikipedia.org/wiki/Logit">logit
function</a>, <span class="math inline">\(g^{-1}(p_i) =
\log\frac{p_i}{1-p_i}\)</span>. This function is known as a <em>link
function</em>.</p>
<p>For a standard regression we take, <span class="math display">\[
f(\mathbf{ x}) = \mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}),
\]</span> if we want to perform classification we perform a logistic
regression. <span class="math display">\[
\log \frac{\pi}{(1-\pi)} = \mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})
\]</span> where the odds ratio between the positive class and the
negative class is given by <span class="math display">\[
\frac{\pi}{(1-\pi)}
\]</span> The odds can never be negative, but can take any value from 0
to <span class="math inline">\(\infty\)</span>. We have defined the link
function as taking the form <span
class="math inline">\(g^{-1}(\cdot)\)</span> implying that the inverse
link function is given by <span class="math inline">\(g(\cdot)\)</span>.
Since we have defined, <span class="math display">\[
g^{-1}(\pi) =
\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x})
\]</span> we can write <span class="math inline">\(\pi\)</span> in terms
of the <em>inverse link</em> function, <span
class="math inline">\(g(\cdot)\)</span> as <span class="math display">\[
\pi = g(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})).
\]</span></p>
<h2 id="basis-function">Basis Function</h2>
<p>We’ll define our prediction, objective and gradient functions below.
But before we start, we need to define a basis function for our model.
Let’s start with the linear basis.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlai</span></code></pre></div>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlai <span class="im">import</span> linear</span></code></pre></div>
<h2 id="prediction-function">Prediction Function</h2>
<p>Now we have the basis function let’s define the prediction
function.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(w, x, basis<span class="op">=</span>linear, <span class="op">**</span>kwargs):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Generates the prediction function and the basis matrix.&quot;</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> basis(x, <span class="op">**</span>kwargs)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> np.dot(Phi, w)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>f)), Phi</span></code></pre></div>
<p>This inverse of the link function is known as the <a
href="http://en.wikipedia.org/wiki/Logistic_function">logistic</a> (thus
the name logistic regression) or sometimes it is called the sigmoid
function. For a particular value of the input to the link function,
<span class="math inline">\(f_i = \mathbf{ w}^\top \boldsymbol{
\phi}(\mathbf{ x}_i)\)</span> we can plot the value of the inverse link
function as below.</p>
<p>By replacing the inverse link with the sigmoid we can write <span
class="math inline">\(\pi\)</span> as a function of the input and the
parameter vector as, <span class="math display">\[
\pi(\mathbf{ x},\mathbf{ w}) = \frac{1}{1+\exp\left(-\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x})\right)}.
\]</span> The process for logistic regression is as follows. Compute the
output of a standard linear basis function composition (<span
class="math inline">\(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\)</span>, as we did for linear regression) and then apply the
inverse link function, <span class="math inline">\(g(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}))\)</span>. In logistic regression this
involves <em>squashing</em> it with the logistic (or sigmoid) function.
Use this value, which now has an interpretation as a
<em>probability</em> in a Bernoulli distribution to form the likelihood.
Then we can assume conditional independence of each data point given the
parameters and develop a likelihod for the entire data set.</p>
<p>As we discussed last time, the Bernoulli likelihood is of the form,
<span class="math display">\[
P(y_i|\mathbf{ w}, \mathbf{ x}) =
\pi_i^{y_i} (1-\pi_i)^{1-y_i}
\]</span> which we can think of as clever trick for mathematically
switching between two probabilities if we were to write it as code it
would be better described as</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bernoulli(x, y, pi):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> y <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pi(x)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>pi(x)</span></code></pre></div>
<p>but writing it mathematically makes it easier to write our objective
function within a single mathematical equation.</p>
<h2 id="maximum-likelihood">Maximum Likelihood</h2>
<p>To obtain the parameters of the model, we need to maximize the
likelihood, or minimize the objective function, normally taken to be the
negative log likelihood. With a data conditional independence assumption
the likelihood has the form, <span class="math display">\[
P(\mathbf{ y}|\mathbf{ w},
\mathbf{X}) = \prod_{i=1}^nP(y_i|\mathbf{ w}, \mathbf{ x}_i).
\]</span> which can be written as a log likelihood in the form <span
class="math display">\[
\log P(\mathbf{ y}|\mathbf{ w},
\mathbf{X}) = \sum_{i=1}^n\log P(y_i|\mathbf{ w}, \mathbf{ x}_i) =
\sum_{i=1}^n
y_i \log \pi_i + \sum_{i=1}^n(1-y_i)\log (1-\pi_i)
\]</span> and if we take the probability of positive outcome for the
<span class="math inline">\(i\)</span>th data point to be given by <span
class="math display">\[
\pi_i = g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x}_i)\right),
\]</span> where <span class="math inline">\(g(\cdot)\)</span> is the
<em>inverse</em> link function, then this leads to an objective function
of the form, <span class="math display">\[
E(\mathbf{ w}) = -  \sum_{i=1}^ny_i \log
g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{ x}_i)\right) -
\sum_{i=1}^n(1-y_i)\log \left(1-g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{ x}_i)\right)\right).
\]</span></p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(g, y):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Computes the objective function.&quot;</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    labs <span class="op">=</span> np.asarray(y, dtype<span class="op">=</span><span class="bu">float</span>).flatten()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    posind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">1</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    negind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">0</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log(g[posind, :]).<span class="bu">sum</span>() <span class="op">-</span> np.log(<span class="dv">1</span><span class="op">-</span>g[negind, :]).<span class="bu">sum</span>()</span></code></pre></div>
<p>As normal, we would like to minimize this objective. This can be done
by differentiating with respect to the parameters of our prediction
function, <span class="math inline">\(\pi(\mathbf{ x};\mathbf{
w})\)</span>, for optimisation. The gradient of the likelihood with
respect to <span class="math inline">\(\pi(\mathbf{ x};\mathbf{
w})\)</span> is of the form, <span class="math display">\[
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = -\sum_{i=1}^n
\frac{y_i}{g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{
x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i) +  \sum_{i=1}^n
\frac{1-y_i}{1-g\left(\mathbf{ w}^\top
\boldsymbol{ \phi}(\mathbf{
x})\right)}\frac{\text{d}g(f_i)}{\text{d}f_i}
\boldsymbol{ \phi}(\mathbf{ x}_i)
\]</span> where we used the chain rule to develop the derivative in
terms of <span
class="math inline">\(\frac{\text{d}g(f_i)}{\text{d}f_i}\)</span>, which
is the gradient of the inverse link function (in our case the gradient
of the sigmoid function).</p>
<p>So the objective function now depends on the gradient of the inverse
link function, as well as the likelihood depends on the gradient of the
inverse link function, as well as the gradient of the log likelihood,
and naturally the gradient of the argument of the inverse link function
with respect to the parameters, which is simply <span
class="math inline">\(\boldsymbol{ \phi}(\mathbf{ x}_i)\)</span>.</p>
<p>The only missing term is the gradient of the inverse link function.
For the sigmoid squashing function we have, <span
class="math display">\[\begin{align*}
g(f_i) &amp;= \frac{1}{1+\exp(-f_i)}\\
&amp;=(1+\exp(-f_i))^{-1}
\end{align*}\]</span> and the gradient can be computed as <span
class="math display">\[\begin{align*}
\frac{\text{d}g(f_i)}{\text{d} f_i} &amp; =
\exp(-f_i)(1+\exp(-f_i))^{-2}\\
&amp; = \frac{1}{1+\exp(-f_i)}
\frac{\exp(-f_i)}{1+\exp(-f_i)} \\
&amp; = g(f_i) (1-g(f_i))
\end{align*}\]</span> so the full gradient can be written down as <span
class="math display">\[
\frac{\text{d}E(\mathbf{ w})}{\text{d}\mathbf{ w}} = -\sum_{i=1}^n
y_i\left(1-g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i) +  \sum_{i=1}^n
(1-y_i)\left(g\left(\mathbf{ w}^\top \boldsymbol{ \phi}(\mathbf{
x})\right)\right)
\boldsymbol{ \phi}(\mathbf{ x}_i).
\]</span></p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient(g, Phi, y):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;Generates the gradient of the parameter vector.&quot;</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    labs <span class="op">=</span> np.asarray(y, dtype<span class="op">=</span><span class="bu">float</span>).flatten()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    posind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">1</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> <span class="op">-</span>(Phi[posind]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>g[posind])).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    negind <span class="op">=</span> np.where(labs<span class="op">==</span><span class="dv">0</span> )</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">+=</span> (Phi[negind]<span class="op">*</span>g[negind]).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dw[:, <span class="va">None</span>]</span></code></pre></div>
<h2 id="optimization-of-the-function">Optimization of the Function</h2>
<p>Reorganizing the gradient to find a stationary point of the function
with respect to the parameters <span class="math inline">\(\mathbf{
w}\)</span> turns out to be impossible. Optimization has to proceed by
<em>numerical methods</em>. Options include the multidimensional variant
of <a href="http://en.wikipedia.org/wiki/Newton%27s_method">Newton’s
method</a> or <a
href="http://en.wikipedia.org/wiki/Gradient_method">gradient based
optimization methods</a> like we used for optimizing matrix
factorization for the movie recommender system. We recall from matrix
factorization that, for large data, <em>stochastic gradient descent</em>
or the Robbins Munro <span class="citation"
data-cites="Robbins:stoch51">(Robbins and Monro, 1951)</span>
optimization procedure worked best for function minimization.</p>
<h2 id="example-prediction-of-malaria-incidence-in-uganda">Example:
Prediction of Malaria Incidence in Uganda</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_health/includes/malaria-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip7">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/advds/./slides/diagrams//people/martin-mubangizi.png" clip-path="url(#clip7)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip8">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacheco
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/advds/./slides/diagrams//people/ricardo-andrade-pacheco.png" clip-path="url(#clip8)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip9">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/advds/./slides/diagrams//people/john-quinn.jpg" clip-path="url(#clip9)"/>
</svg>
</div>
<p>As an example of using Gaussian process models within the full
pipeline from data to decsion, we’ll consider the prediction of Malaria
incidence in Uganda. For the purposes of this study malaria reports come
in two forms, HMIS reports from health centres and Sentinel data, which
is curated by the WHO. There are limited sentinel sites and many HMIS
sites.</p>
<p>The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in
collaboration with John Quinn and Martin Mubangizi <span
class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>. John and Martin were
initally from the AI-DEV group from the University of Makerere in
Kampala and more latterly they were based at UN Global Pulse in Kampala.
You can see the work summarized on the UN Global Pulse <a
href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">disease
outbreaks project site here</a>.</p>
<ul>
<li>See <a href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">UN
Global Pulse Disease Outbreaks Site</a></li>
</ul>
<p>Malaria data is spatial data. Uganda is split into districts, and
health reports can be found for each district. This suggests that models
such as conditional random fields could be used for spatial modelling,
but there are two complexities with this. First of all, occasionally
districts split into two. Secondly, sentinel sites are a specific
location within a district, such as Nagongera which is a sentinel site
based in the Tororo district.</p>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uganda-districts-2006-magnify" class="magnify"
onclick="magnifyFigure(&#39;uganda-districts-2006&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="uganda-districts-2006-caption" class="caption-frame">
<p>Figure: Ugandan districts. Data SRTM/NASA from <a
href="https://dds.cr.usgs.gov/srtm/version2_1"
class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.</p>
</div>
</div>
<div style="text-align:right">
<span class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>
</div>
<p>The common standard for collecting health data on the African
continent is from the Health management information systems (HMIS).
However, this data suffers from missing values <span class="citation"
data-cites="Gething:hmis06">(Gething et al., 2006)</span> and diagnosis
of diseases like typhoid and malaria may be confounded.</p>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/advds/./slides/diagrams//health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="tororo-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;tororo-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="tororo-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Tororo district, where the sentinel site, Nagongera, is
located.</p>
</div>
</div>
<p><a
href="https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/">World
Health Organization Sentinel Surveillance systems</a> are set up “when
high-quality data are needed about a particular disease that cannot be
obtained through a passive system”. Several sentinel sites give accurate
assessment of malaria disease levels in Uganda, including a site in
Nagongera.</p>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://mlatcl.github.io/advds/./slides/diagrams//health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sentinel-nagongera-magnify" class="magnify"
onclick="magnifyFigure(&#39;sentinel-nagongera&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="sentinel-nagongera-caption" class="caption-frame">
<p>Figure: Sentinel and HMIS data along with rainfall and temperature
for the Nagongera sentinel station in the Tororo district.</p>
</div>
</div>
<p>In collaboration with the AI Research Group at Makerere we chose to
investigate whether Gaussian process models could be used to assimilate
information from these two different sources of disease informaton.
Further, we were interested in whether local information on rainfall and
temperature could be used to improve malaria estimates.</p>
<p>The aim of the project was to use WHO Sentinel sites, alongside
rainfall and temperature, to improve predictions from HMIS data of
levels of malaria.</p>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/advds/./slides/diagrams//health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="mubende-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;mubende-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mubende-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Mubende District.</p>
</div>
</div>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="malaria-prediction-mubende-magnify" class="magnify"
onclick="magnifyFigure(&#39;malaria-prediction-mubende&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="malaria-prediction-mubende-caption" class="caption-frame">
<p>Figure: Prediction of malaria incidence in Mubende.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The project arose out of the Gaussian process summer school
held at Makerere in Kampala in 2013. The school led, in turn, to the
Data Science Africa initiative.</p>
</div>
</div>
<h2 id="early-warning-systems">Early Warning Systems</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="https://mlatcl.github.io/advds/./slides/diagrams//health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
<div id="kabarole-district-in-uganda-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-district-in-uganda&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kabarole district in Uganda.</p>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="kabarole-disease-over-time-magnify" class="magnify"
onclick="magnifyFigure(&#39;kabarole-disease-over-time&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kabarole-disease-over-time-caption" class="caption-frame">
<p>Figure: Estimate of the current disease situation in the Kabarole
district over time. Estimate is constructed with a Gaussian process with
an additive covariance funciton.</p>
</div>
</div>
<p>Health monitoring system for the Kabarole district. Here we have
fitted the reports with a Gaussian process with an additive covariance
function. It has two components, one is a long time scale component (in
red above) the other is a short time scale component (in blue).</p>
<p>Monitoring proceeds by considering two aspects of the curve. Is the
blue line (the short term report signal) above the red (which represents
the long term trend? If so we have higher than expected reports. If this
is the case <em>and</em> the gradient is still positive (i.e. reports
are going up) we encode this with a <em>red</em> color. If it is the
case and the gradient of the blue line is negative (i.e. reports are
going down) we encode this with an <em>amber</em> color. Conversely, if
the blue line is below the red <em>and</em> decreasing, we color
<em>green</em>. On the other hand if it is below red but increasing, we
color <em>yellow</em>.</p>
<p>This gives us an early warning system for disease. Red is a bad
situation getting worse, amber is bad, but improving. Green is good and
getting better and yellow good but degrading.</p>
<p>Finally, there is a gray region which represents when the scale of
the effect is small.</p>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-warning-system-map-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-warning-system-map&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-warning-system-map-caption" class="caption-frame">
<p>Figure: The map of Ugandan districts with an overview of the Malaria
situation in each district.</p>
</div>
</div>
<p>These colors can now be observed directly on a spatial map of the
districts to give an immediate impression of the current status of the
disease across the country.</p>
<h1 id="deep-learning">Deep Learning</h1>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-overview.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Classical statistical models and simple machine learning models have
a great deal in common. The main difference between the fields is
philosophical. Machine learning practitioners are typically more
concerned with the quality of prediciton (e.g. measured by ROC curve)
while statisticians tend to focus more on the interpretability of the
model and the validity of any decisions drawn from that interpretation.
For example, a statistical model may be used to validate whether a large
scale intervention (such as the mass provision of mosquito nets) has had
a long term effect on disease (such as malaria). In this case one of the
covariates is likely to be the provision level of nets in a particular
region. The response variable would be the rate of malaria disease in
the region. The parmaeter, <span class="math inline">\(\beta_1\)</span>
associated with that covariate will demonstrate a positive or negative
effect which would be validated in answering the question. The focus in
statistics would be less on the accuracy of the response variable and
more on the validity of the interpretation of the effect variable, <span
class="math inline">\(\beta_1\)</span>.</p>
<p>A machine learning practitioner on the other hand would typically
denote the parameter <span class="math inline">\(w_1\)</span>, instead
of <span class="math inline">\(\beta_1\)</span> and would only be
interested in the output of the prediction function, <span
class="math inline">\(f(\cdot)\)</span> rather than the parameter
itself. The general formalism of the prediction function allows for
<em>non-linear</em> models. In machine learning, the emphasis on
prediction over interpretability means that non-linear models are often
used. The parameters, <span class="math inline">\(\mathbf{w}\)</span>,
are a means to an end (good prediction) rather than an end in themselves
(interpretable).</p>
<!-- No slide titles in this context -->
<h2 id="deepface">DeepFace</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-face.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="deep-face-magnify" class="magnify"
onclick="magnifyFigure(&#39;deep-face&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="deep-face-caption" class="caption-frame">
<p>Figure: The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.</p>
</div>
</div>
<p>The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span> consists
of layers that deal with <em>translation</em> invariances, known as
convolutional layers. These layers are followed by three
locally-connected layers and two fully-connected layers. Color
illustrates feature maps produced at each layer. The neural network
includes more than 120 million parameters, where more than 95% come from
the local and fully connected layers.</p>
<h3 id="deep-learning-as-pinball">Deep Learning as Pinball</h3>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/deep-learning-as-pinball.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-pinball-magnify" class="magnify"
onclick="magnifyFigure(&#39;early-pinball&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="early-pinball-caption" class="caption-frame">
<p>Figure: Deep learning models are composition of simple functions. We
can think of a pinball machine as an analogy. Each layer of pins
corresponds to one of the layers of functions in the model. Input data
is represented by the location of the ball from left to right when it is
dropped in from the top. Output class comes from the position of the
ball as it leaves the pins at the bottom.</p>
</div>
</div>
<p>Sometimes deep learning models are described as being like the brain,
or too complex to understand, but one analogy I find useful to help the
gist of these models is to think of them as being similar to early pin
ball machines.</p>
<p>In a deep neural network, we input a number (or numbers), whereas in
pinball, we input a ball.</p>
<p>Think of the location of the ball on the left-right axis as a single
number. Our simple pinball machine can only take one number at a time.
As the ball falls through the machine, each layer of pins can be thought
of as a different layer of ‘neurons’. Each layer acts to move the ball
from left to right.</p>
<p>In a pinball machine, when the ball gets to the bottom it might fall
into a hole defining a score, in a neural network, that is equivalent to
the decision: a classification of the input object.</p>
<p>An image has more than one number associated with it, so it is like
playing pinball in a <em>hyper-space</em>.</p>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-initialization-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-initialization&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-initialization-caption" class="caption-frame">
<p>Figure: At initialization, the pins, which represent the parameters
of the function, aren’t in the right place to bring the balls to the
correct decisions.</p>
</div>
</div>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
<div id="pinball-trained-magnify" class="magnify"
onclick="magnifyFigure(&#39;pinball-trained&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="pinball-trained-caption" class="caption-frame">
<p>Figure: After learning the pins are now in the right place to bring
the balls to the correct decisions.</p>
</div>
</div>
<p>Learning involves moving all the pins to be in the correct position,
so that the ball ends up in the right place when it’s fallen through the
machine. But moving all these pins in hyperspace can be difficult.</p>
<p>In a hyper-space you have to put a lot of data through the machine
for to explore the positions of all the pins. Even when you feed many
millions of data points through the machine, there are likely to be
regions in the hyper-space where no ball has passed. When future test
data passes through the machine in a new route unusual things can
happen.</p>
<p><em>Adversarial examples</em> exploit this high dimensional space. If
you have access to the pinball machine, you can use gradient methods to
find a position for the ball in the hyper space where the image looks
like one thing, but will be classified as another.</p>
<p>Probabilistic methods explore more of the space by considering a
range of possible paths for the ball through the machine. This helps to
make them more data efficient and gives some robustness to adversarial
examples.</p>
<h2 id="what-are-large-language-models">What are Large Language
Models?</h2>
<h2 id="probability-conversations">Probability Conversations</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ai/includes/conversation-probability.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="anne-probability-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//ai/anne-probability-conversation.svg" width="80%" style=" ">
</object>
</div>
<div id="anne-probability-conversation-magnify" class="magnify"
onclick="magnifyFigure(&#39;anne-probability-conversation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="anne-probability-conversation-caption" class="caption-frame">
<p>Figure: The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.</p>
</div>
</div>
<p>In practice we know that probabilities can be very unintuitive, for
example in court there is a fallacy known as the “prosecutor’s fallacy”
that confuses conditional probabilities. This can cause problems in jury
trials <span class="citation" data-cites="Thompson-juries89">(Thompson,
1989)</span>.</p>
<h2 id="what-are-large-language-models-1">What are Large Language
Models?</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-are-large-language-models.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/what-are-large-language-models.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<h2 id="the-moniac">The MONIAC</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_simulation/includes/the-moniac.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p><a href="https://en.wikipedia.org/wiki/MONIAC">The MONIAC</a> was an
analogue computer designed to simulate the UK economy. Analogue
comptuers work through analogy, the analogy in the MONIAC is that both
money and water flow. The MONIAC exploits this through a system of
tanks, pipes, valves and floats that represent the flow of money through
the UK economy. Water flowed from the treasury tank at the top of the
model to other tanks representing government spending, such as health
and education. The machine was initially designed for teaching support
but was also found to be a useful economic simulator. Several were built
and today you can see the original at Leeds Business School, there is
also one in the London Science Museum and one <a
href="https://www.econ.cam.ac.uk/economics-alumni/drip-down-economics-phillips-machine">in
the Unisversity of Cambridge’s economics faculty</a>.</p>
<div class="figure">
<div id="the-moniac-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-moniac-magnify" class="magnify"
onclick="magnifyFigure(&#39;the-moniac&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-moniac-caption" class="caption-frame">
<p>Figure: Bill Phillips and his MONIAC (completed in 1949). The machine
is an analogue computer designed to simulate the workings of the UK
economy.</p>
</div>
</div>
<p>See <span class="citation" data-cites="Lawrence-atomic24">Lawrence
(2024)</span> MONIAC p. 232-233, 266, 343.</p>
<p>But if we can avoid the pitfalls of counterfeit people, this also
offers us an opportunity to <em>psychologically represent</em> <span
class="citation"
data-cites="Heider:interpersonal58">(<strong>Heider:interpersonal58?</strong>)</span>
the machine in a manner where humans can communicate without special
training. This in turn offers the opportunity to overcome the challenge
of <em>intellectual debt</em>.</p>
<p>Despite the lack of interpretability of machine learning models, they
allow us access to what the machine is doing in a way that bypasses many
of the traditional techniques developed in statistics. But understanding
this new route for access is a major new challenge.</p>
<h2 id="ham">HAM</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/new-flow-of-information-ham.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The Human-Analogue Machine or HAM therefore provides a route through
which we could better understand our world through improving the way we
interact with machines.</p>
<div class="figure">
<div id="new-flow-of-information-4-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//data-science/new-flow-of-information004.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-4-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-4&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-4-caption" class="caption-frame">
<p>Figure: The trinity of human, data, and computer, and highlights the
modern phenomenon. The communication channel between computer and data
now has an extremely high bandwidth. The channel between human and
computer and the channel between data and human is narrow. New direction
of information flow, information is reaching us mediated by the
computer. The focus on classical statistics reflected the importance of
the direct communication between human and data. The modern challenges
of data science emerge when that relationship is being mediated by the
machine.</p>
</div>
</div>
<p>The HAM can provide an interface between the digital computer and the
human allowing humans to work closely with computers regardless of their
understandin gf the more technical parts of software engineering.</p>
<div class="figure">
<div id="new-flow-of-information-ham-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//data-science/new-flow-of-information-ham.svg" width="70%" style=" ">
</object>
</div>
<div id="new-flow-of-information-ham-magnify" class="magnify"
onclick="magnifyFigure(&#39;new-flow-of-information-ham&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="new-flow-of-information-ham-caption" class="caption-frame">
<p>Figure: The HAM now sits between us and the traditional digital
computer.</p>
</div>
</div>
<p>Of course this route provides new routes for manipulation, new ways
in which the machine can undermine our autonomy or exploit our cognitive
foibles. The major challenge we face is steering between these worlds
where we gain the advantage of the computer’s bandwidth without
undermining our culture and individual autonomy.</p>
<p>See <span class="citation" data-cites="Lawrence-atomic24">Lawrence
(2024)</span> human-analogue machine (HAMs) p. 343-347, 359-359,
365-368.</p>
<h2 id="complexity-in-action">Complexity in Action</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_psychology/includes/selective-attention-bias.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>As an exercise in understanding complexity, watch the following
video. You will see the basketball being bounced around, and the players
moving. Your job is to count the passes of those dressed in white and
ignore those of the individuals dressed in black.</p>
<div class="figure">
<div id="monkey-business-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/vJG698U2Mvo?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="monkey-business-magnify" class="magnify"
onclick="magnifyFigure(&#39;monkey-business&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="monkey-business-caption" class="caption-frame">
<p>Figure: Daniel Simon’s famous illusion “monkey business”. Focus on
the movement of the ball distracts the viewer from seeing other aspects
of the image.</p>
</div>
</div>
<p>In a classic study <span class="citation"
data-cites="Simons-gorillas99">Simons and Chabris (1999)</span> ask
subjects to count the number of passes of the basketball between players
on the team wearing white shirts. Fifty percent of the time, these
subjects don’t notice the gorilla moving across the scene.</p>
<p>The phenomenon of inattentional blindness is well known, e.g in their
paper Simons and Charbris quote the Hungarian neurologist, Rezsö
Bálint,</p>
<blockquote>
<p>It is a well-known phenomenon that we do not notice anything
happening in our surroundings while being absorbed in the inspection of
something; focusing our attention on a certain object may happen to such
an extent that we cannot perceive other objects placed in the peripheral
parts of our visual field, although the light rays they emit arrive
completely at the visual sphere of the cerebral cortex.</p>
<p>Rezsö Bálint 1907 (translated in Husain and Stein 1988, page 91)</p>
</blockquote>
<p>When we combine the complexity of the world with our relatively low
bandwidth for information, problems can arise. Our focus on what we
perceive to be the most important problem can cause us to miss other
(potentially vital) contextual information.</p>
<p>This phenomenon is known as selective attention or ‘inattentional
blindness’.</p>
<div class="figure">
<div id="daniel-simons-monkey-business-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/_oGAzq5wM_Q?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="daniel-simons-monkey-business-magnify" class="magnify"
onclick="magnifyFigure(&#39;daniel-simons-monkey-business&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="daniel-simons-monkey-business-caption" class="caption-frame">
<p>Figure: For a longer talk on inattentional bias from Daniel Simons
see this video.</p>
</div>
</div>
<h2 id="data-selective-attention-bias">Data Selective Attention
Bias</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-selection-attention-bias.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/data-selection-attention-bias.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We are going to see how inattention biases can play out in data
analysis by going through a simple example. The analysis involves body
mass index and activity information.</p>
<h2 id="bmi-steps-data">BMI Steps Data</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_datasets/includes/bmi-steps-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/bmi-steps-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>The BMI Steps example is taken from <span class="citation"
data-cites="Yanai-hypothesis20">Yanai and Lercher (2020)</span>. We are
given a data set of body-mass index measurements against step counts.
For convenience we have packaged the data so that it can be easily
downloaded.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pods</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pods.datasets.bmi_steps()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>] </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</span></code></pre></div>
<p>It is good practice to give our variables interpretable names so that
the analysis may be clearly understood by others. Here the
<code>steps</code> count is the first dimension of the covariate, the
<code>bmi</code> is the second dimension and the <code>gender</code> is
stored in <code>y</code> with <code>1</code> for female and
<code>0</code> for male.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> X[:, <span class="dv">0</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>bmi <span class="op">=</span> X[:, <span class="dv">1</span>]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>gender <span class="op">=</span> y[:, <span class="dv">0</span>]</span></code></pre></div>
<p>We can check the mean steps and the mean of the BMI.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Steps mean is </span><span class="sc">{mean}</span><span class="st">.&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>steps.mean()))</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;BMI mean is </span><span class="sc">{mean}</span><span class="st">.&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>bmi.mean()))</span></code></pre></div>
<h2 id="bmi-steps-data-analysis">BMI Steps Data Analysis</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/bmi-steps-analysis.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/bmi-steps-analysis.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>We can also separate out the means from the male and female
populations. In python this can be done by setting male and female
indices as follows.</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>male_ind <span class="op">=</span> (gender<span class="op">==</span><span class="dv">0</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>female_ind <span class="op">=</span> (gender<span class="op">==</span><span class="dv">1</span>)</span></code></pre></div>
<p>And now we can extract the variables for the two populations.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>male_steps <span class="op">=</span> steps[male_ind]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>male_bmi <span class="op">=</span> bmi[male_ind]</span></code></pre></div>
<p>And as before we compute the mean.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Male steps mean is </span><span class="sc">{mean}</span><span class="st">.&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>male_steps.mean()))</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Male BMI mean is </span><span class="sc">{mean}</span><span class="st">.&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>male_bmi.mean()))</span></code></pre></div>
<p>Similarly, we can get the same result for the female portion of the
populaton.</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>female_steps <span class="op">=</span> steps[female_ind]</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>female_bmi <span class="op">=</span> bmi[female_ind]</span></code></pre></div>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Female steps mean is </span><span class="sc">{mean}</span><span class="st">.&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>female_steps.mean()))</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Female BMI mean is </span><span class="sc">{mean}</span><span class="st">.&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>female_bmi.mean()))</span></code></pre></div>
<p>Interesting, the female BMI average is slightly higher than the male
BMI average. The number of steps in the male group is higher than that
in the female group. Perhaps the steps and the BMI are anti-correlated.
The more steps, the lower the BMI.</p>
<p>Python provides a statistics package. We’ll import this in
<code>python</code> so that we can try and understand the correlation
between the <code>steps</code> and the <code>BMI</code>.</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> pearsonr</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>corr, _ <span class="op">=</span> pearsonr(steps, bmi)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Pearson&#39;s overall correlation: </span><span class="sc">{corr}</span><span class="st">&quot;</span>.<span class="bu">format</span>(corr<span class="op">=</span>corr))</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>male_corr, _ <span class="op">=</span> pearsonr(male_steps, male_bmi)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Pearson&#39;s correlation for males: </span><span class="sc">{corr}</span><span class="st">&quot;</span>.<span class="bu">format</span>(corr<span class="op">=</span>male_corr))</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>female_corr, _ <span class="op">=</span> pearsonr(female_steps, female_bmi)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Pearson&#39;s correlation for females: </span><span class="sc">{corr}</span><span class="st">&quot;</span>.<span class="bu">format</span>(corr<span class="op">=</span>female_corr))</span></code></pre></div>
<h2 id="a-hypothesis-as-a-liability">A Hypothesis as a Liability</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/hypothesis-as-a-liability.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/hypothesis-as-a-liability.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>This analysis is from an article titled “A Hypothesis as a Liability”
<span class="citation" data-cites="Yanai-hypothesis20">(Yanai and
Lercher, 2020)</span>, they start their article with the following quite
from Herman Hesse.</p>
<blockquote>
<p>“ ‘When someone seeks,’ said Siddhartha, ‘then it easily happens that
his eyes see only the thing that he seeks, and he is able to find
nothing, to take in nothing. […] Seeking means: having a goal. But
finding means: being free, being open, having no goal.’ ”</p>
<p>Hermann Hesse</p>
</blockquote>
<p>Their idea is that having a hypothesis can constrain our thinking.
However, in answer to their paper <span class="citation"
data-cites="Felin-data20">Felin et al. (2021)</span> argue that some
form of hypothesis is always necessary, suggesting that a hypothesis
<em>can</em> be a liability</p>
<p>My view is captured in the introductory chapter to an edited volume
on computational systems biology that I worked on with Mark Girolami,
Magnus Rattray and Guido Sanguinetti.</p>
<div class="figure">
<div id="licsb-popper-quote-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//data-science/licsb-popper-quote.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="licsb-popper-quote-magnify" class="magnify"
onclick="magnifyFigure(&#39;licsb-popper-quote&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="licsb-popper-quote-caption" class="caption-frame">
<p>Figure: Quote from <span class="citation"
data-cites="Lawrence:licsbintro10">Lawrence (2010)</span> highlighting
the importance of interaction between data and hypothesis.</p>
</div>
</div>
<p>Popper nicely captures the interaction between hypothesis and data by
relating it to the chicken and the egg. The important thing is that
these two co-evolve.</p>
<h2 id="number-theatre">Number Theatre</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/number-data-theatre.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/number-data-theatre.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>Unfortunately, we don’t always have time to wait for this process to
converge to an answer we can all rely on before a decision is
required.</p>
<p>Not only can we be misled by data before a decision is made, but
sometimes we can be misled by data to justify the making of a decision.
David Spiegelhalter refers to the phenomenon of “Number Theatre” in a
conversation with Andrew Marr from May 2020 on the presentation of
data.</p>
<div class="figure">
<div id="david-andrew-marr-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/9388XmWIHXg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="david-andrew-marr-magnify" class="magnify"
onclick="magnifyFigure(&#39;david-andrew-marr&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="david-andrew-marr-caption" class="caption-frame">
<p>Figure: Professor Sir David Spiegelhalter on Andrew Marr on 10th May
2020 speaking about some of the challengers around data, data
presentation, and decision making in a pandemic. David mentions number
theatre at 9 minutes 10 seconds.</p>
</div>
</div>
<!--includebbcvideo{p08csg28}-->
<h2 id="data-theatre">Data Theatre</h2>
<p>Data Theatre exploits data inattention bias to present a particular
view on events that may misrepresents through selective presentation.
Statisticians are one of the few groups that are trained with a
sufficient degree of data skepticism. But it can also be combatted
through ensuring there are domain experts present, and that they can
speak freely.</p>
<div class="figure">
<div id="data-theatre-001-figure" class="figure-frame">
<object class="svgplot " data="https://mlatcl.github.io/advds/./slides/diagrams//business/data-theatre001.svg" width="60%" style=" ">
</object>
</div>
<div id="data-theatre-001-magnify" class="magnify"
onclick="magnifyFigure(&#39;data-theatre-001&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="data-theatre-001-caption" class="caption-frame">
<p>Figure: The phenomenon of number theatre or <em>data theatre</em> was
described by David Spiegelhalter and is nicely summarized by Martin
Robbins in this sub-stack article <a
href="https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards"
class="uri">https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards</a>.</p>
</div>
</div>
<h2 id="sir-david-spiegelhalter">Sir David Spiegelhalter</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_data-science/includes/sir-david-spiegelhalter.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_data-science/includes/sir-david-spiegelhalter.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<center>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip10">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
David Spiegelhalter
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://mlatcl.github.io/advds/./slides/diagrams//people/david-spiegelhalter.png" clip-path="url(#clip10)"/>
</svg>
</center>
<p>The statistician’s craft is based on humility in front of data and
developing the appropriate skeptical thinking around conclusions from
data. The best individual I’ve seen at conveying and developing that
sense is Sir David Spiegelhalter.</p>
<h2 id="the-art-of-statistics">The Art of Statistics</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/the-art-of-statistics.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-art-of-statistics.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<div class="figure">
<div id="art-of-statistics-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//books/the-art-of-statistics.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="art-of-statistics-magnify" class="magnify"
onclick="magnifyFigure(&#39;art-of-statistics&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="art-of-statistics-caption" class="caption-frame">
<p>Figure: <a
href="https://www.amazon.co.uk/Art-Statistics-Learning-Pelican-Books-ebook/dp/B07HQDJD99">The
Art of Statistics by David Spiegelhalter</a> is an excellent read on the
pitfalls of data interpretation.</p>
</div>
</div>
<p><em>The Art of Statistics</em> <span class="citation"
data-cites="Spiegelhalter-art19">(Spiegelhalter, 2019)</span> brings
important examples from statistics to life in an intelligent and
entertaining way. It is highly readable and gives an opportunity to
fast-track towards the important skill of data-skepticism that is the
mark of a professional statistician.</p>
<h2 id="the-art-of-uncertainty">The Art of Uncertainty</h2>
<div style="text-align:right">
<span class="editsection-bracket" style="">[</span><span
class="editsection"
style=""><a href="https://github.com/lawrennd/snippets/edit/main/_books/includes/the-art-of-uncertainty.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_books/includes/the-art-of-uncertainty.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span>
</div>
<p>David has also released a new book that focusses on Uncertainty.</p>
<div class="figure">
<div id="art-of-uncertainty-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://mlatcl.github.io/advds/./slides/diagrams//books/the-art-of-uncertainty.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="art-of-uncertainty-magnify" class="magnify"
onclick="magnifyFigure(&#39;art-of-uncertainty&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="art-of-uncertainty-caption" class="caption-frame">
<p>Figure: <a
href="https://www.amazon.co.uk/Art-Uncertainty-Navigate-Chance-Ignorance/dp/0241658624">The
Art of Uncertainty by David Spiegelhalter</a>.</p>
</div>
</div>
<p>See <span class="citation"
data-cites="Spiegelhalter-art24">(Spiegelhalter, 2024)</span></p>
<p>In today’s lecture we’ve drilled down further on a difficult aspect
of data science. By focusing too much on the data and the technical
challenges we face, we can forget the context. But to do data science
well, we must not forget the context of the data. We need to pay
attention to domain experts and introduce their understanding to our
analysis. Above all we must not forget that data is almost always (in
the end) about people.</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li>Section 5.2.2 up to pg 182 of <span class="citation"
data-cites="Rogers:book11">Rogers and Girolami (2011)</span></li>
</ul>
<h1 id="references">References</h1>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to
check the following resources.</p>
<ul>
<li>book: <a
href="https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248">The
Atomic Human</a></li>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></li>
<li>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></li>
<li>blog: <a
href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Andrade:consistent14" class="csl-entry" role="listitem">
Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.
Consistent mapping of government malaria records across a changing
territory delimitation. Malaria Journal 13. <a
href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>
</div>
<div id="ref-Felin-data20" class="csl-entry" role="listitem">
Felin, T., Koenderink, J., Krueger, J.I., Noble, D., Ellis, G.F.R.,
2021. The data-hypothesis relationship. Genome Biology 22. <a
href="https://doi.org/10.1186/s13059-021-02276-4">https://doi.org/10.1186/s13059-021-02276-4</a>
</div>
<div id="ref-Gething:hmis06" class="csl-entry" role="listitem">
Gething, P.W., Noor, A.M., Gikandi, P.W., Ogara, E.A.A., Hay, S.I.,
Nixon, M.S., Snow, R.W., Atkinson, P.M., 2006. Improving imperfect data
from health management information systems in <span>A</span>frica using
space–time geostatistics. PLoS Medicine 3. <a
href="https://doi.org/10.1371/journal.pmed.0030271">https://doi.org/10.1371/journal.pmed.0030271</a>
</div>
<div id="ref-Lawrence-atomic24" class="csl-entry" role="listitem">
Lawrence, N.D., 2024. <a
href="https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248">The
atomic human: Understanding ourselves in the age of AI</a>. Allen Lane.
</div>
<div id="ref-Lawrence:licsbintro10" class="csl-entry" role="listitem">
Lawrence, N.D., 2010. Introduction to learning and inference in
computational systems biology.
</div>
<div id="ref-Mubangizi:malaria14" class="csl-entry" role="listitem">
Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,
N.D., 2014. Malaria surveillance with multiple data sources using
<span>Gaussian</span> process models, in: 1st International Conference
on the Use of Mobile <span>ICT</span> in Africa.
</div>
<div id="ref-Robbins:stoch51" class="csl-entry" role="listitem">
Robbins, H., Monro, S., 1951. A stochastic approximation method. Annals
of Mathematical Statistics 22, 400–407.
</div>
<div id="ref-Rogers:book11" class="csl-entry" role="listitem">
Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC
Press.
</div>
<div id="ref-Simons-gorillas99" class="csl-entry" role="listitem">
Simons, D.J., Chabris, C.F., 1999. Gorillas in our midst: Sustained
inattentional blindness for dynamic events. Perception 28, 1059–1074. <a
href="https://doi.org/10.1068/p281059">https://doi.org/10.1068/p281059</a>
</div>
<div id="ref-Spiegelhalter-art24" class="csl-entry" role="listitem">
Spiegelhalter, D.J., 2024. The art of uncertainty. Pelican.
</div>
<div id="ref-Spiegelhalter-art19" class="csl-entry" role="listitem">
Spiegelhalter, D.J., 2019. The art of statistics. Pelican.
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Thompson-juries89" class="csl-entry" role="listitem">
Thompson, W.C., 1989. <a href="http://www.jstor.org/stable/1191906">Are
juries competent to evaluate statistical evidence?</a> Law and
Contemporary Problems 52, 9–41.
</div>
<div id="ref-Wiener-cybernetics48" class="csl-entry" role="listitem">
Wiener, N., 1948. Cybernetics: Control and communication in the animal
and the machine. mitp, Cambridge, MA.
</div>
<div id="ref-Yanai-hypothesis20" class="csl-entry" role="listitem">
Yanai, I., Lercher, M., 2020. A hypothesis is a liability. Genome
Biology 21.
</div>
</div>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>The logarithm of a number less than one is negative, for
a number greater than one the logarithm is positive. So if odds are
greater than evens (odds-on) the log-odds are positive, if the odds are
less than evens (odds-against) the log-odds will be negative.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>

